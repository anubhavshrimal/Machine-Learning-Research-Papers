# Author-wise

<!-- no toc -->
- [Adam Paszke](#Adam-Paszke)
- [Aditya Ramesh](#Aditya-Ramesh)
- [Agata Mosinska](#Agata-Mosinska)
- [Alec Radford](#Alec-Radford)
- [Aleksander Madry](#Aleksander-Madry)
- [Alexandre Alahi](#Alexandre-Alahi)
- [Alexei A. Efros](#Alexei-A.-Efros)
- [Alexey Dosovitskiy](#Alexey-Dosovitskiy)
- [Alexis Conneau](#Alexis-Conneau)
- [Alfredo Canziani](#Alfredo-Canziani)
- [Ali Farhadi](#Ali-Farhadi)
- [Alimohammad Beigi](#Alimohammad-Beigi)
- [Anant Jain](#Anant-Jain)
- [Andreas Mayr](#Andreas-Mayr)
- [Andrew G. Howard](#Andrew-G.-Howard)
- [Andrew Ilyas](#Andrew-Ilyas)
- [Anselm Levskaya](#Anselm-Levskaya)
- [Antoine Bosselut](#Antoine-Bosselut)
- [Ari S. Morcos](#Ari-S.-Morcos)
- [Ashish Vaswani](#Ashish-Vaswani)
- [Asim Kadav](#Asim-Kadav)
- [Augustus Odena](#Augustus-Odena)
- [Bharath Hariharan](#Bharath-Hariharan)
- [Boaz Barak](#Boaz-Barak)
- [Carroll L. Wainwright](#Carroll-L.-Wainwright)
- [Chandra Bhagavatula](#Chandra-Bhagavatula)
- [Christian Buck](#Christian-Buck)
- [Christian Szegedy](#Christian-Szegedy)
- [Colin Raffel](#Colin-Raffel)
- [Danqi Chen](#Danqi-Chen)
- [Dario Amodei](#Dario-Amodei)
- [David Berthelot](#David-Berthelot)
- [David J. Schwab](#David-J.-Schwab)
- [David Silver](#David-Silver)
- [Davis Wertheimer](#Davis-Wertheimer)
- [Dedy Kredo](#Dedy-Kredo)
- [Demis Hassabis](#Demis-Hassabis)
- [Denny Zhou](#Denny-Zhou)
- [Dian Yu](#Dian-Yu)
- [Diederik P. Kingma](#Diederik-P.-Kingma)
- [Dimitris Metaxas](#Dimitris-Metaxas)
- [Dimitris Tsipras](#Dimitris-Tsipras)
- [Diogo Almeida](#Diogo-Almeida)
- [Diyi Yang](#Diyi-Yang)
- [Dmytro Mishkin](#Dmytro-Mishkin)
- [Eugenio Culurciello](#Eugenio-Culurciello)
- [Fangxiaoyu Feng](#Fangxiaoyu-Feng)
- [Forrest N. Iandola](#Forrest-N.-Iandola)
- [Gabriel Goh](#Gabriel-Goh)
- [Gal Kaplun](#Gal-Kaplun)
- [Geoffrey E Hinton](#Geoffrey-E-Hinton)
- [Greg Corrado](#Greg-Corrado)
- [Guillaume Lample](#Guillaume-Lample)
- [Günter Klambauer](#Günter-Klambauer)
- [Han Zhang](#Han-Zhang)
- [Hannah Rashkin](#Hannah-Rashkin)
- [Hao Li](#Hao-Li)
- [Hao Tan](#Hao-Tan)
- [Hattie Zhou](#Hattie-Zhou)
- [Hyung Won Chung](#Hyung-Won-Chung)
- [Ian Goodfellow](#Ian-Goodfellow)
- [Iftekhar Naim](#Iftekhar-Naim)
- [Illia Polosukhin](#Illia-Polosukhin)
- [Ilya Sutskever](#Ilya-Sutskever)
- [Ioannis Antonoglou](#Ioannis-Antonoglou)
- [Itamar Friedman](#Itamar-Friedman)
- [Izhak Shafran](#Izhak-Shafran)
- [Jaakko Lehtinen](#Jaakko-Lehtinen)
- [Jack Hessel](#Jack-Hessel)
- [Jacob Devlin](#Jacob-Devlin)
- [Jae Sung Park](#Jae-Sung-Park)
- [Jaegul Choo](#Jaegul-Choo)
- [Jaemin Cho](#Jaemin-Cho)
- [Jakob Uszkoreit](#Jakob-Uszkoreit)
- [James J. Little](#James-J.-Little)
- [Jan Leike](#Jan-Leike)
- [Janice Lan](#Janice-Lan)
- [Jared Kaplan](#Jared-Kaplan)
- [Jason Wei](#Jason-Wei)
- [Jason Weston](#Jason-Weston)
- [Jason Yosinski](#Jason-Yosinski)
- [Javier Romero](#Javier-Romero)
- [Jeff Wu](#Jeff-Wu)
- [Jeffrey Dean](#Jeffrey-Dean)
- [Jeffrey Wu](#Jeffrey-Wu)
- [Jeffrey Zhao](#Jeffrey-Zhao)
- [Jessica B. Hamrick](#Jessica-B.-Hamrick)
- [Jiakai Zhang](#Jiakai-Zhang)
- [Jie Le](#Jie-Le)
- [Jimmy Ba](#Jimmy-Ba)
- [Jiri Matas](#Jiri-Matas)
- [Jitendra Malik](#Jitendra-Malik)
- [Jonathan Frankle](#Jonathan-Frankle)
- [Jong Wook Kim](#Jong-Wook-Kim)
- [Jordan Hoffmann](#Jordan-Hoffmann)
- [Julian Schrittwiese](#Julian-Schrittwiese)
- [Julieta Martinez](#Julieta-Martinez)
- [Jun Huang](#Jun-Huang)
- [Jun-Yan Zhu](#Jun-Yan-Zhu)
- [Jung-Woo Ha](#Jung-Woo-Ha)
- [Justin Johnson](#Justin-Johnson)
- [Kai Chen](#Kai-Chen)
- [Kaiming He](#Kaiming-He)
- [Karthik Narasimhan](#Karthik-Narasimhan)
- [Karthik Raman](#Karthik-Raman)
- [Ke Li](#Ke-Li)
- [Kenton Lee](#Kenton-Lee)
- [Krishna Srinivasan](#Krishna-Srinivasan)
- [Kristen Grauman](#Kristen-Grauman)
- [Kristina Toutanova](#Kristina-Toutanova)
- [Larry Davis](#Larry-Davis)
- [Laurent Sifre](#Laurent-Sifre)
- [Le Hou](#Le-Hou)
- [Leonard Adolphs](#Leonard-Adolphs)
- [Li Fei-Fei](#Li-Fei-Fei)
- [Li Yang](#Li-Yang)
- [Long Ouyang](#Long-Ouyang)
- [Lucas Beyer](#Lucas-Beyer)
- [Ludovic Denoyer](#Ludovic-Denoyer)
- [Lukasz Kaiser](#Lukasz-Kaiser)
- [Luke Metz](#Luke-Metz)
- [Luming Tang](#Luming-Tang)
- [Léon Bottou](#Léon-Bottou)
- [Maarten Sap](#Maarten-Sap)
- [Mandar Joshi](#Mandar-Joshi)
- [Marc'Aurelio Ranzato](#Marc'Aurelio-Ranzato)
- [Mario Fritz](#Mario-Fritz)
- [Martin Arjovsky](#Martin-Arjovsky)
- [Masanori Koyama](#Masanori-Koyama)
- [Mateusz Koziński](#Mateusz-Koziński)
- [Matthew D. Zeiler](#Matthew-D.-Zeiler)
- [Matthias Bethge](#Matthias-Bethge)
- [Mengli Cheng](#Mengli-Cheng)
- [Menglin Jia](#Menglin-Jia)
- [Menglong Zhu](#Menglong-Zhu)
- [Michael Carbin](#Michael-Carbin)
- [Michelle Chen Huebscher](#Michelle-Chen-Huebscher)
- [Mike Lewis](#Mike-Lewis)
- [Mikhail Pavlov](#Mikhail-Pavlov)
- [Ming-Wei Chang](#Ming-Wei-Chang)
- [Minghui Qiu](#Minghui-Qiu)
- [Minje Choi](#Minje-Choi)
- [Minjun Li](#Minjun-Li)
- [Mohit Bansal](#Mohit-Bansal)
- [Munyoung Kim](#Munyoung-Kim)
- [Myle Ott](#Myle-Ott)
- [Nan Du](#Nan-Du)
- [Neil Houlsby](#Neil-Houlsby)
- [Nicholas Frosst](#Nicholas-Frosst)
- [Nikita Kitaev](#Nikita-Kitaev)
- [Ning Yu](#Ning-Yu)
- [Noah A. Smith](#Noah-A.-Smith)
- [Noam Shazeer](#Noam-Shazeer)
- [Oriol Vinyals](#Oriol-Vinyals)
- [Other](#Other)
- [Pablo Márquez-Neila](#Pablo-Márquez-Neila)
- [Pamela Mishkin](#Pamela-Mishkin)
- [Pascal Fua](#Pascal-Fua)
- [Paul Christiano](#Paul-Christiano)
- [Peng Zhou](#Peng-Zhou)
- [Peter J. Liu](#Peter-J.-Liu)
- [Peter W. Battaglia](#Peter-W.-Battaglia)
- [Peter West](#Peter-West)
- [Peter Wonka](#Peter-Wonka)
- [Phillip Isola](#Phillip-Isola)
- [Ping Yu](#Ping-Yu)
- [Preetum Nakkiran](#Preetum-Nakkiran)
- [Qifan Wang](#Qifan-Wang)
- [Quoc V. Le](#Quoc-V.-Le)
- [Rameen Abdal](#Rameen-Abdal)
- [Rayat Hossain](#Rayat-Hossain)
- [Rob Fergus](#Rob-Fergus)
- [Ronan Le Bras](#Ronan-Le-Bras)
- [Rosanne Liu](#Rosanne-Liu)
- [Ryan Lowe](#Ryan-Lowe)
- [Samuli Laine](#Samuli-Laine)
- [Santhosh K. Ramakrishnan](#Santhosh-K.-Ramakrishnan)
- [Sara Sabour](#Sara-Sabour)
- [Scott Gray](#Scott-Gray)
- [Sebastian Borgeaud](#Sebastian-Borgeaud)
- [Serge Belongie](#Serge-Belongie)
- [Shayne Longpre](#Shayne-Longpre)
- [Shibani Santurkar](#Shibani-Santurkar)
- [Shiyu Chang](#Shiyu-Chang)
- [Shunyu Yao](#Shunyu-Yao)
- [Song Han](#Song-Han)
- [Soumith Chintala](#Soumith-Chintala)
- [Sowmya Yellapragada](#Sowmya-Yellapragada)
- [Stanislas Polu](#Stanislas-Polu)
- [Stephen Merity](#Stephen-Merity)
- [Sudharshan Chandra Babu](#Sudharshan-Chandra-Babu)
- [Sunghun Kim](#Sunghun-Kim)
- [Taesung Park](#Taesung-Park)
- [Takeru Miyato](#Takeru-Miyato)
- [Takeshi Kojima](#Takeshi-Kojima)
- [Tal Ridnik](#Tal-Ridnik)
- [Tero Karras](#Tero-Karras)
- [Thomas Schumm](#Thomas-Schumm)
- [Thomas Unterthiner](#Thomas-Unterthiner)
- [Tim Salimans](#Tim-Salimans)
- [Timo Aila](#Timo-Aila)
- [Timothy Dozat](#Timothy-Dozat)
- [Tinghui Zhou](#Tinghui-Zhou)
- [Tomas Mikolov](#Tomas-Mikolov)
- [Tong He](#Tong-He)
- [Toshiki Kataoka](#Toshiki-Kataoka)
- [Tristan Yang](#Tristan-Yang)
- [Vicki Cheung](#Vicki-Cheung)
- [Wei Lin](#Wei-Lin)
- [Wei Liu](#Wei-Liu)
- [Wieland Brendel](#Wieland-Brendel)
- [Wojciech Zaremba](#Wojciech-Zaremba)
- [Xi Chen](#Xi-Chen)
- [Xian Li](#Xian-Li)
- [Xiangyu Zhang](#Xiangyu-Zhang)
- [Xiaofei Sun](#Xiaofei-Sun)
- [Xing Shi](#Xing-Shi)
- [Xu Jiang](#Xu-Jiang)
- [Xuezhi Wang](#Xuezhi-Wang)
- [Xun Huang](#Xun-Huang)
- [Yamini Bansal](#Yamini-Bansal)
- [Yanghua Jin](#Yanghua-Jin)
- [Yanqi Zhou](#Yanqi-Zhou)
- [Yejin Choi](#Yejin-Choi)
- [Ygor Rebouças Serpa](#Ygor-Rebouças-Serpa)
- [Yifan Jiang](#Yifan-Jiang)
- [Yin Cui](#Yin-Cui)
- [Yinfei Yang](#Yinfei-Yang)
- [Yipeng Qin](#Yipeng-Qin)
- [Yuan Cao](#Yuan-Cao)
- [Yuichi Yoshida](#Yuichi-Yoshida)
- [Yunjey Choi](#Yunjey-Choi)
- [Yuntao Ba](#Yuntao-Ba)
- [Yusuke Iwasawa](#Yusuke-Iwasawa)
- [Yuxin Wu](#Yuxin-Wu)
- [Zhangyang Wang](#Zhangyang-Wang)
- [Zhen Tan](#Zhen-Tan)
- [Zhi Zhang](#Zhi-Zhang)
- [Ziad Al-Halah](#Ziad-Al-Halah)
- [Łukasz Kaiser](#Łukasz-Kaiser)

---

## Adam Paszke

|   | Paper Name                                                                                                                                             | Status | Topic             | Category   | Year | Conference | Author                                             | Summary                                                                                                                                     | Link                                                          |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------ | ------ | ----------------- | ---------- | ---- | ---------- | -------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------- |
| 0 | [Evaluation of neural network architectures for embedded systems](Research_Papers_Anubhav_Reads/Evaluation_of_neural_network_architectures_for_emb.md) | Read   | CNNs, CV , Image  | Comparison | 2017 | IEEE ISCAS | Adam Paszke, Alfredo Canziani, Eugenio Culurciello | Compare CNN classification architectures on accuracy, memory footprint, parameters, operations count, inference time and power consumption. | [link](https://ieeexplore.ieee.org/abstract/document/8050276) |


---

## Aditya Ramesh

|   | Paper Name                                                                                             | Status  | Topic                       | Category | Year | Conference | Author                                                                 | Summary | Link                                    |
| - | ------------------------------------------------------------------------------------------------------ | ------- | --------------------------- | -------- | ---- | ---------- | ---------------------------------------------------------------------- | ------- | --------------------------------------- |
| 0 | [DALL·E: Creating Images from Text](Research_Papers_Anubhav_Reads/DALL·E_Creating_Images_from_Text.md) | Pending | Image , Text , Transformers |          | 2021 | Blog       | Aditya Ramesh, Gabriel Goh, Ilya Sutskever, Mikhail Pavlov, Scott Gray |         | [link](https://openai.com/blog/dall-e/) |


---

## Agata Mosinska

|   | Paper Name                                                                                                                                                         | Status  | Topic                               | Category | Year | Conference | Author                                                            | Summary | Link                                                                                                             |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------- | ----------------------------------- | -------- | ---- | ---------- | ----------------------------------------------------------------- | ------- | ---------------------------------------------------------------------------------------------------------------- |
| 0 | [Topological Loss: Beyond the Pixel-Wise Loss for Topology-Aware Delineation](Research_Papers_Anubhav_Reads/Topological_Loss_Beyond_the_Pixel-Wise_Loss_for_To.md) | Pending | Image , Loss Function, Segmentation |          | 2018 | CVPR       | Agata Mosinska, Mateusz Koziński, Pablo Márquez-Neila, Pascal Fua |         | [link](https://openaccess.thecvf.com/content_cvpr_2018/html/Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper.html) |


---

## Alec Radford

|   | Paper Name                                                                                                                                        | Status  | Topic                          | Category                 | Year | Conference | Author                                                                              | Summary | Link                                                                                                                                                                         |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------------------------ | ------------------------ | ---- | ---------- | ----------------------------------------------------------------------------------- | ------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 0 | [GPT-2 (Language Models are Unsupervised Multitask Learners)](Research_Papers_Anubhav_Reads/GPT-2_Language_Models_are_Unsupervised_Multitask_.md) | Pending | Attention, Text , Transformers |                          | 2019 |            | Alec Radford, Dario Amodei, Ilya Sutskever, Jeffrey Wu                              |         | [link](https://www.ceid.upatras.gr/webpages/faculty/zaro/teaching/alg-ds/PRESENTATIONS/PAPERS/2019-Radford-et-al_Language-Models-Are-Unsupervised-Multitask-%20Learners.pdf) |
| 1 | [Improved Techniques for Training GANs](Research_Papers_Anubhav_Reads/Improved_Techniques_for_Training_GANs.md)                                   | Pending | GANs, Image                    | Semi-Supervised          | 2016 | NIPS       | Alec Radford, Ian Goodfellow, Tim Salimans, Vicki Cheung, Wojciech Zaremba, Xi Chen |         | [link](http://papers.nips.cc/paper/6124-improved-techniques-for-training-gans)                                                                                               |
| 2 | [CLIP: Connecting Text and Images](Research_Papers_Anubhav_Reads/CLIP_Connecting_Text_and_Images.md)                                              | Pending | Image , Text , Transformers    | Multimodal, Pre-Training | 2021 | arXiv      | Alec Radford, Ilya Sutskever, Jong Wook Kim                                         |         | [link](https://openai.com/blog/clip/)                                                                                                                                        |


---

## Aleksander Madry

|   | Paper Name                                                                                                                         | Status  | Topic              | Category      | Year | Conference | Author                                                              | Summary | Link                                     |
| - | ---------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------------ | ------------- | ---- | ---------- | ------------------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [How Does Batch Normalization Help Optimization?](Research_Papers_Anubhav_Reads/How_Does_Batch_Normalization_Help_Optimization.md) | Pending | NNs, Normalization | Optimizations | 2018 | arXiv      | Aleksander Madry, Andrew Ilyas, Dimitris Tsipras, Shibani Santurkar |         | [link](https://arxiv.org/abs/1805.11604) |


---

## Alexandre Alahi

|   | Paper Name                                                                                                                                                 | Status  | Topic              | Category | Year | Conference | Author                                      | Summary | Link                                     |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------------ | -------- | ---- | ---------- | ------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](Research_Papers_Anubhav_Reads/Perceptual_Losses_for_Real-Time_Style_Transfer_and.md) | Pending | Loss Function, NNs |          | 2016 | ECCV       | Alexandre Alahi, Justin Johnson, Li Fei-Fei |         | [link](https://arxiv.org/abs/1603.08155) |


---

## Alexei A. Efros

|   | Paper Name                                                                                                                                                                       | Status  | Topic        | Category     | Year | Conference | Author                                                    | Summary                                                                                                  | Link                                                                                                                      |
| - | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | ------------ | ---- | ---------- | --------------------------------------------------------- | -------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------- |
| 0 | [Pix2Pix: Image-to-Image Translation with Conditional Adversarial Nets](Research_Papers_Anubhav_Reads/Pix2Pix_Image-to-Image_Translation_with_Conditiona.md)                     | Read    | GANs, Image  |              | 2017 | CVPR       | Alexei A. Efros, Jun-Yan Zhu, Phillip Isola, Tinghui Zhou | Image to image translation using Conditional GANs and dataset of image pairs from one domain to another. | [link](https://arxiv.org/abs/1611.07004)                                                                                  |
| 1 | [CycleGAN: Unpaired Image-To-Image Translation Using Cycle-Consistent Adversarial Networks](Research_Papers_Anubhav_Reads/CycleGAN_Unpaired_Image-To-Image_Translation_Using.md) | Pending | GANs, Image  | Architecture | 2017 | ICCV       | Alexei A. Efros, Jun-Yan Zhu, Phillip Isola, Taesung Park |                                                                                                          | [link](https://openaccess.thecvf.com/content_iccv_2017/html/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.html) |


---

## Alexey Dosovitskiy

|   | Paper Name                                                                                                                                                                            | Status  | Topic                           | Category | Year | Conference | Author                                                         | Summary | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------------------------- | -------- | ---- | ---------- | -------------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Vision Transformer: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](Research_Papers_Anubhav_Reads/Vision_Transformer_An_Image_is_Worth_16x16_Words_T.md) | Pending | Attention, Image , Transformers |          | 2021 | ICLR       | Alexey Dosovitskiy, Jakob Uszkoreit, Lucas Beyer, Neil Houlsby |         | [link](https://arxiv.org/abs/2010.11929) |


---

## Alexis Conneau

|   | Paper Name                                                                                                                                             | Status  | Topic                          | Category     | Year | Conference | Author                                                                            | Summary | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------ | ------- | ------------------------------ | ------------ | ---- | ---------- | --------------------------------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Phrase-Based & Neural Unsupervised Machine Translation](Research_Papers_Anubhav_Reads/Phrase-Based_&_Neural_Unsupervised_Machine_Transla.md)          | Pending | NMT, Text , Transformers       | Unsupervised | 2018 | arXiv      | Alexis Conneau, Guillaume Lample, Ludovic Denoyer, Marc'Aurelio Ranzato, Myle Ott |         | [link](https://arxiv.org/abs/1804.07755) |
| 1 | [Unsupervised Machine Translation Using Monolingual Corpora Only](Research_Papers_Anubhav_Reads/Unsupervised_Machine_Translation_Using_Monolingual.md) | Pending | GANs, NMT, Text , Transformers | Unsupervised | 2017 | arXiv      | Alexis Conneau, Guillaume Lample, Ludovic Denoyer, Marc'Aurelio Ranzato, Myle Ott |         | [link](https://arxiv.org/abs/1711.00043) |
| 2 | [Cross-lingual Language Model Pretraining](Research_Papers_Anubhav_Reads/Cross-lingual_Language_Model_Pretraining.md)                                  | Pending | NMT, Text , Transformers       | Unsupervised | 2019 | arXiv      | Alexis Conneau, Guillaume Lample                                                  |         | [link](https://arxiv.org/abs/1901.07291) |


---

## Alfredo Canziani

|   | Paper Name                                                                                                                                             | Status | Topic             | Category   | Year | Conference | Author                                             | Summary                                                                                                                                     | Link                                                          |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------ | ------ | ----------------- | ---------- | ---- | ---------- | -------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------- |
| 0 | [Evaluation of neural network architectures for embedded systems](Research_Papers_Anubhav_Reads/Evaluation_of_neural_network_architectures_for_emb.md) | Read   | CNNs, CV , Image  | Comparison | 2017 | IEEE ISCAS | Adam Paszke, Alfredo Canziani, Eugenio Culurciello | Compare CNN classification architectures on accuracy, memory footprint, parameters, operations count, inference time and power consumption. | [link](https://ieeexplore.ieee.org/abstract/document/8050276) |


---

## Ali Farhadi

|   | Paper Name                                                                                                                                               | Status  | Topic                                     | Category | Year | Conference | Author                                                      | Summary | Link                                                                           |
| - | -------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----------------------------------------- | -------- | ---- | ---------- | ----------------------------------------------------------- | ------- | ------------------------------------------------------------------------------ |
| 0 | [VisualCOMET: Reasoning about the Dynamic Context of a Still Image](Research_Papers_Anubhav_Reads/VisualCOMET_Reasoning_about_the_Dynamic_Context_of.md) | Pending | AGI, Dataset, Image , Text , Transformers |          | 2020 | ECCV       | Ali Farhadi, Chandra Bhagavatula, Jae Sung Park, Yejin Choi |         | [link](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500494.pdf) |


---

## Alimohammad Beigi

|   | Paper Name                                                                                                                                 | Status    | Topic                                      | Category                 | Year | Conference | Author                      | Summary | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------ | --------- | ------------------------------------------ | ------------------------ | ---- | ---------- | --------------------------- | ------- | ---------------------------------------- |
| 0 | [Large Language Models for Data Annotation: A Survey](Research_Papers_Anubhav_Reads/Large_Language_Models_for_Data_Annotation_A_Survey.md) | This week | Dataset, Generative, Large-Language-Models | Prompting, Tips & Tricks | 2024 | arXiv      | Alimohammad Beigi, Zhen Tan |         | [link](https://arxiv.org/abs/2402.13446) |


---

## Anant Jain

|   | Paper Name                                                                                                                              | Status  | Topic        | Category    | Year | Conference | Author     | Summary | Link                                                                                                  |
| - | --------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | ----------- | ---- | ---------- | ---------- | ------- | ----------------------------------------------------------------------------------------------------- |
| 0 | [Breaking neural networks with adversarial attacks](Research_Papers_Anubhav_Reads/Breaking_neural_networks_with_adversarial_attacks.md) | Pending | CNNs, Image  | Adversarial | 2019 | Blog       | Anant Jain |         | [link](https://towardsdatascience.com/breaking-neural-networks-with-adversarial-attacks-f4290a9a45aa) |


---

## Andreas Mayr

|   | Paper Name                                                                                            | Status  | Topic                        | Category                     | Year | Conference | Author                                             | Summary | Link                                                                           |
| - | ----------------------------------------------------------------------------------------------------- | ------- | ---------------------------- | ---------------------------- | ---- | ---------- | -------------------------------------------------- | ------- | ------------------------------------------------------------------------------ |
| 0 | [Self-Normalizing Neural Networks](Research_Papers_Anubhav_Reads/Self-Normalizing_Neural_Networks.md) | Pending | Activation Function, Tabular | Optimizations, Tips & Tricks | 2017 | NIPS       | Andreas Mayr, Günter Klambauer, Thomas Unterthiner |         | [link](https://papers.nips.cc/paper/6698-self-normalizing-neural-networks.pdf) |


---

## Andrew G. Howard

|   | Paper Name                                                                                                                                                               | Status  | Topic             | Category                                 | Year | Conference | Author                         | Summary | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------- | ----------------- | ---------------------------------------- | ---- | ---------- | ------------------------------ | ------- | ---------------------------------------- |
| 0 | [MobileNet (Efficient Convolutional Neural Networks for Mobile Vision Applications)](Research_Papers_Anubhav_Reads/MobileNet_Efficient_Convolutional_Neural_Networks.md) | Pending | CNNs, CV , Image  | Architecture, Optimization-No. of params | 2017 | arXiv      | Andrew G. Howard, Menglong Zhu |         | [link](https://arxiv.org/abs/1704.04861) |


---

## Andrew Ilyas

|   | Paper Name                                                                                                                         | Status  | Topic              | Category      | Year | Conference | Author                                                              | Summary | Link                                     |
| - | ---------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------------ | ------------- | ---- | ---------- | ------------------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [How Does Batch Normalization Help Optimization?](Research_Papers_Anubhav_Reads/How_Does_Batch_Normalization_Help_Optimization.md) | Pending | NNs, Normalization | Optimizations | 2018 | arXiv      | Aleksander Madry, Andrew Ilyas, Dimitris Tsipras, Shibani Santurkar |         | [link](https://arxiv.org/abs/1805.11604) |


---

## Anselm Levskaya

|   | Paper Name                                                                                                 | Status | Topic                          | Category                                                      | Year | Conference | Author                                        | Summary                                                                                                                 | Link                                     |
| - | ---------------------------------------------------------------------------------------------------------- | ------ | ------------------------------ | ------------------------------------------------------------- | ---- | ---------- | --------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [Reformer: The Efficient Transformer](Research_Papers_Anubhav_Reads/Reformer_The_Efficient_Transformer.md) | Read   | Attention, Text , Transformers | Architecture, Optimization-Memory, Optimization-No. of params | 2020 | arXiv      | Anselm Levskaya, Lukasz Kaiser, Nikita Kitaev | Overcome time and memory complexity of Transformers by bucketing Query, Keys and using Reversible residual connections. | [link](https://arxiv.org/abs/2001.04451) |


---

## Antoine Bosselut

|   | Paper Name                                                                                                                                                        | Status  | Topic                    | Category | Year | Conference | Author                                       | Summary | Link                                         |
| - | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------------------ | -------- | ---- | ---------- | -------------------------------------------- | ------- | -------------------------------------------- |
| 0 | [COMET: Commonsense Transformers for Automatic Knowledge Graph Construction](Research_Papers_Anubhav_Reads/COMET_Commonsense_Transformers_for_Automatic_Knowl.md) | Pending | AGI, Text , Transformers |          | 2019 | ACL        | Antoine Bosselut, Hannah Rashkin, Yejin Choi |         | [link](https://arxiv.org/pdf/1906.05317.pdf) |


---

## Ari S. Morcos

|   | Paper Name                                                                                                                                                                       | Status  | Topic        | Category | Year | Conference | Author                                           | Summary | Link                                     |
| - | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | -------- | ---- | ---------- | ------------------------------------------------ | ------- | ---------------------------------------- |
| 0 | [Training BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs](Research_Papers_Anubhav_Reads/Training_BatchNorm_and_Only_BatchNorm_On_the_Expre.md) | Pending | CNNs, Image  |          | 2020 | arXiv      | Ari S. Morcos, David J. Schwab, Jonathan Frankle |         | [link](https://arxiv.org/abs/2003.00152) |


---

## Ashish Vaswani

|   | Paper Name                                                                              | Status | Topic                          | Category     | Year | Conference | Author                                                        | Summary                                                                                       | Link                                                               |
| - | --------------------------------------------------------------------------------------- | ------ | ------------------------------ | ------------ | ---- | ---------- | ------------------------------------------------------------- | --------------------------------------------------------------------------------------------- | ------------------------------------------------------------------ |
| 0 | [Attention is All you Need](Research_Papers_Anubhav_Reads/Attention_is_All_you_Need.md) | Read   | Attention, Text , Transformers | Architecture | 2017 | NIPS       | Ashish Vaswani, Illia Polosukhin, Noam Shazeer, Łukasz Kaiser | Talks about Transformer architecture which brings SOTA performance for different tasks in NLP | [link](http://papers.nips.cc/paper/7181-attention-is-all-you-need) |


---

## Asim Kadav

|   | Paper Name                                                                                                        | Status  | Topic             | Category                   | Year | Conference | Author             | Summary | Link                                     |
| - | ----------------------------------------------------------------------------------------------------------------- | ------- | ----------------- | -------------------------- | ---- | ---------- | ------------------ | ------- | ---------------------------------------- |
| 0 | [Pruning Filters for Efficient ConvNets](Research_Papers_Anubhav_Reads/Pruning_Filters_for_Efficient_ConvNets.md) | Pending | CNNs, CV , Image  | Optimization-No. of params | 2017 | arXiv      | Asim Kadav, Hao Li |         | [link](https://arxiv.org/abs/1608.08710) |


---

## Augustus Odena

|   | Paper Name                                                                                                                                   | Status  | Topic                   | Category     | Year | Conference | Author                                                      | Summary | Link                                     |
| - | -------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----------------------- | ------------ | ---- | ---------- | ----------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [SAGAN: Self-Attention Generative Adversarial Networks](Research_Papers_Anubhav_Reads/SAGAN_Self-Attention_Generative_Adversarial_Networ.md) | Pending | Attention, GANs, Image  | Architecture | 2018 | arXiv      | Augustus Odena, Dimitris Metaxas, Han Zhang, Ian Goodfellow |         | [link](https://arxiv.org/abs/1805.08318) |


---

## Bharath Hariharan

|   | Paper Name                                                                                                                                                 | Status  | Topic        | Category          | Year | Conference | Author                                           | Summary | Link                                                                                                                                                        |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | ----------------- | ---- | ---------- | ------------------------------------------------ | ------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 0 | [Few-Shot Learning with Localization in Realistic Settings](Research_Papers_Anubhav_Reads/Few-Shot_Learning_with_Localization_in_Realistic_S.md)           | Pending | CNNs, Image  | Few-shot-learning | 2019 | CVPR       | Bharath Hariharan, Davis Wertheimer              |         | [link](https://openaccess.thecvf.com/content_CVPR_2019/papers/Wertheimer_Few-Shot_Learning_With_Localization_in_Realistic_Settings_CVPR_2019_paper.pdf)     |
| 1 | [Revisiting Pose-Normalization for Fine-Grained Few-Shot Recognition](Research_Papers_Anubhav_Reads/Revisiting_Pose-Normalization_for_Fine-Grained_Few.md) | Pending | CNNs, Image  | Few-shot-learning | 2020 | CVPR       | Bharath Hariharan, Davis Wertheimer, Luming Tang |         | [link](https://openaccess.thecvf.com/content_CVPR_2020/papers/Tang_Revisiting_Pose-Normalization_for_Fine-Grained_Few-Shot_Recognition_CVPR_2020_paper.pdf) |


---

## Boaz Barak

|   | Paper Name                                                                                                                                         | Status  | Topic | Category | Year | Conference | Author                                                                                | Summary | Link                                     |
| - | -------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----- | -------- | ---- | ---------- | ------------------------------------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Deep Double Descent: Where Bigger Models and More Data Hurt](Research_Papers_Anubhav_Reads/Deep_Double_Descent_Where_Bigger_Models_and_More_D.md) | Pending | NNs   |          | 2019 | arXiv      | Boaz Barak, Gal Kaplun, Ilya Sutskever, Preetum Nakkiran, Tristan Yang, Yamini Bansal |         | [link](https://arxiv.org/abs/1912.02292) |


---

## Carroll L. Wainwright

|   | Paper Name                                                                                                                                                 | Status  | Topic                                              | Category                                                        | Year | Conference | Author                                                                                                                      | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Link                                         |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | -------------------------------------------------- | --------------------------------------------------------------- | ---- | ---------- | --------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |
| 0 | [Training language models to follow instructions with human feedback](Research_Papers_Anubhav_Reads/Training_language_models_to_follow_instructions_wi.md) | Pending | Generative, Large-Language-Models, Training Method | Instruction-Finetuning, Reinforcement-Learning, Semi-Supervised | 2022 | arXiv      | Carroll L. Wainwright, Diogo Almeida, Jan Leike, Jeff Wu, Long Ouyang, Pamela Mishkin, Paul Christiano, Ryan Lowe, Xu Jiang | This paper presents InstructGPT, a model fine-tuned with human feedback to better align with user intent across various tasks. Despite having significantly fewer parameters than larger models, InstructGPT outperforms them in human evaluations, demonstrating improved truthfulness, reduced toxicity, and minimal performance regressions on public NLP datasets, highlighting the potential of fine-tuning with human feedback for enhancing language model alignment with human intent. | [link](https://arxiv.org/pdf/2203.02155.pdf) |


---

## Chandra Bhagavatula

|   | Paper Name                                                                                                                                                                 | Status  | Topic                                     | Category                     | Year | Conference | Author                                                      | Summary | Link                                                                           |
| - | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----------------------------------------- | ---------------------------- | ---- | ---------- | ----------------------------------------------------------- | ------- | ------------------------------------------------------------------------------ |
| 0 | [VisualCOMET: Reasoning about the Dynamic Context of a Still Image](Research_Papers_Anubhav_Reads/VisualCOMET_Reasoning_about_the_Dynamic_Context_of.md)                   | Pending | AGI, Dataset, Image , Text , Transformers |                              | 2020 | ECCV       | Ali Farhadi, Chandra Bhagavatula, Jae Sung Park, Yejin Choi |         | [link](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500494.pdf) |
| 1 | [Symbolic Knowledge Distillation: from General Language Models to Commonsense Models](Research_Papers_Anubhav_Reads/Symbolic_Knowledge_Distillation_from_General_Langu.md) | Pending | Dataset, Text , Transformers              | Optimizations, Tips & Tricks | 2021 | arXiv      | Chandra Bhagavatula, Jack Hessel, Peter West, Yejin Choi    |         | [link](https://arxiv.org/abs/2110.07178)                                       |


---

## Christian Buck

|   | Paper Name                                                                                                                                             | Status  | Topic | Category                 | Year | Conference | Author                                                   | Summary | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------ | ------- | ----- | ------------------------ | ---- | ---------- | -------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Decoding a Neural Retriever’s Latent Space for Query Suggestion](Research_Papers_Anubhav_Reads/Decoding_a_Neural_Retriever’s_Latent_Space_for_Que.md) | Pending | Text  | Embeddings, Latent space | 2022 | arXiv      | Christian Buck, Leonard Adolphs, Michelle Chen Huebscher |         | [link](https://arxiv.org/abs/2210.12084) |


---

## Christian Szegedy

|   | Paper Name                                                                                                                    | Status | Topic             | Category     | Year | Conference | Author                     | Summary                                                                                           | Link                                    |
| - | ----------------------------------------------------------------------------------------------------------------------------- | ------ | ----------------- | ------------ | ---- | ---------- | -------------------------- | ------------------------------------------------------------------------------------------------- | --------------------------------------- |
| 0 | [Inception-v1 (Going Deeper With Convolutions)](Research_Papers_Anubhav_Reads/Inception-v1_Going_Deeper_With_Convolutions.md) | Read   | CNNs, CV , Image  | Architecture | 2015 | CVPR       | Christian Szegedy, Wei Liu | Propose the use of 1x1 conv operations to reduce the number of parameters in a deep and wide CNN  | [link](https://arxiv.org/abs/1409.4842) |


---

## Colin Raffel

|   | Paper Name                                                                                                                                                                   | Status | Topic                          | Category | Year | Conference | Author                                                        | Summary                                                                                                                                                                                                             | Link                                     |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------ | ------------------------------ | -------- | ---- | ---------- | ------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](Research_Papers_Anubhav_Reads/T5_Exploring_the_Limits_of_Transfer_Learning_with_.md) | Read   | Attention, Text , Transformers |          | 2020 | JMLR       | Colin Raffel, Noam Shazeer, Peter J. Liu, Wei Liu, Yanqi Zhou | Presents a Text-to-Text transformer model with multi-task learning capabilities, simultaneously solving problems such as machine translation, document summarization, question answering, and classification tasks. | [link](https://arxiv.org/abs/1910.10683) |


---

## Danqi Chen

|   | Paper Name                                                                                                                                                   | Status | Topic                                   | Category     | Year | Conference | Author                   | Summary                                                                                              | Link                                                    |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------ | --------------------------------------- | ------------ | ---- | ---------- | ------------------------ | ---------------------------------------------------------------------------------------------------- | ------------------------------------------------------- |
| 0 | [SpanBERT: Improving Pre-training by Representing and Predicting Spans](Research_Papers_Anubhav_Reads/SpanBERT_Improving_Pre-training_by_Representing_an.md) | Read   | Question-Answering, Text , Transformers | Pre-Training | 2020 | TACL       | Danqi Chen, Mandar Joshi | A different pre-training strategy for BERT model to improve performance for Question Answering task. | [link](https://www.aclweb.org/anthology/2020.tacl-1.5/) |


---

## Dario Amodei

|   | Paper Name                                                                                                                                        | Status  | Topic                          | Category | Year | Conference | Author                                                 | Summary | Link                                                                                                                                                                         |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------------------------ | -------- | ---- | ---------- | ------------------------------------------------------ | ------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 0 | [GPT-2 (Language Models are Unsupervised Multitask Learners)](Research_Papers_Anubhav_Reads/GPT-2_Language_Models_are_Unsupervised_Multitask_.md) | Pending | Attention, Text , Transformers |          | 2019 |            | Alec Radford, Dario Amodei, Ilya Sutskever, Jeffrey Wu |         | [link](https://www.ceid.upatras.gr/webpages/faculty/zaro/teaching/alg-ds/PRESENTATIONS/PAPERS/2019-Radford-et-al_Language-Models-Are-Unsupervised-Multitask-%20Learners.pdf) |


---

## David Berthelot

|   | Paper Name                                                                                                                                         | Status  | Topic        | Category | Year | Conference | Author                                    | Summary | Link                                     |
| - | -------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | -------- | ---- | ---------- | ----------------------------------------- | ------- | ---------------------------------------- |
| 0 | [BEGAN: Boundary Equilibrium Generative Adversarial Networks](Research_Papers_Anubhav_Reads/BEGAN_Boundary_Equilibrium_Generative_Adversarial_.md) | Pending | GANs, Image  |          | 2017 | arXiv      | David Berthelot, Luke Metz, Thomas Schumm |         | [link](https://arxiv.org/abs/1703.10717) |


---

## David J. Schwab

|   | Paper Name                                                                                                                                                                       | Status  | Topic        | Category | Year | Conference | Author                                           | Summary | Link                                     |
| - | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | -------- | ---- | ---------- | ------------------------------------------------ | ------- | ---------------------------------------- |
| 0 | [Training BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs](Research_Papers_Anubhav_Reads/Training_BatchNorm_and_Only_BatchNorm_On_the_Expre.md) | Pending | CNNs, Image  |          | 2020 | arXiv      | Ari S. Morcos, David J. Schwab, Jonathan Frankle |         | [link](https://arxiv.org/abs/2003.00152) |


---

## David Silver

|   | Paper Name                                                                                                                                        | Status  | Topic | Category               | Year | Conference | Author                                                                | Summary | Link                                                                                              |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----- | ---------------------- | ---- | ---------- | --------------------------------------------------------------------- | ------- | ------------------------------------------------------------------------------------------------- |
| 0 | [MuZero: Mastering Go, chess, shogi and Atari without rules](Research_Papers_Anubhav_Reads/MuZero_Mastering_Go,_chess,_shogi_and_Atari_withou.md) | Pending |       | Reinforcement-Learning | 2020 | Nature     | David Silver, Demis Hassabis, Ioannis Antonoglou, Julian Schrittwiese |         | [link](https://deepmind.com/blog/article/muzero-mastering-go-chess-shogi-and-atari-without-rules) |


---

## Davis Wertheimer

|   | Paper Name                                                                                                                                                 | Status  | Topic        | Category          | Year | Conference | Author                                           | Summary | Link                                                                                                                                                        |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | ----------------- | ---- | ---------- | ------------------------------------------------ | ------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 0 | [Few-Shot Learning with Localization in Realistic Settings](Research_Papers_Anubhav_Reads/Few-Shot_Learning_with_Localization_in_Realistic_S.md)           | Pending | CNNs, Image  | Few-shot-learning | 2019 | CVPR       | Bharath Hariharan, Davis Wertheimer              |         | [link](https://openaccess.thecvf.com/content_CVPR_2019/papers/Wertheimer_Few-Shot_Learning_With_Localization_in_Realistic_Settings_CVPR_2019_paper.pdf)     |
| 1 | [Revisiting Pose-Normalization for Fine-Grained Few-Shot Recognition](Research_Papers_Anubhav_Reads/Revisiting_Pose-Normalization_for_Fine-Grained_Few.md) | Pending | CNNs, Image  | Few-shot-learning | 2020 | CVPR       | Bharath Hariharan, Davis Wertheimer, Luming Tang |         | [link](https://openaccess.thecvf.com/content_CVPR_2020/papers/Tang_Revisiting_Pose-Normalization_for_Fine-Grained_Few-Shot_Recognition_CVPR_2020_paper.pdf) |


---

## Dedy Kredo

|   | Paper Name                                                                                                                                                           | Status  | Topic                 | Category                 | Year | Conference | Author                                  | Summary                                                                                                                                                                                      | Link                                     |
| - | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | --------------------- | ------------------------ | ---- | ---------- | --------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering](Research_Papers_Anubhav_Reads/Code_Generation_with_AlphaCodium_From_Prompt_Engin.md) | Pending | Large-Language-Models | Prompting, Tips & Tricks | 2024 | arXiv      | Dedy Kredo, Itamar Friedman, Tal Ridnik | This paper introduces AlphaCodium, a novel test-based, multi-stage, code-oriented iterative approach for improving the performance of Language Model Models (LLMs) on code generation tasks. | [link](https://arxiv.org/abs/2401.08500) |


---

## Demis Hassabis

|   | Paper Name                                                                                                                                        | Status  | Topic | Category               | Year | Conference | Author                                                                | Summary | Link                                                                                              |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----- | ---------------------- | ---- | ---------- | --------------------------------------------------------------------- | ------- | ------------------------------------------------------------------------------------------------- |
| 0 | [MuZero: Mastering Go, chess, shogi and Atari without rules](Research_Papers_Anubhav_Reads/MuZero_Mastering_Go,_chess,_shogi_and_Atari_withou.md) | Pending |       | Reinforcement-Learning | 2020 | Nature     | David Silver, Demis Hassabis, Ioannis Antonoglou, Julian Schrittwiese |         | [link](https://deepmind.com/blog/article/muzero-mastering-go-chess-shogi-and-atari-without-rules) |


---

## Denny Zhou

|   | Paper Name                                                                                                                                                   | Status  | Topic                                   | Category | Year | Conference | Author                             | Summary | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------- | --------------------------------------- | -------- | ---- | ---------- | ---------------------------------- | ------- | ---------------------------------------- |
| 0 | [Chain of Thought Prompting Elicits Reasoning in Large Language Models](Research_Papers_Anubhav_Reads/Chain_of_Thought_Prompting_Elicits_Reasoning_in_La.md) | Pending | Question-Answering, Text , Transformers |          | 2022 | arXiv      | Denny Zhou, Jason Wei, Xuezhi Wang |         | [link](https://arxiv.org/abs/2201.11903) |


---

## Dian Yu

|   | Paper Name                                                                                                                                        | Status  | Topic                                    | Category                     | Year | Conference | Author                                                                                 | Summary                                                                                                                                                                                                                                                                                                                                                  | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ---------------------------------------- | ---------------------------- | ---- | ---------- | -------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [ReAct: Synergizing Reasoning and Acting in Language Models](Research_Papers_Anubhav_Reads/ReAct_Synergizing_Reasoning_and_Acting_in_Language.md) | Pending | Generative, Large-Language-Models, Text  | Optimizations, Tips & Tricks | 2023 | ICLR       | Dian Yu, Izhak Shafran, Jeffrey Zhao, Karthik Narasimhan, Nan Du, Shunyu Yao, Yuan Cao | This paper introduces ReAct, a novel approach that leverages Large Language Models (LLMs) to interleave reasoning traces and task-specific actions. ReAct outperforms existing methods on various language and decision-making tasks, addressing issues like hallucination, error propagation, and improving human interpretability and trustworthiness. | [link](https://arxiv.org/abs/2210.03629) |


---

## Diederik P. Kingma

|   | Paper Name                                                                                                               | Status  | Topic           | Category | Year | Conference | Author                       | Summary | Link                                    |
| - | ------------------------------------------------------------------------------------------------------------------------ | ------- | --------------- | -------- | ---- | ---------- | ---------------------------- | ------- | --------------------------------------- |
| 0 | [Adam: A Method for Stochastic Optimization](Research_Papers_Anubhav_Reads/Adam_A_Method_for_Stochastic_Optimization.md) | Pending | NNs, Optimizers |          | 2015 | ICLR       | Diederik P. Kingma, Jimmy Ba |         | [link](https://arxiv.org/abs/1412.6980) |


---

## Dimitris Metaxas

|   | Paper Name                                                                                                                                   | Status  | Topic                   | Category     | Year | Conference | Author                                                      | Summary | Link                                     |
| - | -------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----------------------- | ------------ | ---- | ---------- | ----------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [SAGAN: Self-Attention Generative Adversarial Networks](Research_Papers_Anubhav_Reads/SAGAN_Self-Attention_Generative_Adversarial_Networ.md) | Pending | Attention, GANs, Image  | Architecture | 2018 | arXiv      | Augustus Odena, Dimitris Metaxas, Han Zhang, Ian Goodfellow |         | [link](https://arxiv.org/abs/1805.08318) |


---

## Dimitris Tsipras

|   | Paper Name                                                                                                                         | Status  | Topic              | Category      | Year | Conference | Author                                                              | Summary | Link                                     |
| - | ---------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------------ | ------------- | ---- | ---------- | ------------------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [How Does Batch Normalization Help Optimization?](Research_Papers_Anubhav_Reads/How_Does_Batch_Normalization_Help_Optimization.md) | Pending | NNs, Normalization | Optimizations | 2018 | arXiv      | Aleksander Madry, Andrew Ilyas, Dimitris Tsipras, Shibani Santurkar |         | [link](https://arxiv.org/abs/1805.11604) |


---

## Diogo Almeida

|   | Paper Name                                                                                                                                                 | Status  | Topic                                              | Category                                                        | Year | Conference | Author                                                                                                                      | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Link                                         |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | -------------------------------------------------- | --------------------------------------------------------------- | ---- | ---------- | --------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |
| 0 | [Training language models to follow instructions with human feedback](Research_Papers_Anubhav_Reads/Training_language_models_to_follow_instructions_wi.md) | Pending | Generative, Large-Language-Models, Training Method | Instruction-Finetuning, Reinforcement-Learning, Semi-Supervised | 2022 | arXiv      | Carroll L. Wainwright, Diogo Almeida, Jan Leike, Jeff Wu, Long Ouyang, Pamela Mishkin, Paul Christiano, Ryan Lowe, Xu Jiang | This paper presents InstructGPT, a model fine-tuned with human feedback to better align with user intent across various tasks. Despite having significantly fewer parameters than larger models, InstructGPT outperforms them in human evaluations, demonstrating improved truthfulness, reduced toxicity, and minimal performance regressions on public NLP datasets, highlighting the potential of fine-tuning with human feedback for enhancing language model alignment with human intent. | [link](https://arxiv.org/pdf/2203.02155.pdf) |


---

## Diyi Yang

|   | Paper Name                                                                                                                                                        | Status  | Topic | Category                  | Year | Conference | Author                 | Summary | Link                                     |
| - | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----- | ------------------------- | ---- | ---------- | ---------------------- | ------- | ---------------------------------------- |
| 0 | [Interpreting Deep Learning Models in Natural Language Processing: A Review](Research_Papers_Anubhav_Reads/Interpreting_Deep_Learning_Models_in_Natural_Langu.md) | Pending | Text  | Comparison, Visualization | 2021 | arXiv      | Diyi Yang, Xiaofei Sun |         | [link](https://arxiv.org/abs/2110.10470) |


---

## Dmytro Mishkin

|   | Paper Name                                                                                  | Status  | Topic             | Category      | Year | Conference | Author                     | Summary | Link                                     |
| - | ------------------------------------------------------------------------------------------- | ------- | ----------------- | ------------- | ---- | ---------- | -------------------------- | ------- | ---------------------------------------- |
| 0 | [All you need is a good init](Research_Papers_Anubhav_Reads/All_you_need_is_a_good_init.md) | Pending | NN Initialization | Tips & Tricks | 2015 | arXiv      | Dmytro Mishkin, Jiri Matas |         | [link](https://arxiv.org/abs/1511.06422) |


---

## Eugenio Culurciello

|   | Paper Name                                                                                                                                             | Status | Topic             | Category   | Year | Conference | Author                                             | Summary                                                                                                                                     | Link                                                          |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------ | ------ | ----------------- | ---------- | ---- | ---------- | -------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------- |
| 0 | [Evaluation of neural network architectures for embedded systems](Research_Papers_Anubhav_Reads/Evaluation_of_neural_network_architectures_for_emb.md) | Read   | CNNs, CV , Image  | Comparison | 2017 | IEEE ISCAS | Adam Paszke, Alfredo Canziani, Eugenio Culurciello | Compare CNN classification architectures on accuracy, memory footprint, parameters, operations count, inference time and power consumption. | [link](https://ieeexplore.ieee.org/abstract/document/8050276) |


---

## Fangxiaoyu Feng

|   | Paper Name                                                                                                              | Status | Topic                                           | Category   | Year | Conference | Author                       | Summary                                                                                                                     | Link                                     |
| - | ----------------------------------------------------------------------------------------------------------------------- | ------ | ----------------------------------------------- | ---------- | ---- | ---------- | ---------------------------- | --------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [Language-Agnostic BERT Sentence Embedding](Research_Papers_Anubhav_Reads/Language-Agnostic_BERT_Sentence_Embedding.md) | Read   | Attention, Siamese Network, Text , Transformers | Embeddings | 2020 | arXiv      | Fangxiaoyu Feng, Yinfei Yang | A BERT model with multilingual sentence embeddings learned over 112 languages and Zero-shot learning over unseen languages. | [link](https://arxiv.org/abs/2007.01852) |


---

## Forrest N. Iandola

|   | Paper Name                                                | Status | Topic             | Category                                 | Year | Conference | Author                       | Summary                                                                   | Link                                     |
| - | --------------------------------------------------------- | ------ | ----------------- | ---------------------------------------- | ---- | ---------- | ---------------------------- | ------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [SqueezeNet](Research_Papers_Anubhav_Reads/SqueezeNet.md) | Read   | CNNs, CV , Image  | Architecture, Optimization-No. of params | 2016 | arXiv      | Forrest N. Iandola, Song Han | Explores model compression by using 1x1 convolutions called fire modules. | [link](https://arxiv.org/abs/1602.07360) |


---

## Gabriel Goh

|   | Paper Name                                                                                             | Status  | Topic                       | Category | Year | Conference | Author                                                                 | Summary | Link                                    |
| - | ------------------------------------------------------------------------------------------------------ | ------- | --------------------------- | -------- | ---- | ---------- | ---------------------------------------------------------------------- | ------- | --------------------------------------- |
| 0 | [DALL·E: Creating Images from Text](Research_Papers_Anubhav_Reads/DALL·E_Creating_Images_from_Text.md) | Pending | Image , Text , Transformers |          | 2021 | Blog       | Aditya Ramesh, Gabriel Goh, Ilya Sutskever, Mikhail Pavlov, Scott Gray |         | [link](https://openai.com/blog/dall-e/) |


---

## Gal Kaplun

|   | Paper Name                                                                                                                                         | Status  | Topic | Category | Year | Conference | Author                                                                                | Summary | Link                                     |
| - | -------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----- | -------- | ---- | ---------- | ------------------------------------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Deep Double Descent: Where Bigger Models and More Data Hurt](Research_Papers_Anubhav_Reads/Deep_Double_Descent_Where_Bigger_Models_and_More_D.md) | Pending | NNs   |          | 2019 | arXiv      | Boaz Barak, Gal Kaplun, Ilya Sutskever, Preetum Nakkiran, Tristan Yang, Yamini Bansal |         | [link](https://arxiv.org/abs/1912.02292) |


---

## Geoffrey E Hinton

|   | Paper Name                                                                                                                               | Status  | Topic       | Category     | Year | Conference | Author                                          | Summary | Link                                     |
| - | ---------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----------- | ------------ | ---- | ---------- | ----------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Capsule Networks: Dynamic Routing Between Capsules](Research_Papers_Anubhav_Reads/Capsule_Networks_Dynamic_Routing_Between_Capsules.md) | Pending | CV , Image  | Architecture | 2017 | arXiv      | Geoffrey E Hinton, Nicholas Frosst, Sara Sabour |         | [link](https://arxiv.org/abs/1710.09829) |


---

## Greg Corrado

|   | Paper Name                                                                                                                                                    | Status  | Topic | Category                  | Year | Conference | Author                                              | Summary | Link                                    |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----- | ------------------------- | ---- | ---------- | --------------------------------------------------- | ------- | --------------------------------------- |
| 0 | [Word2Vec: Efficient Estimation of Word Representations in Vector Space](Research_Papers_Anubhav_Reads/Word2Vec_Efficient_Estimation_of_Word_Representati.md) | Pending | Text  | Embeddings, Tips & Tricks | 2013 | arXiv      | Greg Corrado, Jeffrey Dean, Kai Chen, Tomas Mikolov |         | [link](https://arxiv.org/abs/1301.3781) |


---

## Guillaume Lample

|   | Paper Name                                                                                                                                             | Status  | Topic                          | Category     | Year | Conference | Author                                                                            | Summary | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------ | ------- | ------------------------------ | ------------ | ---- | ---------- | --------------------------------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Phrase-Based & Neural Unsupervised Machine Translation](Research_Papers_Anubhav_Reads/Phrase-Based_&_Neural_Unsupervised_Machine_Transla.md)          | Pending | NMT, Text , Transformers       | Unsupervised | 2018 | arXiv      | Alexis Conneau, Guillaume Lample, Ludovic Denoyer, Marc'Aurelio Ranzato, Myle Ott |         | [link](https://arxiv.org/abs/1804.07755) |
| 1 | [Unsupervised Machine Translation Using Monolingual Corpora Only](Research_Papers_Anubhav_Reads/Unsupervised_Machine_Translation_Using_Monolingual.md) | Pending | GANs, NMT, Text , Transformers | Unsupervised | 2017 | arXiv      | Alexis Conneau, Guillaume Lample, Ludovic Denoyer, Marc'Aurelio Ranzato, Myle Ott |         | [link](https://arxiv.org/abs/1711.00043) |
| 2 | [Cross-lingual Language Model Pretraining](Research_Papers_Anubhav_Reads/Cross-lingual_Language_Model_Pretraining.md)                                  | Pending | NMT, Text , Transformers       | Unsupervised | 2019 | arXiv      | Alexis Conneau, Guillaume Lample                                                  |         | [link](https://arxiv.org/abs/1901.07291) |


---

## Günter Klambauer

|   | Paper Name                                                                                            | Status  | Topic                        | Category                     | Year | Conference | Author                                             | Summary | Link                                                                           |
| - | ----------------------------------------------------------------------------------------------------- | ------- | ---------------------------- | ---------------------------- | ---- | ---------- | -------------------------------------------------- | ------- | ------------------------------------------------------------------------------ |
| 0 | [Self-Normalizing Neural Networks](Research_Papers_Anubhav_Reads/Self-Normalizing_Neural_Networks.md) | Pending | Activation Function, Tabular | Optimizations, Tips & Tricks | 2017 | NIPS       | Andreas Mayr, Günter Klambauer, Thomas Unterthiner |         | [link](https://papers.nips.cc/paper/6698-self-normalizing-neural-networks.pdf) |


---

## Han Zhang

|   | Paper Name                                                                                                                                   | Status  | Topic                   | Category     | Year | Conference | Author                                                      | Summary | Link                                     |
| - | -------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----------------------- | ------------ | ---- | ---------- | ----------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [SAGAN: Self-Attention Generative Adversarial Networks](Research_Papers_Anubhav_Reads/SAGAN_Self-Attention_Generative_Adversarial_Networ.md) | Pending | Attention, GANs, Image  | Architecture | 2018 | arXiv      | Augustus Odena, Dimitris Metaxas, Han Zhang, Ian Goodfellow |         | [link](https://arxiv.org/abs/1805.08318) |


---

## Hannah Rashkin

|   | Paper Name                                                                                                                                                        | Status  | Topic                    | Category | Year | Conference | Author                                       | Summary | Link                                         |
| - | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------------------ | -------- | ---- | ---------- | -------------------------------------------- | ------- | -------------------------------------------- |
| 0 | [COMET: Commonsense Transformers for Automatic Knowledge Graph Construction](Research_Papers_Anubhav_Reads/COMET_Commonsense_Transformers_for_Automatic_Knowl.md) | Pending | AGI, Text , Transformers |          | 2019 | ACL        | Antoine Bosselut, Hannah Rashkin, Yejin Choi |         | [link](https://arxiv.org/pdf/1906.05317.pdf) |


---

## Hao Li

|   | Paper Name                                                                                                        | Status  | Topic             | Category                   | Year | Conference | Author             | Summary | Link                                     |
| - | ----------------------------------------------------------------------------------------------------------------- | ------- | ----------------- | -------------------------- | ---- | ---------- | ------------------ | ------- | ---------------------------------------- |
| 0 | [Pruning Filters for Efficient ConvNets](Research_Papers_Anubhav_Reads/Pruning_Filters_for_Efficient_ConvNets.md) | Pending | CNNs, CV , Image  | Optimization-No. of params | 2017 | arXiv      | Asim Kadav, Hao Li |         | [link](https://arxiv.org/abs/1608.08710) |


---

## Hao Tan

|   | Paper Name                                                                                                                                                                             | Status    | Topic                                                                                         | Category                                           | Year | Conference | Author                                    | Summary                                                                                                                                                             | Link                                         |
| - | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------- | --------------------------------------------------------------------------------------------- | -------------------------------------------------- | ---- | ---------- | ----------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |
| 0 | [Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision](Research_Papers_Anubhav_Reads/Vokenization_Improving_Language_Understanding_with.md) | This week | Image , Text , Transformers                                                                   | Multimodal                                         | 2020 | EMNLP      | Hao Tan, Mohit Bansal                     |                                                                                                                                                                     | [link](https://arxiv.org/abs/2010.06775)     |
| 1 | [VL-T5: Unifying Vision-and-Language Tasks via Text Generation](Research_Papers_Anubhav_Reads/VL-T5_Unifying_Vision-and-Language_Tasks_via_Text_.md)                                   | Read      | CNNs, CV , Generative, Image , Large-Language-Models, Question-Answering, Text , Transformers | Architecture, Embeddings, Multimodal, Pre-Training | 2021 | arXiv      | Hao Tan, Jaemin Cho, Jie Le, Mohit Bansal | Unifying two modalities (image and text) together in a single transformer model to solve multiple tasks in a single architecture using text prefixes similar to T5. | [link](https://arxiv.org/pdf/2102.02779.pdf) |


---

## Hattie Zhou

|   | Paper Name                                                                                                                                             | Status | Topic                  | Category                                              | Year | Conference | Author                                               | Summary                                                                                                                          | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------ | ------ | ---------------------- | ----------------------------------------------------- | ---- | ---------- | ---------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask](Research_Papers_Anubhav_Reads/Deconstructing_Lottery_Tickets_Zeros,_Signs,_and_t.md) | Read   | NN Initialization, NNs | Comparison, Optimization-No. of params, Tips & Tricks | 2019 | NeurIPS    | Hattie Zhou, Janice Lan, Jason Yosinski, Rosanne Liu | Follow up on Lottery Ticket Hypothesis exploring the effects of different Masking criteria as well as Mask-1 and Mask-0 actions. | [link](https://arxiv.org/abs/1905.01067) |


---

## Hyung Won Chung

|   | Paper Name                                                                                                                                    | Status  | Topic                                                                      | Category                   | Year | Conference | Author                                                                       | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Link                                     |
| - | --------------------------------------------------------------------------------------------------------------------------------------------- | ------- | -------------------------------------------------------------------------- | -------------------------- | ---- | ---------- | ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [Flan-T5: Scaling Instruction-Finetuned Language Models](Research_Papers_Anubhav_Reads/Flan-T5_Scaling_Instruction-Finetuned_Language_Mod.md) | Pending | Generative, Text , Transformers                                            | Architecture, Pre-Training | 2022 | arXiv      | Hyung Won Chung, Le Hou                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | [link](https://arxiv.org/abs/2210.11416) |
| 1 | [Scaling Instruction-Finetuned Language Models (FLAN)](Research_Papers_Anubhav_Reads/Scaling_Instruction-Finetuned_Language_Models_FLA.md)    | Pending | Generative, Large-Language-Models, Question-Answering, Text , Transformers | Instruction-Finetuning     | 2022 | arXiv      | Hyung Won Chung, Jason Wei, Jeffrey Dean, Le Hou, Quoc V. Le, Shayne Longpre | https://arxiv.org/abs/2210.11416 introduces FLAN (Fine-tuned LAnguage Net), an instruction finetuning method, and presents the results of its application. The study demonstrates that by fine-tuning the 540B PaLM model on 1836 tasks while incorporating Chain-of-Thought Reasoning data, FLAN achieves improvements in generalization, human usability, and zero-shot reasoning over the base model. The paper also provides detailed information on how each these aspects was evaluated. | [link](https://arxiv.org/abs/2210.11416) |


---

## Ian Goodfellow

|   | Paper Name                                                                                                                                   | Status  | Topic                   | Category        | Year | Conference | Author                                                                              | Summary | Link                                                                           |
| - | -------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----------------------- | --------------- | ---- | ---------- | ----------------------------------------------------------------------------------- | ------- | ------------------------------------------------------------------------------ |
| 0 | [SAGAN: Self-Attention Generative Adversarial Networks](Research_Papers_Anubhav_Reads/SAGAN_Self-Attention_Generative_Adversarial_Networ.md) | Pending | Attention, GANs, Image  | Architecture    | 2018 | arXiv      | Augustus Odena, Dimitris Metaxas, Han Zhang, Ian Goodfellow                         |         | [link](https://arxiv.org/abs/1805.08318)                                       |
| 1 | [Improved Techniques for Training GANs](Research_Papers_Anubhav_Reads/Improved_Techniques_for_Training_GANs.md)                              | Pending | GANs, Image             | Semi-Supervised | 2016 | NIPS       | Alec Radford, Ian Goodfellow, Tim Salimans, Vicki Cheung, Wojciech Zaremba, Xi Chen |         | [link](http://papers.nips.cc/paper/6124-improved-techniques-for-training-gans) |


---

## Iftekhar Naim

|   | Paper Name                                                                                                                              | Status  | Topic             | Category                  | Year | Conference | Author                                           | Summary | Link                                         |
| - | --------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----------------- | ------------------------- | ---- | ---------- | ------------------------------------------------ | ------- | -------------------------------------------- |
| 0 | [Transforming Sequence Tagging Into A Seq2Seq Task](Research_Papers_Anubhav_Reads/Transforming_Sequence_Tagging_Into_A_Seq2Seq_Task.md) | Pending | Generative, Text  | Comparison, Tips & Tricks | 2022 | arXiv      | Iftekhar Naim, Karthik Raman, Krishna Srinivasan |         | [link](https://arxiv.org/pdf/2203.08378.pdf) |


---

## Illia Polosukhin

|   | Paper Name                                                                              | Status | Topic                          | Category     | Year | Conference | Author                                                        | Summary                                                                                       | Link                                                               |
| - | --------------------------------------------------------------------------------------- | ------ | ------------------------------ | ------------ | ---- | ---------- | ------------------------------------------------------------- | --------------------------------------------------------------------------------------------- | ------------------------------------------------------------------ |
| 0 | [Attention is All you Need](Research_Papers_Anubhav_Reads/Attention_is_All_you_Need.md) | Read   | Attention, Text , Transformers | Architecture | 2017 | NIPS       | Ashish Vaswani, Illia Polosukhin, Noam Shazeer, Łukasz Kaiser | Talks about Transformer architecture which brings SOTA performance for different tasks in NLP | [link](http://papers.nips.cc/paper/7181-attention-is-all-you-need) |


---

## Ilya Sutskever

|   | Paper Name                                                                                                                                               | Status  | Topic                          | Category                 | Year | Conference | Author                                                                                | Summary | Link                                                                                                                                                                         |
| - | -------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------------------------ | ------------------------ | ---- | ---------- | ------------------------------------------------------------------------------------- | ------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 0 | [GPT-2 (Language Models are Unsupervised Multitask Learners)](Research_Papers_Anubhav_Reads/GPT-2_Language_Models_are_Unsupervised_Multitask_.md)        | Pending | Attention, Text , Transformers |                          | 2019 |            | Alec Radford, Dario Amodei, Ilya Sutskever, Jeffrey Wu                                |         | [link](https://www.ceid.upatras.gr/webpages/faculty/zaro/teaching/alg-ds/PRESENTATIONS/PAPERS/2019-Radford-et-al_Language-Models-Are-Unsupervised-Multitask-%20Learners.pdf) |
| 1 | [Deep Double Descent: Where Bigger Models and More Data Hurt](Research_Papers_Anubhav_Reads/Deep_Double_Descent_Where_Bigger_Models_and_More_D.md)       | Pending | NNs                            |                          | 2019 | arXiv      | Boaz Barak, Gal Kaplun, Ilya Sutskever, Preetum Nakkiran, Tristan Yang, Yamini Bansal |         | [link](https://arxiv.org/abs/1912.02292)                                                                                                                                     |
| 2 | [GPT-f: Generative Language Modeling for Automated Theorem Proving](Research_Papers_Anubhav_Reads/GPT-f_Generative_Language_Modeling_for_Automated_T.md) | Pending | Attention, Transformers        |                          | 2020 | arXiv      | Ilya Sutskever, Stanislas Polu                                                        |         | [link](https://arxiv.org/abs/2009.03393)                                                                                                                                     |
| 3 | [DALL·E: Creating Images from Text](Research_Papers_Anubhav_Reads/DALL·E_Creating_Images_from_Text.md)                                                   | Pending | Image , Text , Transformers    |                          | 2021 | Blog       | Aditya Ramesh, Gabriel Goh, Ilya Sutskever, Mikhail Pavlov, Scott Gray                |         | [link](https://openai.com/blog/dall-e/)                                                                                                                                      |
| 4 | [CLIP: Connecting Text and Images](Research_Papers_Anubhav_Reads/CLIP_Connecting_Text_and_Images.md)                                                     | Pending | Image , Text , Transformers    | Multimodal, Pre-Training | 2021 | arXiv      | Alec Radford, Ilya Sutskever, Jong Wook Kim                                           |         | [link](https://openai.com/blog/clip/)                                                                                                                                        |


---

## Ioannis Antonoglou

|   | Paper Name                                                                                                                                        | Status  | Topic | Category               | Year | Conference | Author                                                                | Summary | Link                                                                                              |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----- | ---------------------- | ---- | ---------- | --------------------------------------------------------------------- | ------- | ------------------------------------------------------------------------------------------------- |
| 0 | [MuZero: Mastering Go, chess, shogi and Atari without rules](Research_Papers_Anubhav_Reads/MuZero_Mastering_Go,_chess,_shogi_and_Atari_withou.md) | Pending |       | Reinforcement-Learning | 2020 | Nature     | David Silver, Demis Hassabis, Ioannis Antonoglou, Julian Schrittwiese |         | [link](https://deepmind.com/blog/article/muzero-mastering-go-chess-shogi-and-atari-without-rules) |


---

## Itamar Friedman

|   | Paper Name                                                                                                                                                           | Status  | Topic                 | Category                 | Year | Conference | Author                                  | Summary                                                                                                                                                                                      | Link                                     |
| - | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | --------------------- | ------------------------ | ---- | ---------- | --------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering](Research_Papers_Anubhav_Reads/Code_Generation_with_AlphaCodium_From_Prompt_Engin.md) | Pending | Large-Language-Models | Prompting, Tips & Tricks | 2024 | arXiv      | Dedy Kredo, Itamar Friedman, Tal Ridnik | This paper introduces AlphaCodium, a novel test-based, multi-stage, code-oriented iterative approach for improving the performance of Language Model Models (LLMs) on code generation tasks. | [link](https://arxiv.org/abs/2401.08500) |


---

## Izhak Shafran

|   | Paper Name                                                                                                                                        | Status  | Topic                                    | Category                     | Year | Conference | Author                                                                                 | Summary                                                                                                                                                                                                                                                                                                                                                  | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ---------------------------------------- | ---------------------------- | ---- | ---------- | -------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [ReAct: Synergizing Reasoning and Acting in Language Models](Research_Papers_Anubhav_Reads/ReAct_Synergizing_Reasoning_and_Acting_in_Language.md) | Pending | Generative, Large-Language-Models, Text  | Optimizations, Tips & Tricks | 2023 | ICLR       | Dian Yu, Izhak Shafran, Jeffrey Zhao, Karthik Narasimhan, Nan Du, Shunyu Yao, Yuan Cao | This paper introduces ReAct, a novel approach that leverages Large Language Models (LLMs) to interleave reasoning traces and task-specific actions. ReAct outperforms existing methods on various language and decision-making tasks, addressing issues like hallucination, error propagation, and improving human interpretability and trustworthiness. | [link](https://arxiv.org/abs/2210.03629) |


---

## Jaakko Lehtinen

|   | Paper Name                                                                                                                                                        | Status  | Topic        | Category      | Year | Conference | Author                                                | Summary | Link                                     |
| - | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | ------------- | ---- | ---------- | ----------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Progressive Growing of GANs for Improved Quality, Stability, and Variation](Research_Papers_Anubhav_Reads/Progressive_Growing_of_GANs_for_Improved_Quality,_.md) | Pending | GANs, Image  | Tips & Tricks | 2018 | ICLR       | Jaakko Lehtinen, Samuli Laine, Tero Karras, Timo Aila |         | [link](https://arxiv.org/abs/1710.10196) |


---

## Jack Hessel

|   | Paper Name                                                                                                                                                                 | Status  | Topic                        | Category                     | Year | Conference | Author                                                   | Summary | Link                                     |
| - | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ---------------------------- | ---------------------------- | ---- | ---------- | -------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Symbolic Knowledge Distillation: from General Language Models to Commonsense Models](Research_Papers_Anubhav_Reads/Symbolic_Knowledge_Distillation_from_General_Langu.md) | Pending | Dataset, Text , Transformers | Optimizations, Tips & Tricks | 2021 | arXiv      | Chandra Bhagavatula, Jack Hessel, Peter West, Yejin Choi |         | [link](https://arxiv.org/abs/2110.07178) |


---

## Jacob Devlin

|   | Paper Name                                                                                                                                                              | Status | Topic                          | Category   | Year | Conference | Author                                                       | Summary                                                                                                                                                                                | Link                                     |
| - | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------ | ------------------------------ | ---------- | ---- | ---------- | ------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](Research_Papers_Anubhav_Reads/BERT_Pre-training_of_Deep_Bidirectional_Transforme.md) | Read   | Attention, Text , Transformers | Embeddings | 2018 | NAACL      | Jacob Devlin, Kenton Lee, Kristina Toutanova, Ming-Wei Chang | BERT is an extension to Transformer based architecture which introduces a masked word pretraining and next sentence prediction task to pretrain the model for a wide variety of tasks. | [link](https://arxiv.org/abs/1810.04805) |


---

## Jae Sung Park

|   | Paper Name                                                                                                                                               | Status  | Topic                                     | Category | Year | Conference | Author                                                      | Summary | Link                                                                           |
| - | -------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----------------------------------------- | -------- | ---- | ---------- | ----------------------------------------------------------- | ------- | ------------------------------------------------------------------------------ |
| 0 | [VisualCOMET: Reasoning about the Dynamic Context of a Still Image](Research_Papers_Anubhav_Reads/VisualCOMET_Reasoning_about_the_Dynamic_Context_of.md) | Pending | AGI, Dataset, Image , Text , Transformers |          | 2020 | ECCV       | Ali Farhadi, Chandra Bhagavatula, Jae Sung Park, Yejin Choi |         | [link](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500494.pdf) |


---

## Jaegul Choo

|   | Paper Name                                                                                                                                                                          | Status  | Topic        | Category | Year | Conference | Author                                                                       | Summary | Link                                                                                                               |
| - | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | -------- | ---- | ---------- | ---------------------------------------------------------------------------- | ------- | ------------------------------------------------------------------------------------------------------------------ |
| 0 | [StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation](Research_Papers_Anubhav_Reads/StarGAN_Unified_Generative_Adversarial_Networks_fo.md) | Pending | GANs, Image  |          | 2018 | CVPR       | Jaegul Choo, Jung-Woo Ha, Minje Choi, Munyoung Kim, Sunghun Kim, Yunjey Choi |         | [link](https://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_StarGAN_Unified_Generative_CVPR_2018_paper.pdf) |


---

## Jaemin Cho

|   | Paper Name                                                                                                                                           | Status | Topic                                                                                         | Category                                           | Year | Conference | Author                                    | Summary                                                                                                                                                             | Link                                         |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------- | ------ | --------------------------------------------------------------------------------------------- | -------------------------------------------------- | ---- | ---------- | ----------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |
| 0 | [VL-T5: Unifying Vision-and-Language Tasks via Text Generation](Research_Papers_Anubhav_Reads/VL-T5_Unifying_Vision-and-Language_Tasks_via_Text_.md) | Read   | CNNs, CV , Generative, Image , Large-Language-Models, Question-Answering, Text , Transformers | Architecture, Embeddings, Multimodal, Pre-Training | 2021 | arXiv      | Hao Tan, Jaemin Cho, Jie Le, Mohit Bansal | Unifying two modalities (image and text) together in a single transformer model to solve multiple tasks in a single architecture using text prefixes similar to T5. | [link](https://arxiv.org/pdf/2102.02779.pdf) |


---

## Jakob Uszkoreit

|   | Paper Name                                                                                                                                                                            | Status  | Topic                           | Category | Year | Conference | Author                                                         | Summary | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------------------------- | -------- | ---- | ---------- | -------------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Vision Transformer: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](Research_Papers_Anubhav_Reads/Vision_Transformer_An_Image_is_Worth_16x16_Words_T.md) | Pending | Attention, Image , Transformers |          | 2021 | ICLR       | Alexey Dosovitskiy, Jakob Uszkoreit, Lucas Beyer, Neil Houlsby |         | [link](https://arxiv.org/abs/2010.11929) |


---

## James J. Little

|   | Paper Name                                                                                                                                          | Status  | Topic                | Category | Year | Conference | Author                                                          | Summary | Link                                                                                                    |
| - | --------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | -------------------- | -------- | ---- | ---------- | --------------------------------------------------------------- | ------- | ------------------------------------------------------------------------------------------------------- |
| 0 | [A Simple yet Effective Baseline for 3D Human Pose Estimation](Research_Papers_Anubhav_Reads/A_Simple_yet_Effective_Baseline_for_3D_Human_Pose_.md) | Pending | CV , Pose Estimation |          | 2017 | ICCV       | James J. Little, Javier Romero, Julieta Martinez, Rayat Hossain |         | [link](https://openaccess.thecvf.com/content_iccv_2017/html/Martinez_A_Simple_yet_ICCV_2017_paper.html) |


---

## Jan Leike

|   | Paper Name                                                                                                                                                 | Status  | Topic                                              | Category                                                        | Year | Conference | Author                                                                                                                      | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Link                                         |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | -------------------------------------------------- | --------------------------------------------------------------- | ---- | ---------- | --------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |
| 0 | [Training language models to follow instructions with human feedback](Research_Papers_Anubhav_Reads/Training_language_models_to_follow_instructions_wi.md) | Pending | Generative, Large-Language-Models, Training Method | Instruction-Finetuning, Reinforcement-Learning, Semi-Supervised | 2022 | arXiv      | Carroll L. Wainwright, Diogo Almeida, Jan Leike, Jeff Wu, Long Ouyang, Pamela Mishkin, Paul Christiano, Ryan Lowe, Xu Jiang | This paper presents InstructGPT, a model fine-tuned with human feedback to better align with user intent across various tasks. Despite having significantly fewer parameters than larger models, InstructGPT outperforms them in human evaluations, demonstrating improved truthfulness, reduced toxicity, and minimal performance regressions on public NLP datasets, highlighting the potential of fine-tuning with human feedback for enhancing language model alignment with human intent. | [link](https://arxiv.org/pdf/2203.02155.pdf) |


---

## Janice Lan

|   | Paper Name                                                                                                                                             | Status | Topic                  | Category                                              | Year | Conference | Author                                               | Summary                                                                                                                          | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------ | ------ | ---------------------- | ----------------------------------------------------- | ---- | ---------- | ---------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask](Research_Papers_Anubhav_Reads/Deconstructing_Lottery_Tickets_Zeros,_Signs,_and_t.md) | Read   | NN Initialization, NNs | Comparison, Optimization-No. of params, Tips & Tricks | 2019 | NeurIPS    | Hattie Zhou, Janice Lan, Jason Yosinski, Rosanne Liu | Follow up on Lottery Ticket Hypothesis exploring the effects of different Masking criteria as well as Mask-1 and Mask-0 actions. | [link](https://arxiv.org/abs/1905.01067) |


---

## Jared Kaplan

|   | Paper Name                                                                                                                           | Status  | Topic                                              | Category                                                     | Year | Conference | Author                  | Summary                                                                                                                                                                                                                                                                                                                                                                           | Link                                         |
| - | ------------------------------------------------------------------------------------------------------------------------------------ | ------- | -------------------------------------------------- | ------------------------------------------------------------ | ---- | ---------- | ----------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |
| 0 | [Constitutional AI: Harmlessness from AI Feedback](Research_Papers_Anubhav_Reads/Constitutional_AI_Harmlessness_from_AI_Feedback.md) | Pending | Generative, Large-Language-Models, Training Method | Instruction-Finetuning, Reinforcement-Learning, Unsupervised | 2022 | arXiv      | Jared Kaplan, Yuntao Ba | The paper introduces Constitutional AI, a method for training a safe AI assistant without human-labeled data on harmful outputs. It combines supervised learning and reinforcement learning phases, enabling the AI to engage with harmful queries by explaining its objections, thus improving control, transparency, and human-judged performance with minimal human oversight. | [link](https://arxiv.org/pdf/2212.08073.pdf) |


---

## Jason Wei

|   | Paper Name                                                                                                                                                   | Status  | Topic                                                                      | Category               | Year | Conference | Author                                                                       | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------- | -------------------------------------------------------------------------- | ---------------------- | ---- | ---------- | ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [Chain of Thought Prompting Elicits Reasoning in Large Language Models](Research_Papers_Anubhav_Reads/Chain_of_Thought_Prompting_Elicits_Reasoning_in_La.md) | Pending | Question-Answering, Text , Transformers                                    |                        | 2022 | arXiv      | Denny Zhou, Jason Wei, Xuezhi Wang                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | [link](https://arxiv.org/abs/2201.11903) |
| 1 | [Scaling Instruction-Finetuned Language Models (FLAN)](Research_Papers_Anubhav_Reads/Scaling_Instruction-Finetuned_Language_Models_FLA.md)                   | Pending | Generative, Large-Language-Models, Question-Answering, Text , Transformers | Instruction-Finetuning | 2022 | arXiv      | Hyung Won Chung, Jason Wei, Jeffrey Dean, Le Hou, Quoc V. Le, Shayne Longpre | https://arxiv.org/abs/2210.11416 introduces FLAN (Fine-tuned LAnguage Net), an instruction finetuning method, and presents the results of its application. The study demonstrates that by fine-tuning the 540B PaLM model on 1836 tasks while incorporating Chain-of-Thought Reasoning data, FLAN achieves improvements in generalization, human usability, and zero-shot reasoning over the base model. The paper also provides detailed information on how each these aspects was evaluated. | [link](https://arxiv.org/abs/2210.11416) |


---

## Jason Weston

|   | Paper Name                                                                                                                          | Status  | Topic                                              | Category               | Year | Conference | Author                                     | Summary                                                                                                                                                                                                                                                                                                                                                                                                       | Link                                         |
| - | ----------------------------------------------------------------------------------------------------------------------------------- | ------- | -------------------------------------------------- | ---------------------- | ---- | ---------- | ------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |
| 0 | [Self-Alignment with Instruction Backtranslation](Research_Papers_Anubhav_Reads/Self-Alignment_with_Instruction_Backtranslation.md) | Pending | Generative, Large-Language-Models, Training Method | Instruction-Finetuning | 2023 | arXiv      | Jason Weston, Mike Lewis, Ping Yu, Xian Li | The paper introduces a scalable method called "instruction backtranslation" to create a high-quality instruction-following language model. This method involves self-augmentation and self-curation of training examples generated from web documents, resulting in a model that outperforms others in its category without relying on distillation data, showcasing its effective self-alignment capability. | [link](https://arxiv.org/pdf/2308.06259.pdf) |


---

## Jason Yosinski

|   | Paper Name                                                                                                                                             | Status | Topic                  | Category                                              | Year | Conference | Author                                               | Summary                                                                                                                          | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------ | ------ | ---------------------- | ----------------------------------------------------- | ---- | ---------- | ---------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask](Research_Papers_Anubhav_Reads/Deconstructing_Lottery_Tickets_Zeros,_Signs,_and_t.md) | Read   | NN Initialization, NNs | Comparison, Optimization-No. of params, Tips & Tricks | 2019 | NeurIPS    | Hattie Zhou, Janice Lan, Jason Yosinski, Rosanne Liu | Follow up on Lottery Ticket Hypothesis exploring the effects of different Masking criteria as well as Mask-1 and Mask-0 actions. | [link](https://arxiv.org/abs/1905.01067) |


---

## Javier Romero

|   | Paper Name                                                                                                                                          | Status  | Topic                | Category | Year | Conference | Author                                                          | Summary | Link                                                                                                    |
| - | --------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | -------------------- | -------- | ---- | ---------- | --------------------------------------------------------------- | ------- | ------------------------------------------------------------------------------------------------------- |
| 0 | [A Simple yet Effective Baseline for 3D Human Pose Estimation](Research_Papers_Anubhav_Reads/A_Simple_yet_Effective_Baseline_for_3D_Human_Pose_.md) | Pending | CV , Pose Estimation |          | 2017 | ICCV       | James J. Little, Javier Romero, Julieta Martinez, Rayat Hossain |         | [link](https://openaccess.thecvf.com/content_iccv_2017/html/Martinez_A_Simple_yet_ICCV_2017_paper.html) |


---

## Jeff Wu

|   | Paper Name                                                                                                                                                 | Status  | Topic                                              | Category                                                        | Year | Conference | Author                                                                                                                      | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Link                                         |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | -------------------------------------------------- | --------------------------------------------------------------- | ---- | ---------- | --------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |
| 0 | [Training language models to follow instructions with human feedback](Research_Papers_Anubhav_Reads/Training_language_models_to_follow_instructions_wi.md) | Pending | Generative, Large-Language-Models, Training Method | Instruction-Finetuning, Reinforcement-Learning, Semi-Supervised | 2022 | arXiv      | Carroll L. Wainwright, Diogo Almeida, Jan Leike, Jeff Wu, Long Ouyang, Pamela Mishkin, Paul Christiano, Ryan Lowe, Xu Jiang | This paper presents InstructGPT, a model fine-tuned with human feedback to better align with user intent across various tasks. Despite having significantly fewer parameters than larger models, InstructGPT outperforms them in human evaluations, demonstrating improved truthfulness, reduced toxicity, and minimal performance regressions on public NLP datasets, highlighting the potential of fine-tuning with human feedback for enhancing language model alignment with human intent. | [link](https://arxiv.org/pdf/2203.02155.pdf) |


---

## Jeffrey Dean

|   | Paper Name                                                                                                                                                    | Status  | Topic                                                                      | Category                  | Year | Conference | Author                                                                       | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | -------------------------------------------------------------------------- | ------------------------- | ---- | ---------- | ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [Word2Vec: Efficient Estimation of Word Representations in Vector Space](Research_Papers_Anubhav_Reads/Word2Vec_Efficient_Estimation_of_Word_Representati.md) | Pending | Text                                                                       | Embeddings, Tips & Tricks | 2013 | arXiv      | Greg Corrado, Jeffrey Dean, Kai Chen, Tomas Mikolov                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | [link](https://arxiv.org/abs/1301.3781)  |
| 1 | [Scaling Instruction-Finetuned Language Models (FLAN)](Research_Papers_Anubhav_Reads/Scaling_Instruction-Finetuned_Language_Models_FLA.md)                    | Pending | Generative, Large-Language-Models, Question-Answering, Text , Transformers | Instruction-Finetuning    | 2022 | arXiv      | Hyung Won Chung, Jason Wei, Jeffrey Dean, Le Hou, Quoc V. Le, Shayne Longpre | https://arxiv.org/abs/2210.11416 introduces FLAN (Fine-tuned LAnguage Net), an instruction finetuning method, and presents the results of its application. The study demonstrates that by fine-tuning the 540B PaLM model on 1836 tasks while incorporating Chain-of-Thought Reasoning data, FLAN achieves improvements in generalization, human usability, and zero-shot reasoning over the base model. The paper also provides detailed information on how each these aspects was evaluated. | [link](https://arxiv.org/abs/2210.11416) |


---

## Jeffrey Wu

|   | Paper Name                                                                                                                                        | Status  | Topic                          | Category | Year | Conference | Author                                                 | Summary | Link                                                                                                                                                                         |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------------------------ | -------- | ---- | ---------- | ------------------------------------------------------ | ------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 0 | [GPT-2 (Language Models are Unsupervised Multitask Learners)](Research_Papers_Anubhav_Reads/GPT-2_Language_Models_are_Unsupervised_Multitask_.md) | Pending | Attention, Text , Transformers |          | 2019 |            | Alec Radford, Dario Amodei, Ilya Sutskever, Jeffrey Wu |         | [link](https://www.ceid.upatras.gr/webpages/faculty/zaro/teaching/alg-ds/PRESENTATIONS/PAPERS/2019-Radford-et-al_Language-Models-Are-Unsupervised-Multitask-%20Learners.pdf) |


---

## Jeffrey Zhao

|   | Paper Name                                                                                                                                        | Status  | Topic                                    | Category                     | Year | Conference | Author                                                                                 | Summary                                                                                                                                                                                                                                                                                                                                                  | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ---------------------------------------- | ---------------------------- | ---- | ---------- | -------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [ReAct: Synergizing Reasoning and Acting in Language Models](Research_Papers_Anubhav_Reads/ReAct_Synergizing_Reasoning_and_Acting_in_Language.md) | Pending | Generative, Large-Language-Models, Text  | Optimizations, Tips & Tricks | 2023 | ICLR       | Dian Yu, Izhak Shafran, Jeffrey Zhao, Karthik Narasimhan, Nan Du, Shunyu Yao, Yuan Cao | This paper introduces ReAct, a novel approach that leverages Large Language Models (LLMs) to interleave reasoning traces and task-specific actions. ReAct outperforms existing methods on various language and decision-making tasks, addressing issues like hallucination, error propagation, and improving human interpretability and trustworthiness. | [link](https://arxiv.org/abs/2210.03629) |


---

## Jessica B. Hamrick

|   | Paper Name                                                                                                                                                                  | Status  | Topic   | Category     | Year | Conference | Author                                                | Summary | Link                                         |
| - | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------- | ------------ | ---- | ---------- | ----------------------------------------------------- | ------- | -------------------------------------------- |
| 0 | [Graph Neural Network: Relational inductive biases, deep learning, and graph networks](Research_Papers_Anubhav_Reads/Graph_Neural_Network_Relational_inductive_biases,_.md) | Pending | GraphNN | Architecture | 2018 | arXiv      | Jessica B. Hamrick, Oriol Vinyals, Peter W. Battaglia |         | [link](https://arxiv.org/pdf/1806.01261.pdf) |


---

## Jiakai Zhang

|   | Paper Name                                                                                                                                                                            | Status  | Topic        | Category | Year | Conference | Author                               | Summary | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | -------- | ---- | ---------- | ------------------------------------ | ------- | ---------------------------------------- |
| 0 | [AnimeGAN: Towards the Automatic Anime Characters Creation with Generative Adversarial Networks](Research_Papers_Anubhav_Reads/AnimeGAN_Towards_the_Automatic_Anime_Characters_Cr.md) | Pending | GANs, Image  |          | 2017 | NIPS       | Jiakai Zhang, Minjun Li, Yanghua Jin |         | [link](https://arxiv.org/abs/1708.05509) |


---

## Jie Le

|   | Paper Name                                                                                                                                           | Status | Topic                                                                                         | Category                                           | Year | Conference | Author                                    | Summary                                                                                                                                                             | Link                                         |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------- | ------ | --------------------------------------------------------------------------------------------- | -------------------------------------------------- | ---- | ---------- | ----------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |
| 0 | [VL-T5: Unifying Vision-and-Language Tasks via Text Generation](Research_Papers_Anubhav_Reads/VL-T5_Unifying_Vision-and-Language_Tasks_via_Text_.md) | Read   | CNNs, CV , Generative, Image , Large-Language-Models, Question-Answering, Text , Transformers | Architecture, Embeddings, Multimodal, Pre-Training | 2021 | arXiv      | Hao Tan, Jaemin Cho, Jie Le, Mohit Bansal | Unifying two modalities (image and text) together in a single transformer model to solve multiple tasks in a single architecture using text prefixes similar to T5. | [link](https://arxiv.org/pdf/2102.02779.pdf) |


---

## Jimmy Ba

|   | Paper Name                                                                                                               | Status  | Topic           | Category | Year | Conference | Author                       | Summary | Link                                    |
| - | ------------------------------------------------------------------------------------------------------------------------ | ------- | --------------- | -------- | ---- | ---------- | ---------------------------- | ------- | --------------------------------------- |
| 0 | [Adam: A Method for Stochastic Optimization](Research_Papers_Anubhav_Reads/Adam_A_Method_for_Stochastic_Optimization.md) | Pending | NNs, Optimizers |          | 2015 | ICLR       | Diederik P. Kingma, Jimmy Ba |         | [link](https://arxiv.org/abs/1412.6980) |


---

## Jiri Matas

|   | Paper Name                                                                                  | Status  | Topic             | Category      | Year | Conference | Author                     | Summary | Link                                     |
| - | ------------------------------------------------------------------------------------------- | ------- | ----------------- | ------------- | ---- | ---------- | -------------------------- | ------- | ---------------------------------------- |
| 0 | [All you need is a good init](Research_Papers_Anubhav_Reads/All_you_need_is_a_good_init.md) | Pending | NN Initialization | Tips & Tricks | 2015 | arXiv      | Dmytro Mishkin, Jiri Matas |         | [link](https://arxiv.org/abs/1511.06422) |


---

## Jitendra Malik

|   | Paper Name                                                                                                                                                                | Status  | Topic | Category | Year | Conference | Author                                                              | Summary | Link                                                                                                                                                                                                                                                   |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----- | -------- | ---- | ---------- | ------------------------------------------------------------------- | ------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 0 | [IMLE-GAN: Inclusive GAN: Improving Data and Minority Coverage in Generative Models](Research_Papers_Anubhav_Reads/IMLE-GAN_Inclusive_GAN_Improving_Data_and_Minority.md) | Pending | GANs  |          | 2020 | arXiv      | Jitendra Malik, Ke Li, Larry Davis, Mario Fritz, Ning Yu, Peng Zhou |         | [link](https://arxiv.org/abs/2004.03355?utm_campaign=The%20Batch&utm_medium=email&_hsmi=96406275&_hsenc=p2ANqtz-8ra-5k3I7Hv0hosTfQ1neO9Z10r3yMPB1oQfzpBEfkCQ_i0q0diEm4w21S8WqkMbOASXxQvDTIoqJbBZvX4i7S-exeOg&utm_content=96406275&utm_source=hs_email) |


---

## Jonathan Frankle

|   | Paper Name                                                                                                                                                                       | Status  | Topic                  | Category                                  | Year | Conference | Author                                           | Summary                                                                                                                                                                                                                                       | Link                                     |
| - | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ---------------------- | ----------------------------------------- | ---- | ---------- | ------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](Research_Papers_Anubhav_Reads/The_Lottery_Ticket_Hypothesis_Finding_Sparse,_Trai.md)                  | Read    | NN Initialization, NNs | Optimization-No. of params, Tips & Tricks | 2019 | ICLR       | Jonathan Frankle, Michael Carbin                 | Lottery ticket hypothesis: dense, randomly-initialized, feed-forward networks contain subnetworks (winning tickets) that—when trained in isolation— reach test accuracy comparable to the original network in a similar number of iterations. | [link](https://arxiv.org/abs/1803.03635) |
| 1 | [Training BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs](Research_Papers_Anubhav_Reads/Training_BatchNorm_and_Only_BatchNorm_On_the_Expre.md) | Pending | CNNs, Image            |                                           | 2020 | arXiv      | Ari S. Morcos, David J. Schwab, Jonathan Frankle |                                                                                                                                                                                                                                               | [link](https://arxiv.org/abs/2003.00152) |


---

## Jong Wook Kim

|   | Paper Name                                                                                           | Status  | Topic                       | Category                 | Year | Conference | Author                                      | Summary | Link                                  |
| - | ---------------------------------------------------------------------------------------------------- | ------- | --------------------------- | ------------------------ | ---- | ---------- | ------------------------------------------- | ------- | ------------------------------------- |
| 0 | [CLIP: Connecting Text and Images](Research_Papers_Anubhav_Reads/CLIP_Connecting_Text_and_Images.md) | Pending | Image , Text , Transformers | Multimodal, Pre-Training | 2021 | arXiv      | Alec Radford, Ilya Sutskever, Jong Wook Kim |         | [link](https://openai.com/blog/clip/) |


---

## Jordan Hoffmann

|   | Paper Name                                                                                                                        | Status  | Topic                               | Category                                                              | Year | Conference | Author                                                            | Summary | Link                                     |
| - | --------------------------------------------------------------------------------------------------------------------------------- | ------- | ----------------------------------- | --------------------------------------------------------------------- | ---- | ---------- | ----------------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Training Compute-Optimal Large Language Models](Research_Papers_Anubhav_Reads/Training_Compute-Optimal_Large_Language_Models.md) | Pending | Large-Language-Models, Transformers | Architecture, Optimization-No. of params, Pre-Training, Tips & Tricks | 2022 | arXiv      | Jordan Hoffmann, Laurent Sifre, Oriol Vinyals, Sebastian Borgeaud |         | [link](https://arxiv.org/abs/2203.15556) |


---

## Julian Schrittwiese

|   | Paper Name                                                                                                                                        | Status  | Topic | Category               | Year | Conference | Author                                                                | Summary | Link                                                                                              |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----- | ---------------------- | ---- | ---------- | --------------------------------------------------------------------- | ------- | ------------------------------------------------------------------------------------------------- |
| 0 | [MuZero: Mastering Go, chess, shogi and Atari without rules](Research_Papers_Anubhav_Reads/MuZero_Mastering_Go,_chess,_shogi_and_Atari_withou.md) | Pending |       | Reinforcement-Learning | 2020 | Nature     | David Silver, Demis Hassabis, Ioannis Antonoglou, Julian Schrittwiese |         | [link](https://deepmind.com/blog/article/muzero-mastering-go-chess-shogi-and-atari-without-rules) |


---

## Julieta Martinez

|   | Paper Name                                                                                                                                          | Status  | Topic                | Category | Year | Conference | Author                                                          | Summary | Link                                                                                                    |
| - | --------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | -------------------- | -------- | ---- | ---------- | --------------------------------------------------------------- | ------- | ------------------------------------------------------------------------------------------------------- |
| 0 | [A Simple yet Effective Baseline for 3D Human Pose Estimation](Research_Papers_Anubhav_Reads/A_Simple_yet_Effective_Baseline_for_3D_Human_Pose_.md) | Pending | CV , Pose Estimation |          | 2017 | ICCV       | James J. Little, Javier Romero, Julieta Martinez, Rayat Hossain |         | [link](https://openaccess.thecvf.com/content_iccv_2017/html/Martinez_A_Simple_yet_ICCV_2017_paper.html) |


---

## Jun Huang

|   | Paper Name                                                                                                                                                                                      | Status  | Topic         | Category | Year | Conference | Author                                                  | Summary | Link                                     |
| - | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------- | -------- | ---- | ---------- | ------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [One-shot Text Field Labeling using Attention and Belief Propagation for Structure Information Extraction](Research_Papers_Anubhav_Reads/One-shot_Text_Field_Labeling_using_Attention_and_B.md) | Pending | Image , Text  |          | 2020 | arXiv      | Jun Huang, Mengli Cheng, Minghui Qiu, Wei Lin, Xing Shi |         | [link](https://arxiv.org/abs/2009.04153) |


---

## Jun-Yan Zhu

|   | Paper Name                                                                                                                                                                       | Status  | Topic        | Category     | Year | Conference | Author                                                    | Summary                                                                                                  | Link                                                                                                                      |
| - | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | ------------ | ---- | ---------- | --------------------------------------------------------- | -------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------- |
| 0 | [Pix2Pix: Image-to-Image Translation with Conditional Adversarial Nets](Research_Papers_Anubhav_Reads/Pix2Pix_Image-to-Image_Translation_with_Conditiona.md)                     | Read    | GANs, Image  |              | 2017 | CVPR       | Alexei A. Efros, Jun-Yan Zhu, Phillip Isola, Tinghui Zhou | Image to image translation using Conditional GANs and dataset of image pairs from one domain to another. | [link](https://arxiv.org/abs/1611.07004)                                                                                  |
| 1 | [CycleGAN: Unpaired Image-To-Image Translation Using Cycle-Consistent Adversarial Networks](Research_Papers_Anubhav_Reads/CycleGAN_Unpaired_Image-To-Image_Translation_Using.md) | Pending | GANs, Image  | Architecture | 2017 | ICCV       | Alexei A. Efros, Jun-Yan Zhu, Phillip Isola, Taesung Park |                                                                                                          | [link](https://openaccess.thecvf.com/content_iccv_2017/html/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.html) |


---

## Jung-Woo Ha

|   | Paper Name                                                                                                                                                                          | Status  | Topic        | Category | Year | Conference | Author                                                                       | Summary | Link                                                                                                               |
| - | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | -------- | ---- | ---------- | ---------------------------------------------------------------------------- | ------- | ------------------------------------------------------------------------------------------------------------------ |
| 0 | [StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation](Research_Papers_Anubhav_Reads/StarGAN_Unified_Generative_Adversarial_Networks_fo.md) | Pending | GANs, Image  |          | 2018 | CVPR       | Jaegul Choo, Jung-Woo Ha, Minje Choi, Munyoung Kim, Sunghun Kim, Yunjey Choi |         | [link](https://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_StarGAN_Unified_Generative_CVPR_2018_paper.pdf) |


---

## Justin Johnson

|   | Paper Name                                                                                                                                                 | Status  | Topic              | Category | Year | Conference | Author                                      | Summary | Link                                     |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------------ | -------- | ---- | ---------- | ------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](Research_Papers_Anubhav_Reads/Perceptual_Losses_for_Real-Time_Style_Transfer_and.md) | Pending | Loss Function, NNs |          | 2016 | ECCV       | Alexandre Alahi, Justin Johnson, Li Fei-Fei |         | [link](https://arxiv.org/abs/1603.08155) |


---

## Kai Chen

|   | Paper Name                                                                                                                                                    | Status  | Topic | Category                  | Year | Conference | Author                                              | Summary | Link                                    |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----- | ------------------------- | ---- | ---------- | --------------------------------------------------- | ------- | --------------------------------------- |
| 0 | [Word2Vec: Efficient Estimation of Word Representations in Vector Space](Research_Papers_Anubhav_Reads/Word2Vec_Efficient_Estimation_of_Word_Representati.md) | Pending | Text  | Embeddings, Tips & Tricks | 2013 | arXiv      | Greg Corrado, Jeffrey Dean, Kai Chen, Tomas Mikolov |         | [link](https://arxiv.org/abs/1301.3781) |


---

## Kaiming He

|   | Paper Name                                                                                                                                  | Status  | Topic              | Category      | Year | Conference | Author                    | Summary                                                                         | Link                                                                                                        |
| - | ------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------------ | ------------- | ---- | ---------- | ------------------------- | ------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------- |
| 0 | [ResNet (Deep Residual Learning for Image Recognition)](Research_Papers_Anubhav_Reads/ResNet_Deep_Residual_Learning_for_Image_Recogniti.md) | Read    | CNNs, CV , Image   | Architecture  | 2016 | CVPR       | Kaiming He, Xiangyu Zhang | Introduces Residual or Skip Connections to allow increase in the depth of a DNN | [link](https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) |
| 1 | [Group Normalization](Research_Papers_Anubhav_Reads/Group_Normalization.md)                                                                 | Pending | NNs, Normalization | Optimizations | 2018 | arXiv      | Kaiming He, Yuxin Wu      |                                                                                 | [link](https://arxiv.org/abs/1803.08494)                                                                    |


---

## Karthik Narasimhan

|   | Paper Name                                                                                                                                        | Status  | Topic                                    | Category                     | Year | Conference | Author                                                                                 | Summary                                                                                                                                                                                                                                                                                                                                                  | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ---------------------------------------- | ---------------------------- | ---- | ---------- | -------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [ReAct: Synergizing Reasoning and Acting in Language Models](Research_Papers_Anubhav_Reads/ReAct_Synergizing_Reasoning_and_Acting_in_Language.md) | Pending | Generative, Large-Language-Models, Text  | Optimizations, Tips & Tricks | 2023 | ICLR       | Dian Yu, Izhak Shafran, Jeffrey Zhao, Karthik Narasimhan, Nan Du, Shunyu Yao, Yuan Cao | This paper introduces ReAct, a novel approach that leverages Large Language Models (LLMs) to interleave reasoning traces and task-specific actions. ReAct outperforms existing methods on various language and decision-making tasks, addressing issues like hallucination, error propagation, and improving human interpretability and trustworthiness. | [link](https://arxiv.org/abs/2210.03629) |


---

## Karthik Raman

|   | Paper Name                                                                                                                              | Status  | Topic             | Category                  | Year | Conference | Author                                           | Summary | Link                                         |
| - | --------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----------------- | ------------------------- | ---- | ---------- | ------------------------------------------------ | ------- | -------------------------------------------- |
| 0 | [Transforming Sequence Tagging Into A Seq2Seq Task](Research_Papers_Anubhav_Reads/Transforming_Sequence_Tagging_Into_A_Seq2Seq_Task.md) | Pending | Generative, Text  | Comparison, Tips & Tricks | 2022 | arXiv      | Iftekhar Naim, Karthik Raman, Krishna Srinivasan |         | [link](https://arxiv.org/pdf/2203.08378.pdf) |


---

## Ke Li

|   | Paper Name                                                                                                                                                                | Status  | Topic | Category | Year | Conference | Author                                                              | Summary | Link                                                                                                                                                                                                                                                   |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----- | -------- | ---- | ---------- | ------------------------------------------------------------------- | ------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 0 | [IMLE-GAN: Inclusive GAN: Improving Data and Minority Coverage in Generative Models](Research_Papers_Anubhav_Reads/IMLE-GAN_Inclusive_GAN_Improving_Data_and_Minority.md) | Pending | GANs  |          | 2020 | arXiv      | Jitendra Malik, Ke Li, Larry Davis, Mario Fritz, Ning Yu, Peng Zhou |         | [link](https://arxiv.org/abs/2004.03355?utm_campaign=The%20Batch&utm_medium=email&_hsmi=96406275&_hsenc=p2ANqtz-8ra-5k3I7Hv0hosTfQ1neO9Z10r3yMPB1oQfzpBEfkCQ_i0q0diEm4w21S8WqkMbOASXxQvDTIoqJbBZvX4i7S-exeOg&utm_content=96406275&utm_source=hs_email) |


---

## Kenton Lee

|   | Paper Name                                                                                                                                                              | Status | Topic                          | Category   | Year | Conference | Author                                                       | Summary                                                                                                                                                                                | Link                                     |
| - | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------ | ------------------------------ | ---------- | ---- | ---------- | ------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](Research_Papers_Anubhav_Reads/BERT_Pre-training_of_Deep_Bidirectional_Transforme.md) | Read   | Attention, Text , Transformers | Embeddings | 2018 | NAACL      | Jacob Devlin, Kenton Lee, Kristina Toutanova, Ming-Wei Chang | BERT is an extension to Transformer based architecture which introduces a masked word pretraining and next sentence prediction task to pretrain the model for a wide variety of tasks. | [link](https://arxiv.org/abs/1810.04805) |


---

## Krishna Srinivasan

|   | Paper Name                                                                                                                              | Status  | Topic             | Category                  | Year | Conference | Author                                           | Summary | Link                                         |
| - | --------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----------------- | ------------------------- | ---- | ---------- | ------------------------------------------------ | ------- | -------------------------------------------- |
| 0 | [Transforming Sequence Tagging Into A Seq2Seq Task](Research_Papers_Anubhav_Reads/Transforming_Sequence_Tagging_Into_A_Seq2Seq_Task.md) | Pending | Generative, Text  | Comparison, Tips & Tricks | 2022 | arXiv      | Iftekhar Naim, Karthik Raman, Krishna Srinivasan |         | [link](https://arxiv.org/pdf/2203.08378.pdf) |


---

## Kristen Grauman

|   | Paper Name                                                                                                                                             | Status  | Topic        | Category               | Year | Conference | Author                                                   | Summary | Link                                         |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------ | ------- | ------------ | ---------------------- | ---- | ---------- | -------------------------------------------------------- | ------- | -------------------------------------------- |
| 0 | [Occupancy Anticipation for Efficient Exploration and Navigation](Research_Papers_Anubhav_Reads/Occupancy_Anticipation_for_Efficient_Exploration_a.md) | Pending | CNNs, Image  | Reinforcement-Learning | 2020 | ECCV       | Kristen Grauman, Santhosh K. Ramakrishnan, Ziad Al-Halah |         | [link](https://arxiv.org/pdf/2008.09285.pdf) |


---

## Kristina Toutanova

|   | Paper Name                                                                                                                                                              | Status | Topic                          | Category   | Year | Conference | Author                                                       | Summary                                                                                                                                                                                | Link                                     |
| - | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------ | ------------------------------ | ---------- | ---- | ---------- | ------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](Research_Papers_Anubhav_Reads/BERT_Pre-training_of_Deep_Bidirectional_Transforme.md) | Read   | Attention, Text , Transformers | Embeddings | 2018 | NAACL      | Jacob Devlin, Kenton Lee, Kristina Toutanova, Ming-Wei Chang | BERT is an extension to Transformer based architecture which introduces a masked word pretraining and next sentence prediction task to pretrain the model for a wide variety of tasks. | [link](https://arxiv.org/abs/1810.04805) |


---

## Larry Davis

|   | Paper Name                                                                                                                                                                | Status  | Topic | Category | Year | Conference | Author                                                              | Summary | Link                                                                                                                                                                                                                                                   |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----- | -------- | ---- | ---------- | ------------------------------------------------------------------- | ------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 0 | [IMLE-GAN: Inclusive GAN: Improving Data and Minority Coverage in Generative Models](Research_Papers_Anubhav_Reads/IMLE-GAN_Inclusive_GAN_Improving_Data_and_Minority.md) | Pending | GANs  |          | 2020 | arXiv      | Jitendra Malik, Ke Li, Larry Davis, Mario Fritz, Ning Yu, Peng Zhou |         | [link](https://arxiv.org/abs/2004.03355?utm_campaign=The%20Batch&utm_medium=email&_hsmi=96406275&_hsenc=p2ANqtz-8ra-5k3I7Hv0hosTfQ1neO9Z10r3yMPB1oQfzpBEfkCQ_i0q0diEm4w21S8WqkMbOASXxQvDTIoqJbBZvX4i7S-exeOg&utm_content=96406275&utm_source=hs_email) |


---

## Laurent Sifre

|   | Paper Name                                                                                                                        | Status  | Topic                               | Category                                                              | Year | Conference | Author                                                            | Summary | Link                                     |
| - | --------------------------------------------------------------------------------------------------------------------------------- | ------- | ----------------------------------- | --------------------------------------------------------------------- | ---- | ---------- | ----------------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Training Compute-Optimal Large Language Models](Research_Papers_Anubhav_Reads/Training_Compute-Optimal_Large_Language_Models.md) | Pending | Large-Language-Models, Transformers | Architecture, Optimization-No. of params, Pre-Training, Tips & Tricks | 2022 | arXiv      | Jordan Hoffmann, Laurent Sifre, Oriol Vinyals, Sebastian Borgeaud |         | [link](https://arxiv.org/abs/2203.15556) |


---

## Le Hou

|   | Paper Name                                                                                                                                    | Status  | Topic                                                                      | Category                   | Year | Conference | Author                                                                       | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Link                                     |
| - | --------------------------------------------------------------------------------------------------------------------------------------------- | ------- | -------------------------------------------------------------------------- | -------------------------- | ---- | ---------- | ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [Flan-T5: Scaling Instruction-Finetuned Language Models](Research_Papers_Anubhav_Reads/Flan-T5_Scaling_Instruction-Finetuned_Language_Mod.md) | Pending | Generative, Text , Transformers                                            | Architecture, Pre-Training | 2022 | arXiv      | Hyung Won Chung, Le Hou                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | [link](https://arxiv.org/abs/2210.11416) |
| 1 | [Scaling Instruction-Finetuned Language Models (FLAN)](Research_Papers_Anubhav_Reads/Scaling_Instruction-Finetuned_Language_Models_FLA.md)    | Pending | Generative, Large-Language-Models, Question-Answering, Text , Transformers | Instruction-Finetuning     | 2022 | arXiv      | Hyung Won Chung, Jason Wei, Jeffrey Dean, Le Hou, Quoc V. Le, Shayne Longpre | https://arxiv.org/abs/2210.11416 introduces FLAN (Fine-tuned LAnguage Net), an instruction finetuning method, and presents the results of its application. The study demonstrates that by fine-tuning the 540B PaLM model on 1836 tasks while incorporating Chain-of-Thought Reasoning data, FLAN achieves improvements in generalization, human usability, and zero-shot reasoning over the base model. The paper also provides detailed information on how each these aspects was evaluated. | [link](https://arxiv.org/abs/2210.11416) |


---

## Leonard Adolphs

|   | Paper Name                                                                                                                                             | Status  | Topic | Category                 | Year | Conference | Author                                                   | Summary | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------ | ------- | ----- | ------------------------ | ---- | ---------- | -------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Decoding a Neural Retriever’s Latent Space for Query Suggestion](Research_Papers_Anubhav_Reads/Decoding_a_Neural_Retriever’s_Latent_Space_for_Que.md) | Pending | Text  | Embeddings, Latent space | 2022 | arXiv      | Christian Buck, Leonard Adolphs, Michelle Chen Huebscher |         | [link](https://arxiv.org/abs/2210.12084) |


---

## Li Fei-Fei

|   | Paper Name                                                                                                                                                 | Status  | Topic              | Category | Year | Conference | Author                                      | Summary | Link                                     |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------------ | -------- | ---- | ---------- | ------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](Research_Papers_Anubhav_Reads/Perceptual_Losses_for_Real-Time_Style_Transfer_and.md) | Pending | Loss Function, NNs |          | 2016 | ECCV       | Alexandre Alahi, Justin Johnson, Li Fei-Fei |         | [link](https://arxiv.org/abs/1603.08155) |


---

## Li Yang

|   | Paper Name                                                                                                                                                                            | Status | Topic                                   | Category           | Year | Conference | Author              | Summary                                                                                                                                                  | Link                                                       |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------ | --------------------------------------- | ------------------ | ---- | ---------- | ------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------- |
| 0 | [Learning to Extract Attribute Value from Product via Question Answering: A Multi-task Approach](Research_Papers_Anubhav_Reads/Learning_to_Extract_Attribute_Value_from_Product_v.md) | Read   | Question-Answering, Text , Transformers | Zero-shot-learning | 2020 | KDD        | Li Yang, Qifan Wang | Question Answering BERT model used to extract attributes from products. Introduce further No Answer loss and distillation to promote zero shot learning. | [link](https://dl.acm.org/doi/pdf/10.1145/3394486.3403047) |


---

## Long Ouyang

|   | Paper Name                                                                                                                                                 | Status  | Topic                                              | Category                                                        | Year | Conference | Author                                                                                                                      | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Link                                         |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | -------------------------------------------------- | --------------------------------------------------------------- | ---- | ---------- | --------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |
| 0 | [Training language models to follow instructions with human feedback](Research_Papers_Anubhav_Reads/Training_language_models_to_follow_instructions_wi.md) | Pending | Generative, Large-Language-Models, Training Method | Instruction-Finetuning, Reinforcement-Learning, Semi-Supervised | 2022 | arXiv      | Carroll L. Wainwright, Diogo Almeida, Jan Leike, Jeff Wu, Long Ouyang, Pamela Mishkin, Paul Christiano, Ryan Lowe, Xu Jiang | This paper presents InstructGPT, a model fine-tuned with human feedback to better align with user intent across various tasks. Despite having significantly fewer parameters than larger models, InstructGPT outperforms them in human evaluations, demonstrating improved truthfulness, reduced toxicity, and minimal performance regressions on public NLP datasets, highlighting the potential of fine-tuning with human feedback for enhancing language model alignment with human intent. | [link](https://arxiv.org/pdf/2203.02155.pdf) |


---

## Lucas Beyer

|   | Paper Name                                                                                                                                                                            | Status  | Topic                           | Category | Year | Conference | Author                                                         | Summary | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------------------------- | -------- | ---- | ---------- | -------------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Vision Transformer: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](Research_Papers_Anubhav_Reads/Vision_Transformer_An_Image_is_Worth_16x16_Words_T.md) | Pending | Attention, Image , Transformers |          | 2021 | ICLR       | Alexey Dosovitskiy, Jakob Uszkoreit, Lucas Beyer, Neil Houlsby |         | [link](https://arxiv.org/abs/2010.11929) |


---

## Ludovic Denoyer

|   | Paper Name                                                                                                                                             | Status  | Topic                          | Category     | Year | Conference | Author                                                                            | Summary | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------ | ------- | ------------------------------ | ------------ | ---- | ---------- | --------------------------------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Phrase-Based & Neural Unsupervised Machine Translation](Research_Papers_Anubhav_Reads/Phrase-Based_&_Neural_Unsupervised_Machine_Transla.md)          | Pending | NMT, Text , Transformers       | Unsupervised | 2018 | arXiv      | Alexis Conneau, Guillaume Lample, Ludovic Denoyer, Marc'Aurelio Ranzato, Myle Ott |         | [link](https://arxiv.org/abs/1804.07755) |
| 1 | [Unsupervised Machine Translation Using Monolingual Corpora Only](Research_Papers_Anubhav_Reads/Unsupervised_Machine_Translation_Using_Monolingual.md) | Pending | GANs, NMT, Text , Transformers | Unsupervised | 2017 | arXiv      | Alexis Conneau, Guillaume Lample, Ludovic Denoyer, Marc'Aurelio Ranzato, Myle Ott |         | [link](https://arxiv.org/abs/1711.00043) |


---

## Lukasz Kaiser

|   | Paper Name                                                                                                 | Status | Topic                          | Category                                                      | Year | Conference | Author                                        | Summary                                                                                                                 | Link                                     |
| - | ---------------------------------------------------------------------------------------------------------- | ------ | ------------------------------ | ------------------------------------------------------------- | ---- | ---------- | --------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [Reformer: The Efficient Transformer](Research_Papers_Anubhav_Reads/Reformer_The_Efficient_Transformer.md) | Read   | Attention, Text , Transformers | Architecture, Optimization-Memory, Optimization-No. of params | 2020 | arXiv      | Anselm Levskaya, Lukasz Kaiser, Nikita Kitaev | Overcome time and memory complexity of Transformers by bucketing Query, Keys and using Reversible residual connections. | [link](https://arxiv.org/abs/2001.04451) |


---

## Luke Metz

|   | Paper Name                                                                                                                                         | Status  | Topic        | Category | Year | Conference | Author                                    | Summary | Link                                     |
| - | -------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | -------- | ---- | ---------- | ----------------------------------------- | ------- | ---------------------------------------- |
| 0 | [BEGAN: Boundary Equilibrium Generative Adversarial Networks](Research_Papers_Anubhav_Reads/BEGAN_Boundary_Equilibrium_Generative_Adversarial_.md) | Pending | GANs, Image  |          | 2017 | arXiv      | David Berthelot, Luke Metz, Thomas Schumm |         | [link](https://arxiv.org/abs/1703.10717) |


---

## Luming Tang

|   | Paper Name                                                                                                                                                 | Status  | Topic        | Category          | Year | Conference | Author                                           | Summary | Link                                                                                                                                                        |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | ----------------- | ---- | ---------- | ------------------------------------------------ | ------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 0 | [Revisiting Pose-Normalization for Fine-Grained Few-Shot Recognition](Research_Papers_Anubhav_Reads/Revisiting_Pose-Normalization_for_Fine-Grained_Few.md) | Pending | CNNs, Image  | Few-shot-learning | 2020 | CVPR       | Bharath Hariharan, Davis Wertheimer, Luming Tang |         | [link](https://openaccess.thecvf.com/content_CVPR_2020/papers/Tang_Revisiting_Pose-Normalization_for_Fine-Grained_Few-Shot_Recognition_CVPR_2020_paper.pdf) |


---

## Léon Bottou

|   | Paper Name                                                                     | Status  | Topic               | Category | Year | Conference | Author                                         | Summary | Link                                     |
| - | ------------------------------------------------------------------------------ | ------- | ------------------- | -------- | ---- | ---------- | ---------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [WGAN: Wasserstein GAN](Research_Papers_Anubhav_Reads/WGAN_Wasserstein_GAN.md) | Pending | GANs, Loss Function |          | 2017 | arXiv      | Léon Bottou, Martin Arjovsky, Soumith Chintala |         | [link](https://arxiv.org/abs/1701.07875) |


---

## Maarten Sap

|   | Paper Name                                                                                                                                           | Status  | Topic               | Category | Year | Conference | Author                                                | Summary | Link                                         |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------------- | -------- | ---- | ---------- | ----------------------------------------------------- | ------- | -------------------------------------------- |
| 0 | [ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning](Research_Papers_Anubhav_Reads/ATOMIC_An_Atlas_of_Machine_Commonsense_for_If-Then.md) | Pending | AGI, Dataset, Text  |          | 2019 | AAAI       | Maarten Sap, Noah A. Smith, Ronan Le Bras, Yejin Choi |         | [link](https://arxiv.org/pdf/1811.00146.pdf) |


---

## Mandar Joshi

|   | Paper Name                                                                                                                                                   | Status | Topic                                   | Category     | Year | Conference | Author                   | Summary                                                                                              | Link                                                    |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------ | --------------------------------------- | ------------ | ---- | ---------- | ------------------------ | ---------------------------------------------------------------------------------------------------- | ------------------------------------------------------- |
| 0 | [SpanBERT: Improving Pre-training by Representing and Predicting Spans](Research_Papers_Anubhav_Reads/SpanBERT_Improving_Pre-training_by_Representing_an.md) | Read   | Question-Answering, Text , Transformers | Pre-Training | 2020 | TACL       | Danqi Chen, Mandar Joshi | A different pre-training strategy for BERT model to improve performance for Question Answering task. | [link](https://www.aclweb.org/anthology/2020.tacl-1.5/) |


---

## Marc'Aurelio Ranzato

|   | Paper Name                                                                                                                                             | Status  | Topic                          | Category     | Year | Conference | Author                                                                            | Summary | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------ | ------- | ------------------------------ | ------------ | ---- | ---------- | --------------------------------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Phrase-Based & Neural Unsupervised Machine Translation](Research_Papers_Anubhav_Reads/Phrase-Based_&_Neural_Unsupervised_Machine_Transla.md)          | Pending | NMT, Text , Transformers       | Unsupervised | 2018 | arXiv      | Alexis Conneau, Guillaume Lample, Ludovic Denoyer, Marc'Aurelio Ranzato, Myle Ott |         | [link](https://arxiv.org/abs/1804.07755) |
| 1 | [Unsupervised Machine Translation Using Monolingual Corpora Only](Research_Papers_Anubhav_Reads/Unsupervised_Machine_Translation_Using_Monolingual.md) | Pending | GANs, NMT, Text , Transformers | Unsupervised | 2017 | arXiv      | Alexis Conneau, Guillaume Lample, Ludovic Denoyer, Marc'Aurelio Ranzato, Myle Ott |         | [link](https://arxiv.org/abs/1711.00043) |


---

## Mario Fritz

|   | Paper Name                                                                                                                                                                | Status  | Topic | Category | Year | Conference | Author                                                              | Summary | Link                                                                                                                                                                                                                                                   |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----- | -------- | ---- | ---------- | ------------------------------------------------------------------- | ------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 0 | [IMLE-GAN: Inclusive GAN: Improving Data and Minority Coverage in Generative Models](Research_Papers_Anubhav_Reads/IMLE-GAN_Inclusive_GAN_Improving_Data_and_Minority.md) | Pending | GANs  |          | 2020 | arXiv      | Jitendra Malik, Ke Li, Larry Davis, Mario Fritz, Ning Yu, Peng Zhou |         | [link](https://arxiv.org/abs/2004.03355?utm_campaign=The%20Batch&utm_medium=email&_hsmi=96406275&_hsenc=p2ANqtz-8ra-5k3I7Hv0hosTfQ1neO9Z10r3yMPB1oQfzpBEfkCQ_i0q0diEm4w21S8WqkMbOASXxQvDTIoqJbBZvX4i7S-exeOg&utm_content=96406275&utm_source=hs_email) |


---

## Martin Arjovsky

|   | Paper Name                                                                     | Status  | Topic               | Category | Year | Conference | Author                                         | Summary | Link                                     |
| - | ------------------------------------------------------------------------------ | ------- | ------------------- | -------- | ---- | ---------- | ---------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [WGAN: Wasserstein GAN](Research_Papers_Anubhav_Reads/WGAN_Wasserstein_GAN.md) | Pending | GANs, Loss Function |          | 2017 | arXiv      | Léon Bottou, Martin Arjovsky, Soumith Chintala |         | [link](https://arxiv.org/abs/1701.07875) |


---

## Masanori Koyama

|   | Paper Name                                                                                          | Status  | Topic               | Category      | Year | Conference | Author                                                          | Summary | Link                                     |
| - | --------------------------------------------------------------------------------------------------- | ------- | ------------------- | ------------- | ---- | ---------- | --------------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Spectral Normalization for GANs](Research_Papers_Anubhav_Reads/Spectral_Normalization_for_GANs.md) | Pending | GANs, Normalization | Optimizations | 2018 | arXiv      | Masanori Koyama, Takeru Miyato, Toshiki Kataoka, Yuichi Yoshida |         | [link](https://arxiv.org/abs/1802.05957) |


---

## Mateusz Koziński

|   | Paper Name                                                                                                                                                         | Status  | Topic                               | Category | Year | Conference | Author                                                            | Summary | Link                                                                                                             |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------- | ----------------------------------- | -------- | ---- | ---------- | ----------------------------------------------------------------- | ------- | ---------------------------------------------------------------------------------------------------------------- |
| 0 | [Topological Loss: Beyond the Pixel-Wise Loss for Topology-Aware Delineation](Research_Papers_Anubhav_Reads/Topological_Loss_Beyond_the_Pixel-Wise_Loss_for_To.md) | Pending | Image , Loss Function, Segmentation |          | 2018 | CVPR       | Agata Mosinska, Mateusz Koziński, Pablo Márquez-Neila, Pascal Fua |         | [link](https://openaccess.thecvf.com/content_cvpr_2018/html/Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper.html) |


---

## Matthew D. Zeiler

|   | Paper Name                                                                                                                                          | Status | Topic             | Category      | Year | Conference | Author                        | Summary                                                                          | Link                                                                   |
| - | --------------------------------------------------------------------------------------------------------------------------------------------------- | ------ | ----------------- | ------------- | ---- | ---------- | ----------------------------- | -------------------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| 0 | [ZF Net (Visualizing and Understanding Convolutional Networks)](Research_Papers_Anubhav_Reads/ZF_Net_Visualizing_and_Understanding_Convolutiona.md) | Read   | CNNs, CV , Image  | Visualization | 2014 | ECCV       | Matthew D. Zeiler, Rob Fergus | Visualize CNN Filters / Kernels using De-Convolutions on CNN filter activations. | [link](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53) |


---

## Matthias Bethge

|   | Paper Name                                                                                                                                                                      | Status  | Topic             | Category | Year | Conference | Author                           | Summary | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----------------- | -------- | ---- | ---------- | -------------------------------- | ------- | ---------------------------------------- |
| 0 | [Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet](Research_Papers_Anubhav_Reads/Approximating_CNNs_with_Bag-of-local-Features_mode.md) | Reading | CNNs, CV , Image  |          | 2019 | arXiv      | Matthias Bethge, Wieland Brendel |         | [link](https://arxiv.org/abs/1904.00760) |


---

## Mengli Cheng

|   | Paper Name                                                                                                                                                                                      | Status  | Topic         | Category | Year | Conference | Author                                                  | Summary | Link                                     |
| - | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------- | -------- | ---- | ---------- | ------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [One-shot Text Field Labeling using Attention and Belief Propagation for Structure Information Extraction](Research_Papers_Anubhav_Reads/One-shot_Text_Field_Labeling_using_Attention_and_B.md) | Pending | Image , Text  |          | 2020 | arXiv      | Jun Huang, Mengli Cheng, Minghui Qiu, Wei Lin, Xing Shi |         | [link](https://arxiv.org/abs/2009.04153) |


---

## Menglin Jia

|   | Paper Name                                                                                                                                      | Status  | Topic         | Category      | Year | Conference | Author               | Summary | Link                                                                                                                                            |
| - | ----------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------- | ------------- | ---- | ---------- | -------------------- | ------- | ----------------------------------------------------------------------------------------------------------------------------------------------- |
| 0 | [Class-Balanced Loss Based on Effective Number of Samples](Research_Papers_Anubhav_Reads/Class-Balanced_Loss_Based_on_Effective_Number_of_S.md) | Pending | Loss Function | Tips & Tricks | 2019 | CVPR       | Menglin Jia, Yin Cui |         | [link](https://openaccess.thecvf.com/content_CVPR_2019/papers/Cui_Class-Balanced_Loss_Based_on_Effective_Number_of_Samples_CVPR_2019_paper.pdf) |


---

## Menglong Zhu

|   | Paper Name                                                                                                                                                               | Status  | Topic             | Category                                 | Year | Conference | Author                         | Summary | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------- | ----------------- | ---------------------------------------- | ---- | ---------- | ------------------------------ | ------- | ---------------------------------------- |
| 0 | [MobileNet (Efficient Convolutional Neural Networks for Mobile Vision Applications)](Research_Papers_Anubhav_Reads/MobileNet_Efficient_Convolutional_Neural_Networks.md) | Pending | CNNs, CV , Image  | Architecture, Optimization-No. of params | 2017 | arXiv      | Andrew G. Howard, Menglong Zhu |         | [link](https://arxiv.org/abs/1704.04861) |


---

## Michael Carbin

|   | Paper Name                                                                                                                                                      | Status | Topic                  | Category                                  | Year | Conference | Author                           | Summary                                                                                                                                                                                                                                       | Link                                     |
| - | --------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------ | ---------------------- | ----------------------------------------- | ---- | ---------- | -------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](Research_Papers_Anubhav_Reads/The_Lottery_Ticket_Hypothesis_Finding_Sparse,_Trai.md) | Read   | NN Initialization, NNs | Optimization-No. of params, Tips & Tricks | 2019 | ICLR       | Jonathan Frankle, Michael Carbin | Lottery ticket hypothesis: dense, randomly-initialized, feed-forward networks contain subnetworks (winning tickets) that—when trained in isolation— reach test accuracy comparable to the original network in a similar number of iterations. | [link](https://arxiv.org/abs/1803.03635) |


---

## Michelle Chen Huebscher

|   | Paper Name                                                                                                                                             | Status  | Topic | Category                 | Year | Conference | Author                                                   | Summary | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------ | ------- | ----- | ------------------------ | ---- | ---------- | -------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Decoding a Neural Retriever’s Latent Space for Query Suggestion](Research_Papers_Anubhav_Reads/Decoding_a_Neural_Retriever’s_Latent_Space_for_Que.md) | Pending | Text  | Embeddings, Latent space | 2022 | arXiv      | Christian Buck, Leonard Adolphs, Michelle Chen Huebscher |         | [link](https://arxiv.org/abs/2210.12084) |


---

## Mike Lewis

|   | Paper Name                                                                                                                          | Status  | Topic                                              | Category               | Year | Conference | Author                                     | Summary                                                                                                                                                                                                                                                                                                                                                                                                       | Link                                         |
| - | ----------------------------------------------------------------------------------------------------------------------------------- | ------- | -------------------------------------------------- | ---------------------- | ---- | ---------- | ------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |
| 0 | [Self-Alignment with Instruction Backtranslation](Research_Papers_Anubhav_Reads/Self-Alignment_with_Instruction_Backtranslation.md) | Pending | Generative, Large-Language-Models, Training Method | Instruction-Finetuning | 2023 | arXiv      | Jason Weston, Mike Lewis, Ping Yu, Xian Li | The paper introduces a scalable method called "instruction backtranslation" to create a high-quality instruction-following language model. This method involves self-augmentation and self-curation of training examples generated from web documents, resulting in a model that outperforms others in its category without relying on distillation data, showcasing its effective self-alignment capability. | [link](https://arxiv.org/pdf/2308.06259.pdf) |


---

## Mikhail Pavlov

|   | Paper Name                                                                                             | Status  | Topic                       | Category | Year | Conference | Author                                                                 | Summary | Link                                    |
| - | ------------------------------------------------------------------------------------------------------ | ------- | --------------------------- | -------- | ---- | ---------- | ---------------------------------------------------------------------- | ------- | --------------------------------------- |
| 0 | [DALL·E: Creating Images from Text](Research_Papers_Anubhav_Reads/DALL·E_Creating_Images_from_Text.md) | Pending | Image , Text , Transformers |          | 2021 | Blog       | Aditya Ramesh, Gabriel Goh, Ilya Sutskever, Mikhail Pavlov, Scott Gray |         | [link](https://openai.com/blog/dall-e/) |


---

## Ming-Wei Chang

|   | Paper Name                                                                                                                                                              | Status | Topic                          | Category   | Year | Conference | Author                                                       | Summary                                                                                                                                                                                | Link                                     |
| - | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------ | ------------------------------ | ---------- | ---- | ---------- | ------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](Research_Papers_Anubhav_Reads/BERT_Pre-training_of_Deep_Bidirectional_Transforme.md) | Read   | Attention, Text , Transformers | Embeddings | 2018 | NAACL      | Jacob Devlin, Kenton Lee, Kristina Toutanova, Ming-Wei Chang | BERT is an extension to Transformer based architecture which introduces a masked word pretraining and next sentence prediction task to pretrain the model for a wide variety of tasks. | [link](https://arxiv.org/abs/1810.04805) |


---

## Minghui Qiu

|   | Paper Name                                                                                                                                                                                      | Status  | Topic         | Category | Year | Conference | Author                                                  | Summary | Link                                     |
| - | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------- | -------- | ---- | ---------- | ------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [One-shot Text Field Labeling using Attention and Belief Propagation for Structure Information Extraction](Research_Papers_Anubhav_Reads/One-shot_Text_Field_Labeling_using_Attention_and_B.md) | Pending | Image , Text  |          | 2020 | arXiv      | Jun Huang, Mengli Cheng, Minghui Qiu, Wei Lin, Xing Shi |         | [link](https://arxiv.org/abs/2009.04153) |


---

## Minje Choi

|   | Paper Name                                                                                                                                                                          | Status  | Topic        | Category | Year | Conference | Author                                                                       | Summary | Link                                                                                                               |
| - | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | -------- | ---- | ---------- | ---------------------------------------------------------------------------- | ------- | ------------------------------------------------------------------------------------------------------------------ |
| 0 | [StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation](Research_Papers_Anubhav_Reads/StarGAN_Unified_Generative_Adversarial_Networks_fo.md) | Pending | GANs, Image  |          | 2018 | CVPR       | Jaegul Choo, Jung-Woo Ha, Minje Choi, Munyoung Kim, Sunghun Kim, Yunjey Choi |         | [link](https://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_StarGAN_Unified_Generative_CVPR_2018_paper.pdf) |


---

## Minjun Li

|   | Paper Name                                                                                                                                                                            | Status  | Topic        | Category | Year | Conference | Author                               | Summary | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | -------- | ---- | ---------- | ------------------------------------ | ------- | ---------------------------------------- |
| 0 | [AnimeGAN: Towards the Automatic Anime Characters Creation with Generative Adversarial Networks](Research_Papers_Anubhav_Reads/AnimeGAN_Towards_the_Automatic_Anime_Characters_Cr.md) | Pending | GANs, Image  |          | 2017 | NIPS       | Jiakai Zhang, Minjun Li, Yanghua Jin |         | [link](https://arxiv.org/abs/1708.05509) |


---

## Mohit Bansal

|   | Paper Name                                                                                                                                                                             | Status    | Topic                                                                                         | Category                                           | Year | Conference | Author                                    | Summary                                                                                                                                                             | Link                                         |
| - | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------- | --------------------------------------------------------------------------------------------- | -------------------------------------------------- | ---- | ---------- | ----------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |
| 0 | [Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision](Research_Papers_Anubhav_Reads/Vokenization_Improving_Language_Understanding_with.md) | This week | Image , Text , Transformers                                                                   | Multimodal                                         | 2020 | EMNLP      | Hao Tan, Mohit Bansal                     |                                                                                                                                                                     | [link](https://arxiv.org/abs/2010.06775)     |
| 1 | [VL-T5: Unifying Vision-and-Language Tasks via Text Generation](Research_Papers_Anubhav_Reads/VL-T5_Unifying_Vision-and-Language_Tasks_via_Text_.md)                                   | Read      | CNNs, CV , Generative, Image , Large-Language-Models, Question-Answering, Text , Transformers | Architecture, Embeddings, Multimodal, Pre-Training | 2021 | arXiv      | Hao Tan, Jaemin Cho, Jie Le, Mohit Bansal | Unifying two modalities (image and text) together in a single transformer model to solve multiple tasks in a single architecture using text prefixes similar to T5. | [link](https://arxiv.org/pdf/2102.02779.pdf) |


---

## Munyoung Kim

|   | Paper Name                                                                                                                                                                          | Status  | Topic        | Category | Year | Conference | Author                                                                       | Summary | Link                                                                                                               |
| - | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | -------- | ---- | ---------- | ---------------------------------------------------------------------------- | ------- | ------------------------------------------------------------------------------------------------------------------ |
| 0 | [StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation](Research_Papers_Anubhav_Reads/StarGAN_Unified_Generative_Adversarial_Networks_fo.md) | Pending | GANs, Image  |          | 2018 | CVPR       | Jaegul Choo, Jung-Woo Ha, Minje Choi, Munyoung Kim, Sunghun Kim, Yunjey Choi |         | [link](https://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_StarGAN_Unified_Generative_CVPR_2018_paper.pdf) |


---

## Myle Ott

|   | Paper Name                                                                                                                                             | Status  | Topic                          | Category     | Year | Conference | Author                                                                            | Summary | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------ | ------- | ------------------------------ | ------------ | ---- | ---------- | --------------------------------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Phrase-Based & Neural Unsupervised Machine Translation](Research_Papers_Anubhav_Reads/Phrase-Based_&_Neural_Unsupervised_Machine_Transla.md)          | Pending | NMT, Text , Transformers       | Unsupervised | 2018 | arXiv      | Alexis Conneau, Guillaume Lample, Ludovic Denoyer, Marc'Aurelio Ranzato, Myle Ott |         | [link](https://arxiv.org/abs/1804.07755) |
| 1 | [Unsupervised Machine Translation Using Monolingual Corpora Only](Research_Papers_Anubhav_Reads/Unsupervised_Machine_Translation_Using_Monolingual.md) | Pending | GANs, NMT, Text , Transformers | Unsupervised | 2017 | arXiv      | Alexis Conneau, Guillaume Lample, Ludovic Denoyer, Marc'Aurelio Ranzato, Myle Ott |         | [link](https://arxiv.org/abs/1711.00043) |


---

## Nan Du

|   | Paper Name                                                                                                                                        | Status  | Topic                                    | Category                     | Year | Conference | Author                                                                                 | Summary                                                                                                                                                                                                                                                                                                                                                  | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ---------------------------------------- | ---------------------------- | ---- | ---------- | -------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [ReAct: Synergizing Reasoning and Acting in Language Models](Research_Papers_Anubhav_Reads/ReAct_Synergizing_Reasoning_and_Acting_in_Language.md) | Pending | Generative, Large-Language-Models, Text  | Optimizations, Tips & Tricks | 2023 | ICLR       | Dian Yu, Izhak Shafran, Jeffrey Zhao, Karthik Narasimhan, Nan Du, Shunyu Yao, Yuan Cao | This paper introduces ReAct, a novel approach that leverages Large Language Models (LLMs) to interleave reasoning traces and task-specific actions. ReAct outperforms existing methods on various language and decision-making tasks, addressing issues like hallucination, error propagation, and improving human interpretability and trustworthiness. | [link](https://arxiv.org/abs/2210.03629) |


---

## Neil Houlsby

|   | Paper Name                                                                                                                                                                            | Status  | Topic                           | Category | Year | Conference | Author                                                         | Summary | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------------------------- | -------- | ---- | ---------- | -------------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Vision Transformer: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](Research_Papers_Anubhav_Reads/Vision_Transformer_An_Image_is_Worth_16x16_Words_T.md) | Pending | Attention, Image , Transformers |          | 2021 | ICLR       | Alexey Dosovitskiy, Jakob Uszkoreit, Lucas Beyer, Neil Houlsby |         | [link](https://arxiv.org/abs/2010.11929) |


---

## Nicholas Frosst

|   | Paper Name                                                                                                                               | Status  | Topic       | Category     | Year | Conference | Author                                          | Summary | Link                                     |
| - | ---------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----------- | ------------ | ---- | ---------- | ----------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Capsule Networks: Dynamic Routing Between Capsules](Research_Papers_Anubhav_Reads/Capsule_Networks_Dynamic_Routing_Between_Capsules.md) | Pending | CV , Image  | Architecture | 2017 | arXiv      | Geoffrey E Hinton, Nicholas Frosst, Sara Sabour |         | [link](https://arxiv.org/abs/1710.09829) |


---

## Nikita Kitaev

|   | Paper Name                                                                                                 | Status | Topic                          | Category                                                      | Year | Conference | Author                                        | Summary                                                                                                                 | Link                                     |
| - | ---------------------------------------------------------------------------------------------------------- | ------ | ------------------------------ | ------------------------------------------------------------- | ---- | ---------- | --------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [Reformer: The Efficient Transformer](Research_Papers_Anubhav_Reads/Reformer_The_Efficient_Transformer.md) | Read   | Attention, Text , Transformers | Architecture, Optimization-Memory, Optimization-No. of params | 2020 | arXiv      | Anselm Levskaya, Lukasz Kaiser, Nikita Kitaev | Overcome time and memory complexity of Transformers by bucketing Query, Keys and using Reversible residual connections. | [link](https://arxiv.org/abs/2001.04451) |


---

## Ning Yu

|   | Paper Name                                                                                                                                                                | Status  | Topic | Category | Year | Conference | Author                                                              | Summary | Link                                                                                                                                                                                                                                                   |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----- | -------- | ---- | ---------- | ------------------------------------------------------------------- | ------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 0 | [IMLE-GAN: Inclusive GAN: Improving Data and Minority Coverage in Generative Models](Research_Papers_Anubhav_Reads/IMLE-GAN_Inclusive_GAN_Improving_Data_and_Minority.md) | Pending | GANs  |          | 2020 | arXiv      | Jitendra Malik, Ke Li, Larry Davis, Mario Fritz, Ning Yu, Peng Zhou |         | [link](https://arxiv.org/abs/2004.03355?utm_campaign=The%20Batch&utm_medium=email&_hsmi=96406275&_hsenc=p2ANqtz-8ra-5k3I7Hv0hosTfQ1neO9Z10r3yMPB1oQfzpBEfkCQ_i0q0diEm4w21S8WqkMbOASXxQvDTIoqJbBZvX4i7S-exeOg&utm_content=96406275&utm_source=hs_email) |


---

## Noah A. Smith

|   | Paper Name                                                                                                                                           | Status  | Topic               | Category | Year | Conference | Author                                                | Summary | Link                                         |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------------- | -------- | ---- | ---------- | ----------------------------------------------------- | ------- | -------------------------------------------- |
| 0 | [ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning](Research_Papers_Anubhav_Reads/ATOMIC_An_Atlas_of_Machine_Commonsense_for_If-Then.md) | Pending | AGI, Dataset, Text  |          | 2019 | AAAI       | Maarten Sap, Noah A. Smith, Ronan Le Bras, Yejin Choi |         | [link](https://arxiv.org/pdf/1811.00146.pdf) |


---

## Noam Shazeer

|   | Paper Name                                                                                                                                                                   | Status | Topic                          | Category     | Year | Conference | Author                                                        | Summary                                                                                                                                                                                                             | Link                                                               |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------ | ------------------------------ | ------------ | ---- | ---------- | ------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------ |
| 0 | [Attention is All you Need](Research_Papers_Anubhav_Reads/Attention_is_All_you_Need.md)                                                                                      | Read   | Attention, Text , Transformers | Architecture | 2017 | NIPS       | Ashish Vaswani, Illia Polosukhin, Noam Shazeer, Łukasz Kaiser | Talks about Transformer architecture which brings SOTA performance for different tasks in NLP                                                                                                                       | [link](http://papers.nips.cc/paper/7181-attention-is-all-you-need) |
| 1 | [T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](Research_Papers_Anubhav_Reads/T5_Exploring_the_Limits_of_Transfer_Learning_with_.md) | Read   | Attention, Text , Transformers |              | 2020 | JMLR       | Colin Raffel, Noam Shazeer, Peter J. Liu, Wei Liu, Yanqi Zhou | Presents a Text-to-Text transformer model with multi-task learning capabilities, simultaneously solving problems such as machine translation, document summarization, question answering, and classification tasks. | [link](https://arxiv.org/abs/1910.10683)                           |


---

## Oriol Vinyals

|   | Paper Name                                                                                                                                                                  | Status  | Topic                               | Category                                                              | Year | Conference | Author                                                            | Summary | Link                                         |
| - | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----------------------------------- | --------------------------------------------------------------------- | ---- | ---------- | ----------------------------------------------------------------- | ------- | -------------------------------------------- |
| 0 | [Graph Neural Network: Relational inductive biases, deep learning, and graph networks](Research_Papers_Anubhav_Reads/Graph_Neural_Network_Relational_inductive_biases,_.md) | Pending | GraphNN                             | Architecture                                                          | 2018 | arXiv      | Jessica B. Hamrick, Oriol Vinyals, Peter W. Battaglia             |         | [link](https://arxiv.org/pdf/1806.01261.pdf) |
| 1 | [Training Compute-Optimal Large Language Models](Research_Papers_Anubhav_Reads/Training_Compute-Optimal_Large_Language_Models.md)                                           | Pending | Large-Language-Models, Transformers | Architecture, Optimization-No. of params, Pre-Training, Tips & Tricks | 2022 | arXiv      | Jordan Hoffmann, Laurent Sifre, Oriol Vinyals, Sebastian Borgeaud |         | [link](https://arxiv.org/abs/2203.15556)     |


---

## Other

|   | Paper Name                                                                                                                               | Status  | Topic                                              | Category               | Year | Conference | Author | Summary | Link                                         |
| - | ---------------------------------------------------------------------------------------------------------------------------------------- | ------- | -------------------------------------------------- | ---------------------- | ---- | ---------- | ------ | ------- | -------------------------------------------- |
| 0 | [Table-GPT: Table-tuned GPT for Diverse Table Tasks](Research_Papers_Anubhav_Reads/Table-GPT_Table-tuned_GPT_for_Diverse_Table_Tasks.md) | Pending | Generative, Large-Language-Models, Training Method | Instruction-Finetuning | 2023 | arXiv      | Other  |         | [link](https://arxiv.org/pdf/2310.09263.pdf) |


---

## Pablo Márquez-Neila

|   | Paper Name                                                                                                                                                         | Status  | Topic                               | Category | Year | Conference | Author                                                            | Summary | Link                                                                                                             |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------- | ----------------------------------- | -------- | ---- | ---------- | ----------------------------------------------------------------- | ------- | ---------------------------------------------------------------------------------------------------------------- |
| 0 | [Topological Loss: Beyond the Pixel-Wise Loss for Topology-Aware Delineation](Research_Papers_Anubhav_Reads/Topological_Loss_Beyond_the_Pixel-Wise_Loss_for_To.md) | Pending | Image , Loss Function, Segmentation |          | 2018 | CVPR       | Agata Mosinska, Mateusz Koziński, Pablo Márquez-Neila, Pascal Fua |         | [link](https://openaccess.thecvf.com/content_cvpr_2018/html/Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper.html) |


---

## Pamela Mishkin

|   | Paper Name                                                                                                                                                 | Status  | Topic                                              | Category                                                        | Year | Conference | Author                                                                                                                      | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Link                                         |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | -------------------------------------------------- | --------------------------------------------------------------- | ---- | ---------- | --------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |
| 0 | [Training language models to follow instructions with human feedback](Research_Papers_Anubhav_Reads/Training_language_models_to_follow_instructions_wi.md) | Pending | Generative, Large-Language-Models, Training Method | Instruction-Finetuning, Reinforcement-Learning, Semi-Supervised | 2022 | arXiv      | Carroll L. Wainwright, Diogo Almeida, Jan Leike, Jeff Wu, Long Ouyang, Pamela Mishkin, Paul Christiano, Ryan Lowe, Xu Jiang | This paper presents InstructGPT, a model fine-tuned with human feedback to better align with user intent across various tasks. Despite having significantly fewer parameters than larger models, InstructGPT outperforms them in human evaluations, demonstrating improved truthfulness, reduced toxicity, and minimal performance regressions on public NLP datasets, highlighting the potential of fine-tuning with human feedback for enhancing language model alignment with human intent. | [link](https://arxiv.org/pdf/2203.02155.pdf) |


---

## Pascal Fua

|   | Paper Name                                                                                                                                                         | Status  | Topic                               | Category | Year | Conference | Author                                                            | Summary | Link                                                                                                             |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------- | ----------------------------------- | -------- | ---- | ---------- | ----------------------------------------------------------------- | ------- | ---------------------------------------------------------------------------------------------------------------- |
| 0 | [Topological Loss: Beyond the Pixel-Wise Loss for Topology-Aware Delineation](Research_Papers_Anubhav_Reads/Topological_Loss_Beyond_the_Pixel-Wise_Loss_for_To.md) | Pending | Image , Loss Function, Segmentation |          | 2018 | CVPR       | Agata Mosinska, Mateusz Koziński, Pablo Márquez-Neila, Pascal Fua |         | [link](https://openaccess.thecvf.com/content_cvpr_2018/html/Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper.html) |


---

## Paul Christiano

|   | Paper Name                                                                                                                                                 | Status  | Topic                                              | Category                                                        | Year | Conference | Author                                                                                                                      | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Link                                         |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | -------------------------------------------------- | --------------------------------------------------------------- | ---- | ---------- | --------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |
| 0 | [Training language models to follow instructions with human feedback](Research_Papers_Anubhav_Reads/Training_language_models_to_follow_instructions_wi.md) | Pending | Generative, Large-Language-Models, Training Method | Instruction-Finetuning, Reinforcement-Learning, Semi-Supervised | 2022 | arXiv      | Carroll L. Wainwright, Diogo Almeida, Jan Leike, Jeff Wu, Long Ouyang, Pamela Mishkin, Paul Christiano, Ryan Lowe, Xu Jiang | This paper presents InstructGPT, a model fine-tuned with human feedback to better align with user intent across various tasks. Despite having significantly fewer parameters than larger models, InstructGPT outperforms them in human evaluations, demonstrating improved truthfulness, reduced toxicity, and minimal performance regressions on public NLP datasets, highlighting the potential of fine-tuning with human feedback for enhancing language model alignment with human intent. | [link](https://arxiv.org/pdf/2203.02155.pdf) |


---

## Peng Zhou

|   | Paper Name                                                                                                                                                                | Status  | Topic | Category | Year | Conference | Author                                                              | Summary | Link                                                                                                                                                                                                                                                   |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----- | -------- | ---- | ---------- | ------------------------------------------------------------------- | ------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 0 | [IMLE-GAN: Inclusive GAN: Improving Data and Minority Coverage in Generative Models](Research_Papers_Anubhav_Reads/IMLE-GAN_Inclusive_GAN_Improving_Data_and_Minority.md) | Pending | GANs  |          | 2020 | arXiv      | Jitendra Malik, Ke Li, Larry Davis, Mario Fritz, Ning Yu, Peng Zhou |         | [link](https://arxiv.org/abs/2004.03355?utm_campaign=The%20Batch&utm_medium=email&_hsmi=96406275&_hsenc=p2ANqtz-8ra-5k3I7Hv0hosTfQ1neO9Z10r3yMPB1oQfzpBEfkCQ_i0q0diEm4w21S8WqkMbOASXxQvDTIoqJbBZvX4i7S-exeOg&utm_content=96406275&utm_source=hs_email) |


---

## Peter J. Liu

|   | Paper Name                                                                                                                                                                   | Status | Topic                          | Category | Year | Conference | Author                                                        | Summary                                                                                                                                                                                                             | Link                                     |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------ | ------------------------------ | -------- | ---- | ---------- | ------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](Research_Papers_Anubhav_Reads/T5_Exploring_the_Limits_of_Transfer_Learning_with_.md) | Read   | Attention, Text , Transformers |          | 2020 | JMLR       | Colin Raffel, Noam Shazeer, Peter J. Liu, Wei Liu, Yanqi Zhou | Presents a Text-to-Text transformer model with multi-task learning capabilities, simultaneously solving problems such as machine translation, document summarization, question answering, and classification tasks. | [link](https://arxiv.org/abs/1910.10683) |


---

## Peter W. Battaglia

|   | Paper Name                                                                                                                                                                  | Status  | Topic   | Category     | Year | Conference | Author                                                | Summary | Link                                         |
| - | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------- | ------------ | ---- | ---------- | ----------------------------------------------------- | ------- | -------------------------------------------- |
| 0 | [Graph Neural Network: Relational inductive biases, deep learning, and graph networks](Research_Papers_Anubhav_Reads/Graph_Neural_Network_Relational_inductive_biases,_.md) | Pending | GraphNN | Architecture | 2018 | arXiv      | Jessica B. Hamrick, Oriol Vinyals, Peter W. Battaglia |         | [link](https://arxiv.org/pdf/1806.01261.pdf) |


---

## Peter West

|   | Paper Name                                                                                                                                                                 | Status  | Topic                        | Category                     | Year | Conference | Author                                                   | Summary | Link                                     |
| - | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ---------------------------- | ---------------------------- | ---- | ---------- | -------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Symbolic Knowledge Distillation: from General Language Models to Commonsense Models](Research_Papers_Anubhav_Reads/Symbolic_Knowledge_Distillation_from_General_Langu.md) | Pending | Dataset, Text , Transformers | Optimizations, Tips & Tricks | 2021 | arXiv      | Chandra Bhagavatula, Jack Hessel, Peter West, Yejin Choi |         | [link](https://arxiv.org/abs/2110.07178) |


---

## Peter Wonka

|   | Paper Name                                                                                                                                                 | Status  | Topic        | Category | Year | Conference | Author                                | Summary | Link                                                                                                                                                      |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | -------- | ---- | ---------- | ------------------------------------- | ------- | --------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 0 | [Image2StyleGAN: How to Embed Images Into the StyleGAN Latent Space?](Research_Papers_Anubhav_Reads/Image2StyleGAN_How_to_Embed_Images_Into_the_StyleG.md) | Pending | GANs, Image  |          | 2019 | ICCV       | Peter Wonka, Rameen Abdal, Yipeng Qin |         | [link](https://openaccess.thecvf.com/content_ICCV_2019/html/Abdal_Image2StyleGAN_How_to_Embed_Images_Into_the_StyleGAN_Latent_Space_ICCV_2019_paper.html) |


---

## Phillip Isola

|   | Paper Name                                                                                                                                                                       | Status  | Topic        | Category     | Year | Conference | Author                                                    | Summary                                                                                                  | Link                                                                                                                      |
| - | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | ------------ | ---- | ---------- | --------------------------------------------------------- | -------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------- |
| 0 | [Pix2Pix: Image-to-Image Translation with Conditional Adversarial Nets](Research_Papers_Anubhav_Reads/Pix2Pix_Image-to-Image_Translation_with_Conditiona.md)                     | Read    | GANs, Image  |              | 2017 | CVPR       | Alexei A. Efros, Jun-Yan Zhu, Phillip Isola, Tinghui Zhou | Image to image translation using Conditional GANs and dataset of image pairs from one domain to another. | [link](https://arxiv.org/abs/1611.07004)                                                                                  |
| 1 | [CycleGAN: Unpaired Image-To-Image Translation Using Cycle-Consistent Adversarial Networks](Research_Papers_Anubhav_Reads/CycleGAN_Unpaired_Image-To-Image_Translation_Using.md) | Pending | GANs, Image  | Architecture | 2017 | ICCV       | Alexei A. Efros, Jun-Yan Zhu, Phillip Isola, Taesung Park |                                                                                                          | [link](https://openaccess.thecvf.com/content_iccv_2017/html/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.html) |


---

## Ping Yu

|   | Paper Name                                                                                                                          | Status  | Topic                                              | Category               | Year | Conference | Author                                     | Summary                                                                                                                                                                                                                                                                                                                                                                                                       | Link                                         |
| - | ----------------------------------------------------------------------------------------------------------------------------------- | ------- | -------------------------------------------------- | ---------------------- | ---- | ---------- | ------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |
| 0 | [Self-Alignment with Instruction Backtranslation](Research_Papers_Anubhav_Reads/Self-Alignment_with_Instruction_Backtranslation.md) | Pending | Generative, Large-Language-Models, Training Method | Instruction-Finetuning | 2023 | arXiv      | Jason Weston, Mike Lewis, Ping Yu, Xian Li | The paper introduces a scalable method called "instruction backtranslation" to create a high-quality instruction-following language model. This method involves self-augmentation and self-curation of training examples generated from web documents, resulting in a model that outperforms others in its category without relying on distillation data, showcasing its effective self-alignment capability. | [link](https://arxiv.org/pdf/2308.06259.pdf) |


---

## Preetum Nakkiran

|   | Paper Name                                                                                                                                         | Status  | Topic | Category | Year | Conference | Author                                                                                | Summary | Link                                     |
| - | -------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----- | -------- | ---- | ---------- | ------------------------------------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Deep Double Descent: Where Bigger Models and More Data Hurt](Research_Papers_Anubhav_Reads/Deep_Double_Descent_Where_Bigger_Models_and_More_D.md) | Pending | NNs   |          | 2019 | arXiv      | Boaz Barak, Gal Kaplun, Ilya Sutskever, Preetum Nakkiran, Tristan Yang, Yamini Bansal |         | [link](https://arxiv.org/abs/1912.02292) |


---

## Qifan Wang

|   | Paper Name                                                                                                                                                                            | Status | Topic                                   | Category           | Year | Conference | Author              | Summary                                                                                                                                                  | Link                                                       |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------ | --------------------------------------- | ------------------ | ---- | ---------- | ------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------- |
| 0 | [Learning to Extract Attribute Value from Product via Question Answering: A Multi-task Approach](Research_Papers_Anubhav_Reads/Learning_to_Extract_Attribute_Value_from_Product_v.md) | Read   | Question-Answering, Text , Transformers | Zero-shot-learning | 2020 | KDD        | Li Yang, Qifan Wang | Question Answering BERT model used to extract attributes from products. Introduce further No Answer loss and distillation to promote zero shot learning. | [link](https://dl.acm.org/doi/pdf/10.1145/3394486.3403047) |


---

## Quoc V. Le

|   | Paper Name                                                                                                                                 | Status  | Topic                                                                      | Category               | Year | Conference | Author                                                                       | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------ | ------- | -------------------------------------------------------------------------- | ---------------------- | ---- | ---------- | ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [Scaling Instruction-Finetuned Language Models (FLAN)](Research_Papers_Anubhav_Reads/Scaling_Instruction-Finetuned_Language_Models_FLA.md) | Pending | Generative, Large-Language-Models, Question-Answering, Text , Transformers | Instruction-Finetuning | 2022 | arXiv      | Hyung Won Chung, Jason Wei, Jeffrey Dean, Le Hou, Quoc V. Le, Shayne Longpre | https://arxiv.org/abs/2210.11416 introduces FLAN (Fine-tuned LAnguage Net), an instruction finetuning method, and presents the results of its application. The study demonstrates that by fine-tuning the 540B PaLM model on 1836 tasks while incorporating Chain-of-Thought Reasoning data, FLAN achieves improvements in generalization, human usability, and zero-shot reasoning over the base model. The paper also provides detailed information on how each these aspects was evaluated. | [link](https://arxiv.org/abs/2210.11416) |


---

## Rameen Abdal

|   | Paper Name                                                                                                                                                 | Status  | Topic        | Category | Year | Conference | Author                                | Summary | Link                                                                                                                                                      |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | -------- | ---- | ---------- | ------------------------------------- | ------- | --------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 0 | [Image2StyleGAN: How to Embed Images Into the StyleGAN Latent Space?](Research_Papers_Anubhav_Reads/Image2StyleGAN_How_to_Embed_Images_Into_the_StyleG.md) | Pending | GANs, Image  |          | 2019 | ICCV       | Peter Wonka, Rameen Abdal, Yipeng Qin |         | [link](https://openaccess.thecvf.com/content_ICCV_2019/html/Abdal_Image2StyleGAN_How_to_Embed_Images_Into_the_StyleGAN_Latent_Space_ICCV_2019_paper.html) |


---

## Rayat Hossain

|   | Paper Name                                                                                                                                          | Status  | Topic                | Category | Year | Conference | Author                                                          | Summary | Link                                                                                                    |
| - | --------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | -------------------- | -------- | ---- | ---------- | --------------------------------------------------------------- | ------- | ------------------------------------------------------------------------------------------------------- |
| 0 | [A Simple yet Effective Baseline for 3D Human Pose Estimation](Research_Papers_Anubhav_Reads/A_Simple_yet_Effective_Baseline_for_3D_Human_Pose_.md) | Pending | CV , Pose Estimation |          | 2017 | ICCV       | James J. Little, Javier Romero, Julieta Martinez, Rayat Hossain |         | [link](https://openaccess.thecvf.com/content_iccv_2017/html/Martinez_A_Simple_yet_ICCV_2017_paper.html) |


---

## Rob Fergus

|   | Paper Name                                                                                                                                          | Status | Topic             | Category      | Year | Conference | Author                        | Summary                                                                          | Link                                                                   |
| - | --------------------------------------------------------------------------------------------------------------------------------------------------- | ------ | ----------------- | ------------- | ---- | ---------- | ----------------------------- | -------------------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| 0 | [ZF Net (Visualizing and Understanding Convolutional Networks)](Research_Papers_Anubhav_Reads/ZF_Net_Visualizing_and_Understanding_Convolutiona.md) | Read   | CNNs, CV , Image  | Visualization | 2014 | ECCV       | Matthew D. Zeiler, Rob Fergus | Visualize CNN Filters / Kernels using De-Convolutions on CNN filter activations. | [link](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53) |


---

## Ronan Le Bras

|   | Paper Name                                                                                                                                           | Status  | Topic               | Category | Year | Conference | Author                                                | Summary | Link                                         |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------------- | -------- | ---- | ---------- | ----------------------------------------------------- | ------- | -------------------------------------------- |
| 0 | [ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning](Research_Papers_Anubhav_Reads/ATOMIC_An_Atlas_of_Machine_Commonsense_for_If-Then.md) | Pending | AGI, Dataset, Text  |          | 2019 | AAAI       | Maarten Sap, Noah A. Smith, Ronan Le Bras, Yejin Choi |         | [link](https://arxiv.org/pdf/1811.00146.pdf) |


---

## Rosanne Liu

|   | Paper Name                                                                                                                                             | Status | Topic                  | Category                                              | Year | Conference | Author                                               | Summary                                                                                                                          | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------ | ------ | ---------------------- | ----------------------------------------------------- | ---- | ---------- | ---------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask](Research_Papers_Anubhav_Reads/Deconstructing_Lottery_Tickets_Zeros,_Signs,_and_t.md) | Read   | NN Initialization, NNs | Comparison, Optimization-No. of params, Tips & Tricks | 2019 | NeurIPS    | Hattie Zhou, Janice Lan, Jason Yosinski, Rosanne Liu | Follow up on Lottery Ticket Hypothesis exploring the effects of different Masking criteria as well as Mask-1 and Mask-0 actions. | [link](https://arxiv.org/abs/1905.01067) |


---

## Ryan Lowe

|   | Paper Name                                                                                                                                                 | Status  | Topic                                              | Category                                                        | Year | Conference | Author                                                                                                                      | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Link                                         |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | -------------------------------------------------- | --------------------------------------------------------------- | ---- | ---------- | --------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |
| 0 | [Training language models to follow instructions with human feedback](Research_Papers_Anubhav_Reads/Training_language_models_to_follow_instructions_wi.md) | Pending | Generative, Large-Language-Models, Training Method | Instruction-Finetuning, Reinforcement-Learning, Semi-Supervised | 2022 | arXiv      | Carroll L. Wainwright, Diogo Almeida, Jan Leike, Jeff Wu, Long Ouyang, Pamela Mishkin, Paul Christiano, Ryan Lowe, Xu Jiang | This paper presents InstructGPT, a model fine-tuned with human feedback to better align with user intent across various tasks. Despite having significantly fewer parameters than larger models, InstructGPT outperforms them in human evaluations, demonstrating improved truthfulness, reduced toxicity, and minimal performance regressions on public NLP datasets, highlighting the potential of fine-tuning with human feedback for enhancing language model alignment with human intent. | [link](https://arxiv.org/pdf/2203.02155.pdf) |


---

## Samuli Laine

|   | Paper Name                                                                                                                                                                | Status  | Topic        | Category      | Year | Conference | Author                                                | Summary | Link                                                                                                                                                              |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | ------------- | ---- | ---------- | ----------------------------------------------------- | ------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 0 | [StyleGAN: A Style-Based Generator Architecture for Generative Adversarial Networks](Research_Papers_Anubhav_Reads/StyleGAN_A_Style-Based_Generator_Architecture_for_.md) | Pending | GANs, Image  |               | 2019 | CVPR       | Samuli Laine, Tero Karras, Timo Aila                  |         | [link](https://openaccess.thecvf.com/content_CVPR_2019/html/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.html) |
| 1 | [Progressive Growing of GANs for Improved Quality, Stability, and Variation](Research_Papers_Anubhav_Reads/Progressive_Growing_of_GANs_for_Improved_Quality,_.md)         | Pending | GANs, Image  | Tips & Tricks | 2018 | ICLR       | Jaakko Lehtinen, Samuli Laine, Tero Karras, Timo Aila |         | [link](https://arxiv.org/abs/1710.10196)                                                                                                                          |


---

## Santhosh K. Ramakrishnan

|   | Paper Name                                                                                                                                             | Status  | Topic        | Category               | Year | Conference | Author                                                   | Summary | Link                                         |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------ | ------- | ------------ | ---------------------- | ---- | ---------- | -------------------------------------------------------- | ------- | -------------------------------------------- |
| 0 | [Occupancy Anticipation for Efficient Exploration and Navigation](Research_Papers_Anubhav_Reads/Occupancy_Anticipation_for_Efficient_Exploration_a.md) | Pending | CNNs, Image  | Reinforcement-Learning | 2020 | ECCV       | Kristen Grauman, Santhosh K. Ramakrishnan, Ziad Al-Halah |         | [link](https://arxiv.org/pdf/2008.09285.pdf) |


---

## Sara Sabour

|   | Paper Name                                                                                                                               | Status  | Topic       | Category     | Year | Conference | Author                                          | Summary | Link                                     |
| - | ---------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----------- | ------------ | ---- | ---------- | ----------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Capsule Networks: Dynamic Routing Between Capsules](Research_Papers_Anubhav_Reads/Capsule_Networks_Dynamic_Routing_Between_Capsules.md) | Pending | CV , Image  | Architecture | 2017 | arXiv      | Geoffrey E Hinton, Nicholas Frosst, Sara Sabour |         | [link](https://arxiv.org/abs/1710.09829) |


---

## Scott Gray

|   | Paper Name                                                                                             | Status  | Topic                       | Category | Year | Conference | Author                                                                 | Summary | Link                                    |
| - | ------------------------------------------------------------------------------------------------------ | ------- | --------------------------- | -------- | ---- | ---------- | ---------------------------------------------------------------------- | ------- | --------------------------------------- |
| 0 | [DALL·E: Creating Images from Text](Research_Papers_Anubhav_Reads/DALL·E_Creating_Images_from_Text.md) | Pending | Image , Text , Transformers |          | 2021 | Blog       | Aditya Ramesh, Gabriel Goh, Ilya Sutskever, Mikhail Pavlov, Scott Gray |         | [link](https://openai.com/blog/dall-e/) |


---

## Sebastian Borgeaud

|   | Paper Name                                                                                                                        | Status  | Topic                               | Category                                                              | Year | Conference | Author                                                            | Summary | Link                                     |
| - | --------------------------------------------------------------------------------------------------------------------------------- | ------- | ----------------------------------- | --------------------------------------------------------------------- | ---- | ---------- | ----------------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Training Compute-Optimal Large Language Models](Research_Papers_Anubhav_Reads/Training_Compute-Optimal_Large_Language_Models.md) | Pending | Large-Language-Models, Transformers | Architecture, Optimization-No. of params, Pre-Training, Tips & Tricks | 2022 | arXiv      | Jordan Hoffmann, Laurent Sifre, Oriol Vinyals, Sebastian Borgeaud |         | [link](https://arxiv.org/abs/2203.15556) |


---

## Serge Belongie

|   | Paper Name                                                                                                                                                        | Status  | Topic        | Category | Year | Conference | Author                    | Summary | Link                                                                                                             |
| - | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | -------- | ---- | ---------- | ------------------------- | ------- | ---------------------------------------------------------------------------------------------------------------- |
| 0 | [Arbitrary Style Transfer in Real-Time With Adaptive Instance Normalization](Research_Papers_Anubhav_Reads/Arbitrary_Style_Transfer_in_Real-Time_With_Adaptiv.md) | Pending | CNNs, Image  |          | 2017 | ICCV       | Serge Belongie, Xun Huang |         | [link](https://openaccess.thecvf.com/content_iccv_2017/html/Huang_Arbitrary_Style_Transfer_ICCV_2017_paper.html) |


---

## Shayne Longpre

|   | Paper Name                                                                                                                                 | Status  | Topic                                                                      | Category               | Year | Conference | Author                                                                       | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------ | ------- | -------------------------------------------------------------------------- | ---------------------- | ---- | ---------- | ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [Scaling Instruction-Finetuned Language Models (FLAN)](Research_Papers_Anubhav_Reads/Scaling_Instruction-Finetuned_Language_Models_FLA.md) | Pending | Generative, Large-Language-Models, Question-Answering, Text , Transformers | Instruction-Finetuning | 2022 | arXiv      | Hyung Won Chung, Jason Wei, Jeffrey Dean, Le Hou, Quoc V. Le, Shayne Longpre | https://arxiv.org/abs/2210.11416 introduces FLAN (Fine-tuned LAnguage Net), an instruction finetuning method, and presents the results of its application. The study demonstrates that by fine-tuning the 540B PaLM model on 1836 tasks while incorporating Chain-of-Thought Reasoning data, FLAN achieves improvements in generalization, human usability, and zero-shot reasoning over the base model. The paper also provides detailed information on how each these aspects was evaluated. | [link](https://arxiv.org/abs/2210.11416) |


---

## Shibani Santurkar

|   | Paper Name                                                                                                                         | Status  | Topic              | Category      | Year | Conference | Author                                                              | Summary | Link                                     |
| - | ---------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------------ | ------------- | ---- | ---------- | ------------------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [How Does Batch Normalization Help Optimization?](Research_Papers_Anubhav_Reads/How_Does_Batch_Normalization_Help_Optimization.md) | Pending | NNs, Normalization | Optimizations | 2018 | arXiv      | Aleksander Madry, Andrew Ilyas, Dimitris Tsipras, Shibani Santurkar |         | [link](https://arxiv.org/abs/1805.11604) |


---

## Shiyu Chang

|   | Paper Name                                                                                                                               | Status  | Topic                      | Category     | Year | Conference | Author                                   | Summary | Link                                     |
| - | ---------------------------------------------------------------------------------------------------------------------------------------- | ------- | -------------------------- | ------------ | ---- | ---------- | ---------------------------------------- | ------- | ---------------------------------------- |
| 0 | [TransGAN: Two Transformers Can Make One Strong GAN](Research_Papers_Anubhav_Reads/TransGAN_Two_Transformers_Can_Make_One_Strong_GAN.md) | Pending | GANs, Image , Transformers | Architecture | 2021 | arXiv      | Shiyu Chang, Yifan Jiang, Zhangyang Wang |         | [link](https://arxiv.org/abs/2102.07074) |


---

## Shunyu Yao

|   | Paper Name                                                                                                                                        | Status  | Topic                                    | Category                     | Year | Conference | Author                                                                                 | Summary                                                                                                                                                                                                                                                                                                                                                  | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ---------------------------------------- | ---------------------------- | ---- | ---------- | -------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [ReAct: Synergizing Reasoning and Acting in Language Models](Research_Papers_Anubhav_Reads/ReAct_Synergizing_Reasoning_and_Acting_in_Language.md) | Pending | Generative, Large-Language-Models, Text  | Optimizations, Tips & Tricks | 2023 | ICLR       | Dian Yu, Izhak Shafran, Jeffrey Zhao, Karthik Narasimhan, Nan Du, Shunyu Yao, Yuan Cao | This paper introduces ReAct, a novel approach that leverages Large Language Models (LLMs) to interleave reasoning traces and task-specific actions. ReAct outperforms existing methods on various language and decision-making tasks, addressing issues like hallucination, error propagation, and improving human interpretability and trustworthiness. | [link](https://arxiv.org/abs/2210.03629) |


---

## Song Han

|   | Paper Name                                                | Status | Topic             | Category                                 | Year | Conference | Author                       | Summary                                                                   | Link                                     |
| - | --------------------------------------------------------- | ------ | ----------------- | ---------------------------------------- | ---- | ---------- | ---------------------------- | ------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [SqueezeNet](Research_Papers_Anubhav_Reads/SqueezeNet.md) | Read   | CNNs, CV , Image  | Architecture, Optimization-No. of params | 2016 | arXiv      | Forrest N. Iandola, Song Han | Explores model compression by using 1x1 convolutions called fire modules. | [link](https://arxiv.org/abs/1602.07360) |


---

## Soumith Chintala

|   | Paper Name                                                                     | Status  | Topic               | Category | Year | Conference | Author                                         | Summary | Link                                     |
| - | ------------------------------------------------------------------------------ | ------- | ------------------- | -------- | ---- | ---------- | ---------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [WGAN: Wasserstein GAN](Research_Papers_Anubhav_Reads/WGAN_Wasserstein_GAN.md) | Pending | GANs, Loss Function |          | 2017 | arXiv      | Léon Bottou, Martin Arjovsky, Soumith Chintala |         | [link](https://arxiv.org/abs/1701.07875) |


---

## Sowmya Yellapragada

|   | Paper Name                                                                                                                          | Status  | Topic                            | Category                  | Year | Conference | Author              | Summary | Link                                                                                                                           |
| - | ----------------------------------------------------------------------------------------------------------------------------------- | ------- | -------------------------------- | ------------------------- | ---- | ---------- | ------------------- | ------- | ------------------------------------------------------------------------------------------------------------------------------ |
| 0 | [Understanding Loss Functions in Computer Vision](Research_Papers_Anubhav_Reads/Understanding_Loss_Functions_in_Computer_Vision.md) | Pending | CV , GANs, Image , Loss Function | Comparison, Tips & Tricks | 2020 | Blog       | Sowmya Yellapragada |         | [link](https://medium.com/ml-cheat-sheet/winning-at-loss-functions-2-important-loss-functions-in-computer-vision-b2b9d293e15a) |


---

## Stanislas Polu

|   | Paper Name                                                                                                                                               | Status  | Topic                   | Category | Year | Conference | Author                         | Summary | Link                                     |
| - | -------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----------------------- | -------- | ---- | ---------- | ------------------------------ | ------- | ---------------------------------------- |
| 0 | [GPT-f: Generative Language Modeling for Automated Theorem Proving](Research_Papers_Anubhav_Reads/GPT-f_Generative_Language_Modeling_for_Automated_T.md) | Pending | Attention, Transformers |          | 2020 | arXiv      | Ilya Sutskever, Stanislas Polu |         | [link](https://arxiv.org/abs/2009.03393) |


---

## Stephen Merity

|   | Paper Name                                                                                                                                       | Status  | Topic                   | Category                   | Year | Conference | Author         | Summary | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------ | ------- | ----------------------- | -------------------------- | ---- | ---------- | -------------- | ------- | ---------------------------------------- |
| 0 | [Single Headed Attention RNN: Stop Thinking With Your Head](Research_Papers_Anubhav_Reads/Single_Headed_Attention_RNN_Stop_Thinking_With_You.md) | Pending | Attention, LSTMs, Text  | Optimization-No. of params | 2019 | arXiv      | Stephen Merity |         | [link](https://arxiv.org/abs/1911.11423) |


---

## Sudharshan Chandra Babu

|   | Paper Name                                                                                                                                      | Status  | Topic                | Category   | Year | Conference | Author                  | Summary | Link                                                              |
| - | ----------------------------------------------------------------------------------------------------------------------------------------------- | ------- | -------------------- | ---------- | ---- | ---------- | ----------------------- | ------- | ----------------------------------------------------------------- |
| 0 | [A 2019 guide to Human Pose Estimation with Deep Learning](Research_Papers_Anubhav_Reads/A_2019_guide_to_Human_Pose_Estimation_with_Deep_Le.md) | Pending | CV , Pose Estimation | Comparison | 2019 | Blog       | Sudharshan Chandra Babu |         | [link](https://nanonets.com/blog/human-pose-estimation-2d-guide/) |


---

## Sunghun Kim

|   | Paper Name                                                                                                                                                                          | Status  | Topic        | Category | Year | Conference | Author                                                                       | Summary | Link                                                                                                               |
| - | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | -------- | ---- | ---------- | ---------------------------------------------------------------------------- | ------- | ------------------------------------------------------------------------------------------------------------------ |
| 0 | [StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation](Research_Papers_Anubhav_Reads/StarGAN_Unified_Generative_Adversarial_Networks_fo.md) | Pending | GANs, Image  |          | 2018 | CVPR       | Jaegul Choo, Jung-Woo Ha, Minje Choi, Munyoung Kim, Sunghun Kim, Yunjey Choi |         | [link](https://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_StarGAN_Unified_Generative_CVPR_2018_paper.pdf) |


---

## Taesung Park

|   | Paper Name                                                                                                                                                                       | Status  | Topic        | Category     | Year | Conference | Author                                                    | Summary | Link                                                                                                                      |
| - | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | ------------ | ---- | ---------- | --------------------------------------------------------- | ------- | ------------------------------------------------------------------------------------------------------------------------- |
| 0 | [CycleGAN: Unpaired Image-To-Image Translation Using Cycle-Consistent Adversarial Networks](Research_Papers_Anubhav_Reads/CycleGAN_Unpaired_Image-To-Image_Translation_Using.md) | Pending | GANs, Image  | Architecture | 2017 | ICCV       | Alexei A. Efros, Jun-Yan Zhu, Phillip Isola, Taesung Park |         | [link](https://openaccess.thecvf.com/content_iccv_2017/html/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.html) |


---

## Takeru Miyato

|   | Paper Name                                                                                          | Status  | Topic               | Category      | Year | Conference | Author                                                          | Summary | Link                                     |
| - | --------------------------------------------------------------------------------------------------- | ------- | ------------------- | ------------- | ---- | ---------- | --------------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Spectral Normalization for GANs](Research_Papers_Anubhav_Reads/Spectral_Normalization_for_GANs.md) | Pending | GANs, Normalization | Optimizations | 2018 | arXiv      | Masanori Koyama, Takeru Miyato, Toshiki Kataoka, Yuichi Yoshida |         | [link](https://arxiv.org/abs/1802.05957) |


---

## Takeshi Kojima

|   | Paper Name                                                                                                                      | Status  | Topic                                 | Category                          | Year | Conference | Author                         | Summary | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------------------------------- | --------------------------------- | ---- | ---------- | ------------------------------ | ------- | ---------------------------------------- |
| 0 | [Large Language Models are Zero-Shot Reasoners](Research_Papers_Anubhav_Reads/Large_Language_Models_are_Zero-Shot_Reasoners.md) | Pending | Generative, Question-Answering, Text  | Tips & Tricks, Zero-shot-learning | 2022 | arXiv      | Takeshi Kojima, Yusuke Iwasawa |         | [link](https://arxiv.org/abs/2205.11916) |


---

## Tal Ridnik

|   | Paper Name                                                                                                                                                           | Status  | Topic                 | Category                 | Year | Conference | Author                                  | Summary                                                                                                                                                                                      | Link                                     |
| - | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | --------------------- | ------------------------ | ---- | ---------- | --------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering](Research_Papers_Anubhav_Reads/Code_Generation_with_AlphaCodium_From_Prompt_Engin.md) | Pending | Large-Language-Models | Prompting, Tips & Tricks | 2024 | arXiv      | Dedy Kredo, Itamar Friedman, Tal Ridnik | This paper introduces AlphaCodium, a novel test-based, multi-stage, code-oriented iterative approach for improving the performance of Language Model Models (LLMs) on code generation tasks. | [link](https://arxiv.org/abs/2401.08500) |


---

## Tero Karras

|   | Paper Name                                                                                                                                                                | Status  | Topic        | Category      | Year | Conference | Author                                                | Summary | Link                                                                                                                                                              |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | ------------- | ---- | ---------- | ----------------------------------------------------- | ------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 0 | [StyleGAN: A Style-Based Generator Architecture for Generative Adversarial Networks](Research_Papers_Anubhav_Reads/StyleGAN_A_Style-Based_Generator_Architecture_for_.md) | Pending | GANs, Image  |               | 2019 | CVPR       | Samuli Laine, Tero Karras, Timo Aila                  |         | [link](https://openaccess.thecvf.com/content_CVPR_2019/html/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.html) |
| 1 | [Progressive Growing of GANs for Improved Quality, Stability, and Variation](Research_Papers_Anubhav_Reads/Progressive_Growing_of_GANs_for_Improved_Quality,_.md)         | Pending | GANs, Image  | Tips & Tricks | 2018 | ICLR       | Jaakko Lehtinen, Samuli Laine, Tero Karras, Timo Aila |         | [link](https://arxiv.org/abs/1710.10196)                                                                                                                          |


---

## Thomas Schumm

|   | Paper Name                                                                                                                                         | Status  | Topic        | Category | Year | Conference | Author                                    | Summary | Link                                     |
| - | -------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | -------- | ---- | ---------- | ----------------------------------------- | ------- | ---------------------------------------- |
| 0 | [BEGAN: Boundary Equilibrium Generative Adversarial Networks](Research_Papers_Anubhav_Reads/BEGAN_Boundary_Equilibrium_Generative_Adversarial_.md) | Pending | GANs, Image  |          | 2017 | arXiv      | David Berthelot, Luke Metz, Thomas Schumm |         | [link](https://arxiv.org/abs/1703.10717) |


---

## Thomas Unterthiner

|   | Paper Name                                                                                            | Status  | Topic                        | Category                     | Year | Conference | Author                                             | Summary | Link                                                                           |
| - | ----------------------------------------------------------------------------------------------------- | ------- | ---------------------------- | ---------------------------- | ---- | ---------- | -------------------------------------------------- | ------- | ------------------------------------------------------------------------------ |
| 0 | [Self-Normalizing Neural Networks](Research_Papers_Anubhav_Reads/Self-Normalizing_Neural_Networks.md) | Pending | Activation Function, Tabular | Optimizations, Tips & Tricks | 2017 | NIPS       | Andreas Mayr, Günter Klambauer, Thomas Unterthiner |         | [link](https://papers.nips.cc/paper/6698-self-normalizing-neural-networks.pdf) |


---

## Tim Salimans

|   | Paper Name                                                                                                      | Status  | Topic        | Category        | Year | Conference | Author                                                                              | Summary | Link                                                                           |
| - | --------------------------------------------------------------------------------------------------------------- | ------- | ------------ | --------------- | ---- | ---------- | ----------------------------------------------------------------------------------- | ------- | ------------------------------------------------------------------------------ |
| 0 | [Improved Techniques for Training GANs](Research_Papers_Anubhav_Reads/Improved_Techniques_for_Training_GANs.md) | Pending | GANs, Image  | Semi-Supervised | 2016 | NIPS       | Alec Radford, Ian Goodfellow, Tim Salimans, Vicki Cheung, Wojciech Zaremba, Xi Chen |         | [link](http://papers.nips.cc/paper/6124-improved-techniques-for-training-gans) |


---

## Timo Aila

|   | Paper Name                                                                                                                                                                | Status  | Topic        | Category      | Year | Conference | Author                                                | Summary | Link                                                                                                                                                              |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | ------------- | ---- | ---------- | ----------------------------------------------------- | ------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 0 | [StyleGAN: A Style-Based Generator Architecture for Generative Adversarial Networks](Research_Papers_Anubhav_Reads/StyleGAN_A_Style-Based_Generator_Architecture_for_.md) | Pending | GANs, Image  |               | 2019 | CVPR       | Samuli Laine, Tero Karras, Timo Aila                  |         | [link](https://openaccess.thecvf.com/content_CVPR_2019/html/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.html) |
| 1 | [Progressive Growing of GANs for Improved Quality, Stability, and Variation](Research_Papers_Anubhav_Reads/Progressive_Growing_of_GANs_for_Improved_Quality,_.md)         | Pending | GANs, Image  | Tips & Tricks | 2018 | ICLR       | Jaakko Lehtinen, Samuli Laine, Tero Karras, Timo Aila |         | [link](https://arxiv.org/abs/1710.10196)                                                                                                                          |


---

## Timothy Dozat

|   | Paper Name                                                                                                                           | Status  | Topic           | Category   | Year | Conference | Author        | Summary | Link                                                      |
| - | ------------------------------------------------------------------------------------------------------------------------------------ | ------- | --------------- | ---------- | ---- | ---------- | ------------- | ------- | --------------------------------------------------------- |
| 0 | [NADAM: Incorporating Nesterov Momentum into Adam](Research_Papers_Anubhav_Reads/NADAM_Incorporating_Nesterov_Momentum_into_Adam.md) | Pending | NNs, Optimizers | Comparison | 2016 |            | Timothy Dozat |         | [link](http://cs229.stanford.edu/proj2015/054_report.pdf) |


---

## Tinghui Zhou

|   | Paper Name                                                                                                                                                   | Status | Topic        | Category | Year | Conference | Author                                                    | Summary                                                                                                  | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------ | ------------ | -------- | ---- | ---------- | --------------------------------------------------------- | -------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [Pix2Pix: Image-to-Image Translation with Conditional Adversarial Nets](Research_Papers_Anubhav_Reads/Pix2Pix_Image-to-Image_Translation_with_Conditiona.md) | Read   | GANs, Image  |          | 2017 | CVPR       | Alexei A. Efros, Jun-Yan Zhu, Phillip Isola, Tinghui Zhou | Image to image translation using Conditional GANs and dataset of image pairs from one domain to another. | [link](https://arxiv.org/abs/1611.07004) |


---

## Tomas Mikolov

|   | Paper Name                                                                                                                                                    | Status  | Topic | Category                  | Year | Conference | Author                                              | Summary | Link                                    |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----- | ------------------------- | ---- | ---------- | --------------------------------------------------- | ------- | --------------------------------------- |
| 0 | [Word2Vec: Efficient Estimation of Word Representations in Vector Space](Research_Papers_Anubhav_Reads/Word2Vec_Efficient_Estimation_of_Word_Representati.md) | Pending | Text  | Embeddings, Tips & Tricks | 2013 | arXiv      | Greg Corrado, Jeffrey Dean, Kai Chen, Tomas Mikolov |         | [link](https://arxiv.org/abs/1301.3781) |


---

## Tong He

|   | Paper Name                                                                                                                                                       | Status | Topic       | Category                     | Year | Conference | Author             | Summary                                                                                        | Link                                     |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------ | ----------- | ---------------------------- | ---- | ---------- | ------------------ | ---------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [Bag of Tricks for Image Classification with Convolutional Neural Networks](Research_Papers_Anubhav_Reads/Bag_of_Tricks_for_Image_Classification_with_Convol.md) | Read   | CV , Image  | Optimizations, Tips & Tricks | 2018 | arXiv      | Tong He, Zhi Zhang | Shows a dozen tricks (mixup, label smoothing, etc.) to improve CNN accuracy and training time. | [link](https://arxiv.org/abs/1812.01187) |


---

## Toshiki Kataoka

|   | Paper Name                                                                                          | Status  | Topic               | Category      | Year | Conference | Author                                                          | Summary | Link                                     |
| - | --------------------------------------------------------------------------------------------------- | ------- | ------------------- | ------------- | ---- | ---------- | --------------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Spectral Normalization for GANs](Research_Papers_Anubhav_Reads/Spectral_Normalization_for_GANs.md) | Pending | GANs, Normalization | Optimizations | 2018 | arXiv      | Masanori Koyama, Takeru Miyato, Toshiki Kataoka, Yuichi Yoshida |         | [link](https://arxiv.org/abs/1802.05957) |


---

## Tristan Yang

|   | Paper Name                                                                                                                                         | Status  | Topic | Category | Year | Conference | Author                                                                                | Summary | Link                                     |
| - | -------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----- | -------- | ---- | ---------- | ------------------------------------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Deep Double Descent: Where Bigger Models and More Data Hurt](Research_Papers_Anubhav_Reads/Deep_Double_Descent_Where_Bigger_Models_and_More_D.md) | Pending | NNs   |          | 2019 | arXiv      | Boaz Barak, Gal Kaplun, Ilya Sutskever, Preetum Nakkiran, Tristan Yang, Yamini Bansal |         | [link](https://arxiv.org/abs/1912.02292) |


---

## Vicki Cheung

|   | Paper Name                                                                                                      | Status  | Topic        | Category        | Year | Conference | Author                                                                              | Summary | Link                                                                           |
| - | --------------------------------------------------------------------------------------------------------------- | ------- | ------------ | --------------- | ---- | ---------- | ----------------------------------------------------------------------------------- | ------- | ------------------------------------------------------------------------------ |
| 0 | [Improved Techniques for Training GANs](Research_Papers_Anubhav_Reads/Improved_Techniques_for_Training_GANs.md) | Pending | GANs, Image  | Semi-Supervised | 2016 | NIPS       | Alec Radford, Ian Goodfellow, Tim Salimans, Vicki Cheung, Wojciech Zaremba, Xi Chen |         | [link](http://papers.nips.cc/paper/6124-improved-techniques-for-training-gans) |


---

## Wei Lin

|   | Paper Name                                                                                                                                                                                      | Status  | Topic         | Category | Year | Conference | Author                                                  | Summary | Link                                     |
| - | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------- | -------- | ---- | ---------- | ------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [One-shot Text Field Labeling using Attention and Belief Propagation for Structure Information Extraction](Research_Papers_Anubhav_Reads/One-shot_Text_Field_Labeling_using_Attention_and_B.md) | Pending | Image , Text  |          | 2020 | arXiv      | Jun Huang, Mengli Cheng, Minghui Qiu, Wei Lin, Xing Shi |         | [link](https://arxiv.org/abs/2009.04153) |


---

## Wei Liu

|   | Paper Name                                                                                                                                                                   | Status | Topic                          | Category     | Year | Conference | Author                                                        | Summary                                                                                                                                                                                                             | Link                                     |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------ | ------------------------------ | ------------ | ---- | ---------- | ------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [Inception-v1 (Going Deeper With Convolutions)](Research_Papers_Anubhav_Reads/Inception-v1_Going_Deeper_With_Convolutions.md)                                                | Read   | CNNs, CV , Image               | Architecture | 2015 | CVPR       | Christian Szegedy, Wei Liu                                    | Propose the use of 1x1 conv operations to reduce the number of parameters in a deep and wide CNN                                                                                                                    | [link](https://arxiv.org/abs/1409.4842)  |
| 1 | [T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](Research_Papers_Anubhav_Reads/T5_Exploring_the_Limits_of_Transfer_Learning_with_.md) | Read   | Attention, Text , Transformers |              | 2020 | JMLR       | Colin Raffel, Noam Shazeer, Peter J. Liu, Wei Liu, Yanqi Zhou | Presents a Text-to-Text transformer model with multi-task learning capabilities, simultaneously solving problems such as machine translation, document summarization, question answering, and classification tasks. | [link](https://arxiv.org/abs/1910.10683) |


---

## Wieland Brendel

|   | Paper Name                                                                                                                                                                      | Status  | Topic             | Category | Year | Conference | Author                           | Summary | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----------------- | -------- | ---- | ---------- | -------------------------------- | ------- | ---------------------------------------- |
| 0 | [Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet](Research_Papers_Anubhav_Reads/Approximating_CNNs_with_Bag-of-local-Features_mode.md) | Reading | CNNs, CV , Image  |          | 2019 | arXiv      | Matthias Bethge, Wieland Brendel |         | [link](https://arxiv.org/abs/1904.00760) |


---

## Wojciech Zaremba

|   | Paper Name                                                                                                      | Status  | Topic        | Category        | Year | Conference | Author                                                                              | Summary | Link                                                                           |
| - | --------------------------------------------------------------------------------------------------------------- | ------- | ------------ | --------------- | ---- | ---------- | ----------------------------------------------------------------------------------- | ------- | ------------------------------------------------------------------------------ |
| 0 | [Improved Techniques for Training GANs](Research_Papers_Anubhav_Reads/Improved_Techniques_for_Training_GANs.md) | Pending | GANs, Image  | Semi-Supervised | 2016 | NIPS       | Alec Radford, Ian Goodfellow, Tim Salimans, Vicki Cheung, Wojciech Zaremba, Xi Chen |         | [link](http://papers.nips.cc/paper/6124-improved-techniques-for-training-gans) |


---

## Xi Chen

|   | Paper Name                                                                                                      | Status  | Topic        | Category        | Year | Conference | Author                                                                              | Summary | Link                                                                           |
| - | --------------------------------------------------------------------------------------------------------------- | ------- | ------------ | --------------- | ---- | ---------- | ----------------------------------------------------------------------------------- | ------- | ------------------------------------------------------------------------------ |
| 0 | [Improved Techniques for Training GANs](Research_Papers_Anubhav_Reads/Improved_Techniques_for_Training_GANs.md) | Pending | GANs, Image  | Semi-Supervised | 2016 | NIPS       | Alec Radford, Ian Goodfellow, Tim Salimans, Vicki Cheung, Wojciech Zaremba, Xi Chen |         | [link](http://papers.nips.cc/paper/6124-improved-techniques-for-training-gans) |


---

## Xian Li

|   | Paper Name                                                                                                                          | Status  | Topic                                              | Category               | Year | Conference | Author                                     | Summary                                                                                                                                                                                                                                                                                                                                                                                                       | Link                                         |
| - | ----------------------------------------------------------------------------------------------------------------------------------- | ------- | -------------------------------------------------- | ---------------------- | ---- | ---------- | ------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |
| 0 | [Self-Alignment with Instruction Backtranslation](Research_Papers_Anubhav_Reads/Self-Alignment_with_Instruction_Backtranslation.md) | Pending | Generative, Large-Language-Models, Training Method | Instruction-Finetuning | 2023 | arXiv      | Jason Weston, Mike Lewis, Ping Yu, Xian Li | The paper introduces a scalable method called "instruction backtranslation" to create a high-quality instruction-following language model. This method involves self-augmentation and self-curation of training examples generated from web documents, resulting in a model that outperforms others in its category without relying on distillation data, showcasing its effective self-alignment capability. | [link](https://arxiv.org/pdf/2308.06259.pdf) |


---

## Xiangyu Zhang

|   | Paper Name                                                                                                                                  | Status | Topic             | Category     | Year | Conference | Author                    | Summary                                                                         | Link                                                                                                        |
| - | ------------------------------------------------------------------------------------------------------------------------------------------- | ------ | ----------------- | ------------ | ---- | ---------- | ------------------------- | ------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------- |
| 0 | [ResNet (Deep Residual Learning for Image Recognition)](Research_Papers_Anubhav_Reads/ResNet_Deep_Residual_Learning_for_Image_Recogniti.md) | Read   | CNNs, CV , Image  | Architecture | 2016 | CVPR       | Kaiming He, Xiangyu Zhang | Introduces Residual or Skip Connections to allow increase in the depth of a DNN | [link](https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) |


---

## Xiaofei Sun

|   | Paper Name                                                                                                                                                        | Status  | Topic | Category                  | Year | Conference | Author                 | Summary | Link                                     |
| - | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----- | ------------------------- | ---- | ---------- | ---------------------- | ------- | ---------------------------------------- |
| 0 | [Interpreting Deep Learning Models in Natural Language Processing: A Review](Research_Papers_Anubhav_Reads/Interpreting_Deep_Learning_Models_in_Natural_Langu.md) | Pending | Text  | Comparison, Visualization | 2021 | arXiv      | Diyi Yang, Xiaofei Sun |         | [link](https://arxiv.org/abs/2110.10470) |


---

## Xing Shi

|   | Paper Name                                                                                                                                                                                      | Status  | Topic         | Category | Year | Conference | Author                                                  | Summary | Link                                     |
| - | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------- | -------- | ---- | ---------- | ------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [One-shot Text Field Labeling using Attention and Belief Propagation for Structure Information Extraction](Research_Papers_Anubhav_Reads/One-shot_Text_Field_Labeling_using_Attention_and_B.md) | Pending | Image , Text  |          | 2020 | arXiv      | Jun Huang, Mengli Cheng, Minghui Qiu, Wei Lin, Xing Shi |         | [link](https://arxiv.org/abs/2009.04153) |


---

## Xu Jiang

|   | Paper Name                                                                                                                                                 | Status  | Topic                                              | Category                                                        | Year | Conference | Author                                                                                                                      | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Link                                         |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | -------------------------------------------------- | --------------------------------------------------------------- | ---- | ---------- | --------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |
| 0 | [Training language models to follow instructions with human feedback](Research_Papers_Anubhav_Reads/Training_language_models_to_follow_instructions_wi.md) | Pending | Generative, Large-Language-Models, Training Method | Instruction-Finetuning, Reinforcement-Learning, Semi-Supervised | 2022 | arXiv      | Carroll L. Wainwright, Diogo Almeida, Jan Leike, Jeff Wu, Long Ouyang, Pamela Mishkin, Paul Christiano, Ryan Lowe, Xu Jiang | This paper presents InstructGPT, a model fine-tuned with human feedback to better align with user intent across various tasks. Despite having significantly fewer parameters than larger models, InstructGPT outperforms them in human evaluations, demonstrating improved truthfulness, reduced toxicity, and minimal performance regressions on public NLP datasets, highlighting the potential of fine-tuning with human feedback for enhancing language model alignment with human intent. | [link](https://arxiv.org/pdf/2203.02155.pdf) |


---

## Xuezhi Wang

|   | Paper Name                                                                                                                                                   | Status  | Topic                                   | Category | Year | Conference | Author                             | Summary | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------- | --------------------------------------- | -------- | ---- | ---------- | ---------------------------------- | ------- | ---------------------------------------- |
| 0 | [Chain of Thought Prompting Elicits Reasoning in Large Language Models](Research_Papers_Anubhav_Reads/Chain_of_Thought_Prompting_Elicits_Reasoning_in_La.md) | Pending | Question-Answering, Text , Transformers |          | 2022 | arXiv      | Denny Zhou, Jason Wei, Xuezhi Wang |         | [link](https://arxiv.org/abs/2201.11903) |


---

## Xun Huang

|   | Paper Name                                                                                                                                                        | Status  | Topic        | Category | Year | Conference | Author                    | Summary | Link                                                                                                             |
| - | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | -------- | ---- | ---------- | ------------------------- | ------- | ---------------------------------------------------------------------------------------------------------------- |
| 0 | [Arbitrary Style Transfer in Real-Time With Adaptive Instance Normalization](Research_Papers_Anubhav_Reads/Arbitrary_Style_Transfer_in_Real-Time_With_Adaptiv.md) | Pending | CNNs, Image  |          | 2017 | ICCV       | Serge Belongie, Xun Huang |         | [link](https://openaccess.thecvf.com/content_iccv_2017/html/Huang_Arbitrary_Style_Transfer_ICCV_2017_paper.html) |


---

## Yamini Bansal

|   | Paper Name                                                                                                                                         | Status  | Topic | Category | Year | Conference | Author                                                                                | Summary | Link                                     |
| - | -------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----- | -------- | ---- | ---------- | ------------------------------------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Deep Double Descent: Where Bigger Models and More Data Hurt](Research_Papers_Anubhav_Reads/Deep_Double_Descent_Where_Bigger_Models_and_More_D.md) | Pending | NNs   |          | 2019 | arXiv      | Boaz Barak, Gal Kaplun, Ilya Sutskever, Preetum Nakkiran, Tristan Yang, Yamini Bansal |         | [link](https://arxiv.org/abs/1912.02292) |


---

## Yanghua Jin

|   | Paper Name                                                                                                                                                                            | Status  | Topic        | Category | Year | Conference | Author                               | Summary | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | -------- | ---- | ---------- | ------------------------------------ | ------- | ---------------------------------------- |
| 0 | [AnimeGAN: Towards the Automatic Anime Characters Creation with Generative Adversarial Networks](Research_Papers_Anubhav_Reads/AnimeGAN_Towards_the_Automatic_Anime_Characters_Cr.md) | Pending | GANs, Image  |          | 2017 | NIPS       | Jiakai Zhang, Minjun Li, Yanghua Jin |         | [link](https://arxiv.org/abs/1708.05509) |


---

## Yanqi Zhou

|   | Paper Name                                                                                                                                                                   | Status | Topic                          | Category | Year | Conference | Author                                                        | Summary                                                                                                                                                                                                             | Link                                     |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------ | ------------------------------ | -------- | ---- | ---------- | ------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](Research_Papers_Anubhav_Reads/T5_Exploring_the_Limits_of_Transfer_Learning_with_.md) | Read   | Attention, Text , Transformers |          | 2020 | JMLR       | Colin Raffel, Noam Shazeer, Peter J. Liu, Wei Liu, Yanqi Zhou | Presents a Text-to-Text transformer model with multi-task learning capabilities, simultaneously solving problems such as machine translation, document summarization, question answering, and classification tasks. | [link](https://arxiv.org/abs/1910.10683) |


---

## Yejin Choi

|   | Paper Name                                                                                                                                                                 | Status  | Topic                                     | Category                     | Year | Conference | Author                                                      | Summary | Link                                                                           |
| - | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----------------------------------------- | ---------------------------- | ---- | ---------- | ----------------------------------------------------------- | ------- | ------------------------------------------------------------------------------ |
| 0 | [ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning](Research_Papers_Anubhav_Reads/ATOMIC_An_Atlas_of_Machine_Commonsense_for_If-Then.md)                       | Pending | AGI, Dataset, Text                        |                              | 2019 | AAAI       | Maarten Sap, Noah A. Smith, Ronan Le Bras, Yejin Choi       |         | [link](https://arxiv.org/pdf/1811.00146.pdf)                                   |
| 1 | [COMET: Commonsense Transformers for Automatic Knowledge Graph Construction](Research_Papers_Anubhav_Reads/COMET_Commonsense_Transformers_for_Automatic_Knowl.md)          | Pending | AGI, Text , Transformers                  |                              | 2019 | ACL        | Antoine Bosselut, Hannah Rashkin, Yejin Choi                |         | [link](https://arxiv.org/pdf/1906.05317.pdf)                                   |
| 2 | [VisualCOMET: Reasoning about the Dynamic Context of a Still Image](Research_Papers_Anubhav_Reads/VisualCOMET_Reasoning_about_the_Dynamic_Context_of.md)                   | Pending | AGI, Dataset, Image , Text , Transformers |                              | 2020 | ECCV       | Ali Farhadi, Chandra Bhagavatula, Jae Sung Park, Yejin Choi |         | [link](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500494.pdf) |
| 3 | [Symbolic Knowledge Distillation: from General Language Models to Commonsense Models](Research_Papers_Anubhav_Reads/Symbolic_Knowledge_Distillation_from_General_Langu.md) | Pending | Dataset, Text , Transformers              | Optimizations, Tips & Tricks | 2021 | arXiv      | Chandra Bhagavatula, Jack Hessel, Peter West, Yejin Choi    |         | [link](https://arxiv.org/abs/2110.07178)                                       |


---

## Ygor Rebouças Serpa

|   | Paper Name                                                                                                                      | Status    | Topic               | Category | Year | Conference | Author              | Summary | Link                                                                                              |
| - | ------------------------------------------------------------------------------------------------------------------------------- | --------- | ------------------- | -------- | ---- | ---------- | ------------------- | ------- | ------------------------------------------------------------------------------------------------- |
| 0 | [A Comprehensive Guide on Activation Functions](Research_Papers_Anubhav_Reads/A_Comprehensive_Guide_on_Activation_Functions.md) | This week | Activation Function |          | 2020 | Blog       | Ygor Rebouças Serpa |         | [link](https://towardsdatascience.com/a-comprehensive-guide-on-activation-functions-b45ed37a4fa5) |


---

## Yifan Jiang

|   | Paper Name                                                                                                                               | Status  | Topic                      | Category     | Year | Conference | Author                                   | Summary | Link                                     |
| - | ---------------------------------------------------------------------------------------------------------------------------------------- | ------- | -------------------------- | ------------ | ---- | ---------- | ---------------------------------------- | ------- | ---------------------------------------- |
| 0 | [TransGAN: Two Transformers Can Make One Strong GAN](Research_Papers_Anubhav_Reads/TransGAN_Two_Transformers_Can_Make_One_Strong_GAN.md) | Pending | GANs, Image , Transformers | Architecture | 2021 | arXiv      | Shiyu Chang, Yifan Jiang, Zhangyang Wang |         | [link](https://arxiv.org/abs/2102.07074) |


---

## Yin Cui

|   | Paper Name                                                                                                                                      | Status  | Topic         | Category      | Year | Conference | Author               | Summary | Link                                                                                                                                            |
| - | ----------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------- | ------------- | ---- | ---------- | -------------------- | ------- | ----------------------------------------------------------------------------------------------------------------------------------------------- |
| 0 | [Class-Balanced Loss Based on Effective Number of Samples](Research_Papers_Anubhav_Reads/Class-Balanced_Loss_Based_on_Effective_Number_of_S.md) | Pending | Loss Function | Tips & Tricks | 2019 | CVPR       | Menglin Jia, Yin Cui |         | [link](https://openaccess.thecvf.com/content_CVPR_2019/papers/Cui_Class-Balanced_Loss_Based_on_Effective_Number_of_Samples_CVPR_2019_paper.pdf) |


---

## Yinfei Yang

|   | Paper Name                                                                                                              | Status | Topic                                           | Category   | Year | Conference | Author                       | Summary                                                                                                                     | Link                                     |
| - | ----------------------------------------------------------------------------------------------------------------------- | ------ | ----------------------------------------------- | ---------- | ---- | ---------- | ---------------------------- | --------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [Language-Agnostic BERT Sentence Embedding](Research_Papers_Anubhav_Reads/Language-Agnostic_BERT_Sentence_Embedding.md) | Read   | Attention, Siamese Network, Text , Transformers | Embeddings | 2020 | arXiv      | Fangxiaoyu Feng, Yinfei Yang | A BERT model with multilingual sentence embeddings learned over 112 languages and Zero-shot learning over unseen languages. | [link](https://arxiv.org/abs/2007.01852) |


---

## Yipeng Qin

|   | Paper Name                                                                                                                                                 | Status  | Topic        | Category | Year | Conference | Author                                | Summary | Link                                                                                                                                                      |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | -------- | ---- | ---------- | ------------------------------------- | ------- | --------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 0 | [Image2StyleGAN: How to Embed Images Into the StyleGAN Latent Space?](Research_Papers_Anubhav_Reads/Image2StyleGAN_How_to_Embed_Images_Into_the_StyleG.md) | Pending | GANs, Image  |          | 2019 | ICCV       | Peter Wonka, Rameen Abdal, Yipeng Qin |         | [link](https://openaccess.thecvf.com/content_ICCV_2019/html/Abdal_Image2StyleGAN_How_to_Embed_Images_Into_the_StyleGAN_Latent_Space_ICCV_2019_paper.html) |


---

## Yuan Cao

|   | Paper Name                                                                                                                                        | Status  | Topic                                    | Category                     | Year | Conference | Author                                                                                 | Summary                                                                                                                                                                                                                                                                                                                                                  | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ---------------------------------------- | ---------------------------- | ---- | ---------- | -------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [ReAct: Synergizing Reasoning and Acting in Language Models](Research_Papers_Anubhav_Reads/ReAct_Synergizing_Reasoning_and_Acting_in_Language.md) | Pending | Generative, Large-Language-Models, Text  | Optimizations, Tips & Tricks | 2023 | ICLR       | Dian Yu, Izhak Shafran, Jeffrey Zhao, Karthik Narasimhan, Nan Du, Shunyu Yao, Yuan Cao | This paper introduces ReAct, a novel approach that leverages Large Language Models (LLMs) to interleave reasoning traces and task-specific actions. ReAct outperforms existing methods on various language and decision-making tasks, addressing issues like hallucination, error propagation, and improving human interpretability and trustworthiness. | [link](https://arxiv.org/abs/2210.03629) |


---

## Yuichi Yoshida

|   | Paper Name                                                                                          | Status  | Topic               | Category      | Year | Conference | Author                                                          | Summary | Link                                     |
| - | --------------------------------------------------------------------------------------------------- | ------- | ------------------- | ------------- | ---- | ---------- | --------------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Spectral Normalization for GANs](Research_Papers_Anubhav_Reads/Spectral_Normalization_for_GANs.md) | Pending | GANs, Normalization | Optimizations | 2018 | arXiv      | Masanori Koyama, Takeru Miyato, Toshiki Kataoka, Yuichi Yoshida |         | [link](https://arxiv.org/abs/1802.05957) |


---

## Yunjey Choi

|   | Paper Name                                                                                                                                                                          | Status  | Topic        | Category | Year | Conference | Author                                                                       | Summary | Link                                                                                                               |
| - | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | -------- | ---- | ---------- | ---------------------------------------------------------------------------- | ------- | ------------------------------------------------------------------------------------------------------------------ |
| 0 | [StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation](Research_Papers_Anubhav_Reads/StarGAN_Unified_Generative_Adversarial_Networks_fo.md) | Pending | GANs, Image  |          | 2018 | CVPR       | Jaegul Choo, Jung-Woo Ha, Minje Choi, Munyoung Kim, Sunghun Kim, Yunjey Choi |         | [link](https://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_StarGAN_Unified_Generative_CVPR_2018_paper.pdf) |


---

## Yuntao Ba

|   | Paper Name                                                                                                                           | Status  | Topic                                              | Category                                                     | Year | Conference | Author                  | Summary                                                                                                                                                                                                                                                                                                                                                                           | Link                                         |
| - | ------------------------------------------------------------------------------------------------------------------------------------ | ------- | -------------------------------------------------- | ------------------------------------------------------------ | ---- | ---------- | ----------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |
| 0 | [Constitutional AI: Harmlessness from AI Feedback](Research_Papers_Anubhav_Reads/Constitutional_AI_Harmlessness_from_AI_Feedback.md) | Pending | Generative, Large-Language-Models, Training Method | Instruction-Finetuning, Reinforcement-Learning, Unsupervised | 2022 | arXiv      | Jared Kaplan, Yuntao Ba | The paper introduces Constitutional AI, a method for training a safe AI assistant without human-labeled data on harmful outputs. It combines supervised learning and reinforcement learning phases, enabling the AI to engage with harmful queries by explaining its objections, thus improving control, transparency, and human-judged performance with minimal human oversight. | [link](https://arxiv.org/pdf/2212.08073.pdf) |


---

## Yusuke Iwasawa

|   | Paper Name                                                                                                                      | Status  | Topic                                 | Category                          | Year | Conference | Author                         | Summary | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------------------------------- | --------------------------------- | ---- | ---------- | ------------------------------ | ------- | ---------------------------------------- |
| 0 | [Large Language Models are Zero-Shot Reasoners](Research_Papers_Anubhav_Reads/Large_Language_Models_are_Zero-Shot_Reasoners.md) | Pending | Generative, Question-Answering, Text  | Tips & Tricks, Zero-shot-learning | 2022 | arXiv      | Takeshi Kojima, Yusuke Iwasawa |         | [link](https://arxiv.org/abs/2205.11916) |


---

## Yuxin Wu

|   | Paper Name                                                                  | Status  | Topic              | Category      | Year | Conference | Author               | Summary | Link                                     |
| - | --------------------------------------------------------------------------- | ------- | ------------------ | ------------- | ---- | ---------- | -------------------- | ------- | ---------------------------------------- |
| 0 | [Group Normalization](Research_Papers_Anubhav_Reads/Group_Normalization.md) | Pending | NNs, Normalization | Optimizations | 2018 | arXiv      | Kaiming He, Yuxin Wu |         | [link](https://arxiv.org/abs/1803.08494) |


---

## Zhangyang Wang

|   | Paper Name                                                                                                                               | Status  | Topic                      | Category     | Year | Conference | Author                                   | Summary | Link                                     |
| - | ---------------------------------------------------------------------------------------------------------------------------------------- | ------- | -------------------------- | ------------ | ---- | ---------- | ---------------------------------------- | ------- | ---------------------------------------- |
| 0 | [TransGAN: Two Transformers Can Make One Strong GAN](Research_Papers_Anubhav_Reads/TransGAN_Two_Transformers_Can_Make_One_Strong_GAN.md) | Pending | GANs, Image , Transformers | Architecture | 2021 | arXiv      | Shiyu Chang, Yifan Jiang, Zhangyang Wang |         | [link](https://arxiv.org/abs/2102.07074) |


---

## Zhen Tan

|   | Paper Name                                                                                                                                 | Status    | Topic                                      | Category                 | Year | Conference | Author                      | Summary | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------ | --------- | ------------------------------------------ | ------------------------ | ---- | ---------- | --------------------------- | ------- | ---------------------------------------- |
| 0 | [Large Language Models for Data Annotation: A Survey](Research_Papers_Anubhav_Reads/Large_Language_Models_for_Data_Annotation_A_Survey.md) | This week | Dataset, Generative, Large-Language-Models | Prompting, Tips & Tricks | 2024 | arXiv      | Alimohammad Beigi, Zhen Tan |         | [link](https://arxiv.org/abs/2402.13446) |


---

## Zhi Zhang

|   | Paper Name                                                                                                                                                       | Status | Topic       | Category                     | Year | Conference | Author             | Summary                                                                                        | Link                                     |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------ | ----------- | ---------------------------- | ---- | ---------- | ------------------ | ---------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [Bag of Tricks for Image Classification with Convolutional Neural Networks](Research_Papers_Anubhav_Reads/Bag_of_Tricks_for_Image_Classification_with_Convol.md) | Read   | CV , Image  | Optimizations, Tips & Tricks | 2018 | arXiv      | Tong He, Zhi Zhang | Shows a dozen tricks (mixup, label smoothing, etc.) to improve CNN accuracy and training time. | [link](https://arxiv.org/abs/1812.01187) |


---

## Ziad Al-Halah

|   | Paper Name                                                                                                                                             | Status  | Topic        | Category               | Year | Conference | Author                                                   | Summary | Link                                         |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------ | ------- | ------------ | ---------------------- | ---- | ---------- | -------------------------------------------------------- | ------- | -------------------------------------------- |
| 0 | [Occupancy Anticipation for Efficient Exploration and Navigation](Research_Papers_Anubhav_Reads/Occupancy_Anticipation_for_Efficient_Exploration_a.md) | Pending | CNNs, Image  | Reinforcement-Learning | 2020 | ECCV       | Kristen Grauman, Santhosh K. Ramakrishnan, Ziad Al-Halah |         | [link](https://arxiv.org/pdf/2008.09285.pdf) |


---

## Łukasz Kaiser

|   | Paper Name                                                                              | Status | Topic                          | Category     | Year | Conference | Author                                                        | Summary                                                                                       | Link                                                               |
| - | --------------------------------------------------------------------------------------- | ------ | ------------------------------ | ------------ | ---- | ---------- | ------------------------------------------------------------- | --------------------------------------------------------------------------------------------- | ------------------------------------------------------------------ |
| 0 | [Attention is All you Need](Research_Papers_Anubhav_Reads/Attention_is_All_you_Need.md) | Read   | Attention, Text , Transformers | Architecture | 2017 | NIPS       | Ashish Vaswani, Illia Polosukhin, Noam Shazeer, Łukasz Kaiser | Talks about Transformer architecture which brings SOTA performance for different tasks in NLP | [link](http://papers.nips.cc/paper/7181-attention-is-all-you-need) |


---

