# Category-wise

<!-- no toc -->
- [Adversarial](#Adversarial)
- [Architecture](#Architecture)
- [Comparison](#Comparison)
- [Embeddings](#Embeddings)
- [Few-shot-learning](#Few-shot-learning)
- [Instruction-Finetuning](#Instruction-Finetuning)
- [Latent space](#Latent-space)
- [Multimodal](#Multimodal)
- [Optimization-Memory](#Optimization-Memory)
- [Optimization-No. of params](#Optimization-No.-of-params)
- [Optimizations](#Optimizations)
- [Other](#Other)
- [Pre-Training](#Pre-Training)
- [Prompting](#Prompting)
- [Reinforcement-Learning](#Reinforcement-Learning)
- [Semi-Supervised](#Semi-Supervised)
- [Tips & Tricks](#Tips-&-Tricks)
- [Unsupervised](#Unsupervised)
- [Visualization](#Visualization)
- [Zero-shot-learning](#Zero-shot-learning)

---

## Adversarial

|   | Paper Name                                                                                                                              | Status  | Topic        | Category    | Year | Conference | Author     | Summary | Link                                                                                                  |
| - | --------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | ----------- | ---- | ---------- | ---------- | ------- | ----------------------------------------------------------------------------------------------------- |
| 0 | [Breaking neural networks with adversarial attacks](Research_Papers_Anubhav_Reads/Breaking_neural_networks_with_adversarial_attacks.md) | Pending | CNNs, Image  | Adversarial | 2019 | Blog       | Anant Jain |         | [link](https://towardsdatascience.com/breaking-neural-networks-with-adversarial-attacks-f4290a9a45aa) |


---

## Architecture

|    | Paper Name                                                                                                                                                                       | Status  | Topic                                                                                         | Category                                                              | Year | Conference | Author                                                            | Summary                                                                                                                                                             | Link                                                                                                                      |
| -- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | --------------------------------------------------------------------------------------------- | --------------------------------------------------------------------- | ---- | ---------- | ----------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------- |
| 0  | [Inception-v1 (Going Deeper With Convolutions)](Research_Papers_Anubhav_Reads/Inception-v1_Going_Deeper_With_Convolutions.md)                                                    | Read    | CNNs, CV , Image                                                                              | Architecture                                                          | 2015 | CVPR       | Christian Szegedy, Wei Liu                                        | Propose the use of 1x1 conv operations to reduce the number of parameters in a deep and wide CNN                                                                    | [link](https://arxiv.org/abs/1409.4842)                                                                                   |
| 1  | [ResNet (Deep Residual Learning for Image Recognition)](Research_Papers_Anubhav_Reads/ResNet_Deep_Residual_Learning_for_Image_Recogniti.md)                                      | Read    | CNNs, CV , Image                                                                              | Architecture                                                          | 2016 | CVPR       | Kaiming He, Xiangyu Zhang                                         | Introduces Residual or Skip Connections to allow increase in the depth of a DNN                                                                                     | [link](https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html)               |
| 2  | [MobileNet (Efficient Convolutional Neural Networks for Mobile Vision Applications)](Research_Papers_Anubhav_Reads/MobileNet_Efficient_Convolutional_Neural_Networks.md)         | Pending | CNNs, CV , Image                                                                              | Architecture, Optimization-No. of params                              | 2017 | arXiv      | Andrew G. Howard, Menglong Zhu                                    |                                                                                                                                                                     | [link](https://arxiv.org/abs/1704.04861)                                                                                  |
| 3  | [SqueezeNet](Research_Papers_Anubhav_Reads/SqueezeNet.md)                                                                                                                        | Read    | CNNs, CV , Image                                                                              | Architecture, Optimization-No. of params                              | 2016 | arXiv      | Forrest N. Iandola, Song Han                                      | Explores model compression by using 1x1 convolutions called fire modules.                                                                                           | [link](https://arxiv.org/abs/1602.07360)                                                                                  |
| 4  | [Attention is All you Need](Research_Papers_Anubhav_Reads/Attention_is_All_you_Need.md)                                                                                          | Read    | Attention, Text , Transformers                                                                | Architecture                                                          | 2017 | NIPS       | Ashish Vaswani, Illia Polosukhin, Noam Shazeer, ≈Åukasz Kaiser     | Talks about Transformer architecture which brings SOTA performance for different tasks in NLP                                                                       | [link](http://papers.nips.cc/paper/7181-attention-is-all-you-need)                                                        |
| 5  | [SAGAN: Self-Attention Generative Adversarial Networks](Research_Papers_Anubhav_Reads/SAGAN_Self-Attention_Generative_Adversarial_Networ.md)                                     | Pending | Attention, GANs, Image                                                                        | Architecture                                                          | 2018 | arXiv      | Augustus Odena, Dimitris Metaxas, Han Zhang, Ian Goodfellow       |                                                                                                                                                                     | [link](https://arxiv.org/abs/1805.08318)                                                                                  |
| 6  | [Reformer: The Efficient Transformer](Research_Papers_Anubhav_Reads/Reformer_The_Efficient_Transformer.md)                                                                       | Read    | Attention, Text , Transformers                                                                | Architecture, Optimization-Memory, Optimization-No. of params         | 2020 | arXiv      | Anselm Levskaya, Lukasz Kaiser, Nikita Kitaev                     | Overcome time and memory complexity of Transformers by bucketing Query, Keys and using Reversible residual connections.                                             | [link](https://arxiv.org/abs/2001.04451)                                                                                  |
| 7  | [CycleGAN: Unpaired Image-To-Image Translation Using Cycle-Consistent Adversarial Networks](Research_Papers_Anubhav_Reads/CycleGAN_Unpaired_Image-To-Image_Translation_Using.md) | Pending | GANs, Image                                                                                   | Architecture                                                          | 2017 | ICCV       | Alexei A. Efros, Jun-Yan Zhu, Phillip Isola, Taesung Park         |                                                                                                                                                                     | [link](https://openaccess.thecvf.com/content_iccv_2017/html/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.html) |
| 8  | [Capsule Networks: Dynamic Routing Between Capsules](Research_Papers_Anubhav_Reads/Capsule_Networks_Dynamic_Routing_Between_Capsules.md)                                         | Pending | CV , Image                                                                                    | Architecture                                                          | 2017 | arXiv      | Geoffrey E Hinton, Nicholas Frosst, Sara Sabour                   |                                                                                                                                                                     | [link](https://arxiv.org/abs/1710.09829)                                                                                  |
| 9  | [Graph Neural Network: Relational inductive biases, deep learning, and graph networks](Research_Papers_Anubhav_Reads/Graph_Neural_Network_Relational_inductive_biases,_.md)      | Pending | GraphNN                                                                                       | Architecture                                                          | 2018 | arXiv      | Jessica B. Hamrick, Oriol Vinyals, Peter W. Battaglia             |                                                                                                                                                                     | [link](https://arxiv.org/pdf/1806.01261.pdf)                                                                              |
| 10 | [TransGAN: Two Transformers Can Make One Strong GAN](Research_Papers_Anubhav_Reads/TransGAN_Two_Transformers_Can_Make_One_Strong_GAN.md)                                         | Pending | GANs, Image , Transformers                                                                    | Architecture                                                          | 2021 | arXiv      | Shiyu Chang, Yifan Jiang, Zhangyang Wang                          |                                                                                                                                                                     | [link](https://arxiv.org/abs/2102.07074)                                                                                  |
| 11 | [Flan-T5: Scaling Instruction-Finetuned Language Models](Research_Papers_Anubhav_Reads/Flan-T5_Scaling_Instruction-Finetuned_Language_Mod.md)                                    | Pending | Generative, Text , Transformers                                                               | Architecture, Pre-Training                                            | 2022 | arXiv      | Hyung Won Chung, Le Hou                                           |                                                                                                                                                                     | [link](https://arxiv.org/abs/2210.11416)                                                                                  |
| 12 | [Training Compute-Optimal Large Language Models](Research_Papers_Anubhav_Reads/Training_Compute-Optimal_Large_Language_Models.md)                                                | Pending | Large-Language-Models, Transformers                                                           | Architecture, Optimization-No. of params, Pre-Training, Tips & Tricks | 2022 | arXiv      | Jordan Hoffmann, Laurent Sifre, Oriol Vinyals, Sebastian Borgeaud |                                                                                                                                                                     | [link](https://arxiv.org/abs/2203.15556)                                                                                  |
| 13 | [VL-T5: Unifying Vision-and-Language Tasks via Text Generation](Research_Papers_Anubhav_Reads/VL-T5_Unifying_Vision-and-Language_Tasks_via_Text_.md)                             | Read    | CNNs, CV , Generative, Image , Large-Language-Models, Question-Answering, Text , Transformers | Architecture, Embeddings, Multimodal, Pre-Training                    | 2021 | arXiv      | Hao Tan, Jaemin Cho, Jie Le, Mohit Bansal                         | Unifying two modalities (image and text) together in a single transformer model to solve multiple tasks in a single architecture using text prefixes similar to T5. | [link](https://arxiv.org/pdf/2102.02779.pdf)                                                                              |


---

## Comparison

|   | Paper Name                                                                                                                                                        | Status  | Topic                            | Category                                              | Year | Conference | Author                                               | Summary                                                                                                                                     | Link                                                                                                                           |
| - | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | -------------------------------- | ----------------------------------------------------- | ---- | ---------- | ---------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------ |
| 0 | [Evaluation of neural network architectures for embedded systems](Research_Papers_Anubhav_Reads/Evaluation_of_neural_network_architectures_for_emb.md)            | Read    | CNNs, CV , Image                 | Comparison                                            | 2017 | IEEE ISCAS | Adam Paszke, Alfredo Canziani, Eugenio Culurciello   | Compare CNN classification architectures on accuracy, memory footprint, parameters, operations count, inference time and power consumption. | [link](https://ieeexplore.ieee.org/abstract/document/8050276)                                                                  |
| 1 | [A 2019 guide to Human Pose Estimation with Deep Learning](Research_Papers_Anubhav_Reads/A_2019_guide_to_Human_Pose_Estimation_with_Deep_Le.md)                   | Pending | CV , Pose Estimation             | Comparison                                            | 2019 | Blog       | Sudharshan Chandra Babu                              |                                                                                                                                             | [link](https://nanonets.com/blog/human-pose-estimation-2d-guide/)                                                              |
| 2 | [Understanding Loss Functions in Computer Vision](Research_Papers_Anubhav_Reads/Understanding_Loss_Functions_in_Computer_Vision.md)                               | Pending | CV , GANs, Image , Loss Function | Comparison, Tips & Tricks                             | 2020 | Blog       | Sowmya Yellapragada                                  |                                                                                                                                             | [link](https://medium.com/ml-cheat-sheet/winning-at-loss-functions-2-important-loss-functions-in-computer-vision-b2b9d293e15a) |
| 3 | [NADAM: Incorporating Nesterov Momentum into Adam](Research_Papers_Anubhav_Reads/NADAM_Incorporating_Nesterov_Momentum_into_Adam.md)                              | Pending | NNs, Optimizers                  | Comparison                                            | 2016 |            | Timothy Dozat                                        |                                                                                                                                             | [link](http://cs229.stanford.edu/proj2015/054_report.pdf)                                                                      |
| 4 | [Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask](Research_Papers_Anubhav_Reads/Deconstructing_Lottery_Tickets_Zeros,_Signs,_and_t.md)            | Read    | NN Initialization, NNs           | Comparison, Optimization-No. of params, Tips & Tricks | 2019 | NeurIPS    | Hattie Zhou, Janice Lan, Jason Yosinski, Rosanne Liu | Follow up on Lottery Ticket Hypothesis exploring the effects of different Masking criteria as well as Mask-1 and Mask-0 actions.            | [link](https://arxiv.org/abs/1905.01067)                                                                                       |
| 5 | [Interpreting Deep Learning Models in Natural Language Processing: A Review](Research_Papers_Anubhav_Reads/Interpreting_Deep_Learning_Models_in_Natural_Langu.md) | Pending | Text                             | Comparison, Visualization                             | 2021 | arXiv      | Diyi Yang, Xiaofei Sun                               |                                                                                                                                             | [link](https://arxiv.org/abs/2110.10470)                                                                                       |
| 6 | [Transforming Sequence Tagging Into A Seq2Seq Task](Research_Papers_Anubhav_Reads/Transforming_Sequence_Tagging_Into_A_Seq2Seq_Task.md)                           | Pending | Generative, Text                 | Comparison, Tips & Tricks                             | 2022 | arXiv      | Iftekhar Naim, Karthik Raman, Krishna Srinivasan     |                                                                                                                                             | [link](https://arxiv.org/pdf/2203.08378.pdf)                                                                                   |


---

## Embeddings

|   | Paper Name                                                                                                                                                              | Status  | Topic                                                                                         | Category                                           | Year | Conference | Author                                                       | Summary                                                                                                                                                                                | Link                                         |
| - | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | --------------------------------------------------------------------------------------------- | -------------------------------------------------- | ---- | ---------- | ------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |
| 0 | [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](Research_Papers_Anubhav_Reads/BERT_Pre-training_of_Deep_Bidirectional_Transforme.md) | Read    | Attention, Text , Transformers                                                                | Embeddings                                         | 2018 | NAACL      | Jacob Devlin, Kenton Lee, Kristina Toutanova, Ming-Wei Chang | BERT is an extension to Transformer based architecture which introduces a masked word pretraining and next sentence prediction task to pretrain the model for a wide variety of tasks. | [link](https://arxiv.org/abs/1810.04805)     |
| 1 | [Language-Agnostic BERT Sentence Embedding](Research_Papers_Anubhav_Reads/Language-Agnostic_BERT_Sentence_Embedding.md)                                                 | Read    | Attention, Siamese Network, Text , Transformers                                               | Embeddings                                         | 2020 | arXiv      | Fangxiaoyu Feng, Yinfei Yang                                 | A BERT model with multilingual sentence embeddings learned over 112 languages and Zero-shot learning over unseen languages.                                                            | [link](https://arxiv.org/abs/2007.01852)     |
| 2 | [Word2Vec: Efficient Estimation of Word Representations in Vector Space](Research_Papers_Anubhav_Reads/Word2Vec_Efficient_Estimation_of_Word_Representati.md)           | Pending | Text                                                                                          | Embeddings, Tips & Tricks                          | 2013 | arXiv      | Greg Corrado, Jeffrey Dean, Kai Chen, Tomas Mikolov          |                                                                                                                                                                                        | [link](https://arxiv.org/abs/1301.3781)      |
| 3 | [Decoding a Neural Retriever‚Äôs Latent Space for Query Suggestion](Research_Papers_Anubhav_Reads/Decoding_a_Neural_Retriever‚Äôs_Latent_Space_for_Que.md)                  | Pending | Text                                                                                          | Embeddings, Latent space                           | 2022 | arXiv      | Christian Buck, Leonard Adolphs, Michelle Chen Huebscher     |                                                                                                                                                                                        | [link](https://arxiv.org/abs/2210.12084)     |
| 4 | [VL-T5: Unifying Vision-and-Language Tasks via Text Generation](Research_Papers_Anubhav_Reads/VL-T5_Unifying_Vision-and-Language_Tasks_via_Text_.md)                    | Read    | CNNs, CV , Generative, Image , Large-Language-Models, Question-Answering, Text , Transformers | Architecture, Embeddings, Multimodal, Pre-Training | 2021 | arXiv      | Hao Tan, Jaemin Cho, Jie Le, Mohit Bansal                    | Unifying two modalities (image and text) together in a single transformer model to solve multiple tasks in a single architecture using text prefixes similar to T5.                    | [link](https://arxiv.org/pdf/2102.02779.pdf) |


---

## Few-shot-learning

|   | Paper Name                                                                                                                                                 | Status  | Topic        | Category          | Year | Conference | Author                                           | Summary | Link                                                                                                                                                        |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------ | ----------------- | ---- | ---------- | ------------------------------------------------ | ------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 0 | [Few-Shot Learning with Localization in Realistic Settings](Research_Papers_Anubhav_Reads/Few-Shot_Learning_with_Localization_in_Realistic_S.md)           | Pending | CNNs, Image  | Few-shot-learning | 2019 | CVPR       | Bharath Hariharan, Davis Wertheimer              |         | [link](https://openaccess.thecvf.com/content_CVPR_2019/papers/Wertheimer_Few-Shot_Learning_With_Localization_in_Realistic_Settings_CVPR_2019_paper.pdf)     |
| 1 | [Revisiting Pose-Normalization for Fine-Grained Few-Shot Recognition](Research_Papers_Anubhav_Reads/Revisiting_Pose-Normalization_for_Fine-Grained_Few.md) | Pending | CNNs, Image  | Few-shot-learning | 2020 | CVPR       | Bharath Hariharan, Davis Wertheimer, Luming Tang |         | [link](https://openaccess.thecvf.com/content_CVPR_2020/papers/Tang_Revisiting_Pose-Normalization_for_Fine-Grained_Few-Shot_Recognition_CVPR_2020_paper.pdf) |


---

## Instruction-Finetuning

|   | Paper Name                                                                                                                                                 | Status  | Topic                                                                      | Category                                                        | Year | Conference | Author                                                                                                                      | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Link                                         |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | -------------------------------------------------------------------------- | --------------------------------------------------------------- | ---- | ---------- | --------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |
| 0 | [Scaling Instruction-Finetuned Language Models (FLAN)](Research_Papers_Anubhav_Reads/Scaling_Instruction-Finetuned_Language_Models_FLA.md)                 | Pending | Generative, Large-Language-Models, Question-Answering, Text , Transformers | Instruction-Finetuning                                          | 2022 | arXiv      | Hyung Won Chung, Jason Wei, Jeffrey Dean, Le Hou, Quoc V. Le, Shayne Longpre                                                | https://arxiv.org/abs/2210.11416 introduces FLAN (Fine-tuned LAnguage Net), an instruction finetuning method, and presents the results of its application. The study demonstrates that by fine-tuning the 540B PaLM model on 1836 tasks while incorporating Chain-of-Thought Reasoning data, FLAN achieves improvements in generalization, human usability, and zero-shot reasoning over the base model. The paper also provides detailed information on how each these aspects was evaluated. | [link](https://arxiv.org/abs/2210.11416)     |
| 1 | [Training language models to follow instructions with human feedback](Research_Papers_Anubhav_Reads/Training_language_models_to_follow_instructions_wi.md) | Pending | Generative, Large-Language-Models, Training Method                         | Instruction-Finetuning, Reinforcement-Learning, Semi-Supervised | 2022 | arXiv      | Carroll L. Wainwright, Diogo Almeida, Jan Leike, Jeff Wu, Long Ouyang, Pamela Mishkin, Paul Christiano, Ryan Lowe, Xu Jiang | This paper presents InstructGPT, a model fine-tuned with human feedback to better align with user intent across various tasks. Despite having significantly fewer parameters than larger models, InstructGPT outperforms them in human evaluations, demonstrating improved truthfulness, reduced toxicity, and minimal performance regressions on public NLP datasets, highlighting the potential of fine-tuning with human feedback for enhancing language model alignment with human intent. | [link](https://arxiv.org/pdf/2203.02155.pdf) |
| 2 | [Constitutional AI: Harmlessness from AI Feedback](Research_Papers_Anubhav_Reads/Constitutional_AI_Harmlessness_from_AI_Feedback.md)                       | Pending | Generative, Large-Language-Models, Training Method                         | Instruction-Finetuning, Reinforcement-Learning, Unsupervised    | 2022 | arXiv      | Jared Kaplan, Yuntao Ba                                                                                                     | The paper introduces Constitutional AI, a method for training a safe AI assistant without human-labeled data on harmful outputs. It combines supervised learning and reinforcement learning phases, enabling the AI to engage with harmful queries by explaining its objections, thus improving control, transparency, and human-judged performance with minimal human oversight.                                                                                                              | [link](https://arxiv.org/pdf/2212.08073.pdf) |
| 3 | [Self-Alignment with Instruction Backtranslation](Research_Papers_Anubhav_Reads/Self-Alignment_with_Instruction_Backtranslation.md)                        | Pending | Generative, Large-Language-Models, Training Method                         | Instruction-Finetuning                                          | 2023 | arXiv      | Jason Weston, Mike Lewis, Ping Yu, Xian Li                                                                                  | The paper introduces a scalable method called "instruction backtranslation" to create a high-quality instruction-following language model. This method involves self-augmentation and self-curation of training examples generated from web documents, resulting in a model that outperforms others in its category without relying on distillation data, showcasing its effective self-alignment capability.                                                                                  | [link](https://arxiv.org/pdf/2308.06259.pdf) |
| 4 | [Table-GPT: Table-tuned GPT for Diverse Table Tasks](Research_Papers_Anubhav_Reads/Table-GPT_Table-tuned_GPT_for_Diverse_Table_Tasks.md)                   | Pending | Generative, Large-Language-Models, Training Method                         | Instruction-Finetuning                                          | 2023 | arXiv      |                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | [link](https://arxiv.org/pdf/2310.09263.pdf) |


---

## Latent space

|   | Paper Name                                                                                                                                             | Status  | Topic | Category                 | Year | Conference | Author                                                   | Summary | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------ | ------- | ----- | ------------------------ | ---- | ---------- | -------------------------------------------------------- | ------- | ---------------------------------------- |
| 0 | [Decoding a Neural Retriever‚Äôs Latent Space for Query Suggestion](Research_Papers_Anubhav_Reads/Decoding_a_Neural_Retriever‚Äôs_Latent_Space_for_Que.md) | Pending | Text  | Embeddings, Latent space | 2022 | arXiv      | Christian Buck, Leonard Adolphs, Michelle Chen Huebscher |         | [link](https://arxiv.org/abs/2210.12084) |


---

## Multimodal

|   | Paper Name                                                                                                                                                                             | Status    | Topic                                                                                         | Category                                           | Year | Conference | Author                                      | Summary                                                                                                                                                             | Link                                         |
| - | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------- | --------------------------------------------------------------------------------------------- | -------------------------------------------------- | ---- | ---------- | ------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |
| 0 | [CLIP: Connecting Text and Images](Research_Papers_Anubhav_Reads/CLIP_Connecting_Text_and_Images.md)                                                                                   | Pending   | Image , Text , Transformers                                                                   | Multimodal, Pre-Training                           | 2021 | arXiv      | Alec Radford, Ilya Sutskever, Jong Wook Kim |                                                                                                                                                                     | [link](https://openai.com/blog/clip/)        |
| 1 | [Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision](Research_Papers_Anubhav_Reads/Vokenization_Improving_Language_Understanding_with.md) | This week | Image , Text , Transformers                                                                   | Multimodal                                         | 2020 | EMNLP      | Hao Tan, Mohit Bansal                       |                                                                                                                                                                     | [link](https://arxiv.org/abs/2010.06775)     |
| 2 | [VL-T5: Unifying Vision-and-Language Tasks via Text Generation](Research_Papers_Anubhav_Reads/VL-T5_Unifying_Vision-and-Language_Tasks_via_Text_.md)                                   | Read      | CNNs, CV , Generative, Image , Large-Language-Models, Question-Answering, Text , Transformers | Architecture, Embeddings, Multimodal, Pre-Training | 2021 | arXiv      | Hao Tan, Jaemin Cho, Jie Le, Mohit Bansal   | Unifying two modalities (image and text) together in a single transformer model to solve multiple tasks in a single architecture using text prefixes similar to T5. | [link](https://arxiv.org/pdf/2102.02779.pdf) |


---

## Optimization-Memory

|   | Paper Name                                                                                                 | Status | Topic                          | Category                                                      | Year | Conference | Author                                        | Summary                                                                                                                 | Link                                     |
| - | ---------------------------------------------------------------------------------------------------------- | ------ | ------------------------------ | ------------------------------------------------------------- | ---- | ---------- | --------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [Reformer: The Efficient Transformer](Research_Papers_Anubhav_Reads/Reformer_The_Efficient_Transformer.md) | Read   | Attention, Text , Transformers | Architecture, Optimization-Memory, Optimization-No. of params | 2020 | arXiv      | Anselm Levskaya, Lukasz Kaiser, Nikita Kitaev | Overcome time and memory complexity of Transformers by bucketing Query, Keys and using Reversible residual connections. | [link](https://arxiv.org/abs/2001.04451) |


---

## Optimization-No. of params

|   | Paper Name                                                                                                                                                               | Status  | Topic                               | Category                                                              | Year | Conference | Author                                                            | Summary                                                                                                                                                                                                                                       | Link                                     |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------- | ----------------------------------- | --------------------------------------------------------------------- | ---- | ---------- | ----------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [MobileNet (Efficient Convolutional Neural Networks for Mobile Vision Applications)](Research_Papers_Anubhav_Reads/MobileNet_Efficient_Convolutional_Neural_Networks.md) | Pending | CNNs, CV , Image                    | Architecture, Optimization-No. of params                              | 2017 | arXiv      | Andrew G. Howard, Menglong Zhu                                    |                                                                                                                                                                                                                                               | [link](https://arxiv.org/abs/1704.04861) |
| 1 | [SqueezeNet](Research_Papers_Anubhav_Reads/SqueezeNet.md)                                                                                                                | Read    | CNNs, CV , Image                    | Architecture, Optimization-No. of params                              | 2016 | arXiv      | Forrest N. Iandola, Song Han                                      | Explores model compression by using 1x1 convolutions called fire modules.                                                                                                                                                                     | [link](https://arxiv.org/abs/1602.07360) |
| 2 | [Pruning Filters for Efficient ConvNets](Research_Papers_Anubhav_Reads/Pruning_Filters_for_Efficient_ConvNets.md)                                                        | Pending | CNNs, CV , Image                    | Optimization-No. of params                                            | 2017 | arXiv      | Asim Kadav, Hao Li                                                |                                                                                                                                                                                                                                               | [link](https://arxiv.org/abs/1608.08710) |
| 3 | [Single Headed Attention RNN: Stop Thinking With Your Head](Research_Papers_Anubhav_Reads/Single_Headed_Attention_RNN_Stop_Thinking_With_You.md)                         | Pending | Attention, LSTMs, Text              | Optimization-No. of params                                            | 2019 | arXiv      | Stephen Merity                                                    |                                                                                                                                                                                                                                               | [link](https://arxiv.org/abs/1911.11423) |
| 4 | [Reformer: The Efficient Transformer](Research_Papers_Anubhav_Reads/Reformer_The_Efficient_Transformer.md)                                                               | Read    | Attention, Text , Transformers      | Architecture, Optimization-Memory, Optimization-No. of params         | 2020 | arXiv      | Anselm Levskaya, Lukasz Kaiser, Nikita Kitaev                     | Overcome time and memory complexity of Transformers by bucketing Query, Keys and using Reversible residual connections.                                                                                                                       | [link](https://arxiv.org/abs/2001.04451) |
| 5 | [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](Research_Papers_Anubhav_Reads/The_Lottery_Ticket_Hypothesis_Finding_Sparse,_Trai.md)          | Read    | NN Initialization, NNs              | Optimization-No. of params, Tips & Tricks                             | 2019 | ICLR       | Jonathan Frankle, Michael Carbin                                  | Lottery ticket hypothesis: dense, randomly-initialized, feed-forward networks contain subnetworks (winning tickets) that‚Äîwhen trained in isolation‚Äî reach test accuracy comparable to the original network in a similar number of iterations. | [link](https://arxiv.org/abs/1803.03635) |
| 6 | [Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask](Research_Papers_Anubhav_Reads/Deconstructing_Lottery_Tickets_Zeros,_Signs,_and_t.md)                   | Read    | NN Initialization, NNs              | Comparison, Optimization-No. of params, Tips & Tricks                 | 2019 | NeurIPS    | Hattie Zhou, Janice Lan, Jason Yosinski, Rosanne Liu              | Follow up on Lottery Ticket Hypothesis exploring the effects of different Masking criteria as well as Mask-1 and Mask-0 actions.                                                                                                              | [link](https://arxiv.org/abs/1905.01067) |
| 7 | [Training Compute-Optimal Large Language Models](Research_Papers_Anubhav_Reads/Training_Compute-Optimal_Large_Language_Models.md)                                        | Pending | Large-Language-Models, Transformers | Architecture, Optimization-No. of params, Pre-Training, Tips & Tricks | 2022 | arXiv      | Jordan Hoffmann, Laurent Sifre, Oriol Vinyals, Sebastian Borgeaud |                                                                                                                                                                                                                                               | [link](https://arxiv.org/abs/2203.15556) |


---

## Optimizations

|   | Paper Name                                                                                                                                                                 | Status  | Topic                                    | Category                     | Year | Conference | Author                                                                                 | Summary                                                                                                                                                                                                                                                                                                                                                  | Link                                                                           |
| - | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ---------------------------------------- | ---------------------------- | ---- | ---------- | -------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------ |
| 0 | [Bag of Tricks for Image Classification with Convolutional Neural Networks](Research_Papers_Anubhav_Reads/Bag_of_Tricks_for_Image_Classification_with_Convol.md)           | Read    | CV , Image                               | Optimizations, Tips & Tricks | 2018 | arXiv      | Tong He, Zhi Zhang                                                                     | Shows a dozen tricks (mixup, label smoothing, etc.) to improve CNN accuracy and training time.                                                                                                                                                                                                                                                           | [link](https://arxiv.org/abs/1812.01187)                                       |
| 1 | [Self-Normalizing Neural Networks](Research_Papers_Anubhav_Reads/Self-Normalizing_Neural_Networks.md)                                                                      | Pending | Activation Function, Tabular             | Optimizations, Tips & Tricks | 2017 | NIPS       | Andreas Mayr, G√ºnter Klambauer, Thomas Unterthiner                                     |                                                                                                                                                                                                                                                                                                                                                          | [link](https://papers.nips.cc/paper/6698-self-normalizing-neural-networks.pdf) |
| 2 | [How Does Batch Normalization Help Optimization?](Research_Papers_Anubhav_Reads/How_Does_Batch_Normalization_Help_Optimization.md)                                         | Pending | NNs, Normalization                       | Optimizations                | 2018 | arXiv      | Aleksander Madry, Andrew Ilyas, Dimitris Tsipras, Shibani Santurkar                    |                                                                                                                                                                                                                                                                                                                                                          | [link](https://arxiv.org/abs/1805.11604)                                       |
| 3 | [Group Normalization](Research_Papers_Anubhav_Reads/Group_Normalization.md)                                                                                                | Pending | NNs, Normalization                       | Optimizations                | 2018 | arXiv      | Kaiming He, Yuxin Wu                                                                   |                                                                                                                                                                                                                                                                                                                                                          | [link](https://arxiv.org/abs/1803.08494)                                       |
| 4 | [Spectral Normalization for GANs](Research_Papers_Anubhav_Reads/Spectral_Normalization_for_GANs.md)                                                                        | Pending | GANs, Normalization                      | Optimizations                | 2018 | arXiv      | Masanori Koyama, Takeru Miyato, Toshiki Kataoka, Yuichi Yoshida                        |                                                                                                                                                                                                                                                                                                                                                          | [link](https://arxiv.org/abs/1802.05957)                                       |
| 5 | [Symbolic Knowledge Distillation: from General Language Models to Commonsense Models](Research_Papers_Anubhav_Reads/Symbolic_Knowledge_Distillation_from_General_Langu.md) | Pending | Dataset, Text , Transformers             | Optimizations, Tips & Tricks | 2021 | arXiv      | Chandra Bhagavatula, Jack Hessel, Peter West, Yejin Choi                               |                                                                                                                                                                                                                                                                                                                                                          | [link](https://arxiv.org/abs/2110.07178)                                       |
| 6 | [ReAct: Synergizing Reasoning and Acting in Language Models](Research_Papers_Anubhav_Reads/ReAct_Synergizing_Reasoning_and_Acting_in_Language.md)                          | Pending | Generative, Large-Language-Models, Text  | Optimizations, Tips & Tricks | 2023 | ICLR       | Dian Yu, Izhak Shafran, Jeffrey Zhao, Karthik Narasimhan, Nan Du, Shunyu Yao, Yuan Cao | This paper introduces ReAct, a novel approach that leverages Large Language Models (LLMs) to interleave reasoning traces and task-specific actions. ReAct outperforms existing methods on various language and decision-making tasks, addressing issues like hallucination, error propagation, and improving human interpretability and trustworthiness. | [link](https://arxiv.org/abs/2210.03629)                                       |


---

## Other

|    | Paper Name                                                                                                                                                                                      | Status    | Topic                                     | Category | Year | Conference | Author                                                                                | Summary                                                                                                                                                                                                             | Link                                                                                                                                                                                                                                                   |
| -- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------- | ----------------------------------------- | -------- | ---- | ---------- | ------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 0  | [GPT-2 (Language Models are Unsupervised Multitask Learners)](Research_Papers_Anubhav_Reads/GPT-2_Language_Models_are_Unsupervised_Multitask_.md)                                               | Pending   | Attention, Text , Transformers            | Other    | 2019 |            | Alec Radford, Dario Amodei, Ilya Sutskever, Jeffrey Wu                                |                                                                                                                                                                                                                     | [link](https://www.ceid.upatras.gr/webpages/faculty/zaro/teaching/alg-ds/PRESENTATIONS/PAPERS/2019-Radford-et-al_Language-Models-Are-Unsupervised-Multitask-%20Learners.pdf)                                                                           |
| 1  | [A Simple yet Effective Baseline for 3D Human Pose Estimation](Research_Papers_Anubhav_Reads/A_Simple_yet_Effective_Baseline_for_3D_Human_Pose_.md)                                             | Pending   | CV , Pose Estimation                      | Other    | 2017 | ICCV       | James J. Little, Javier Romero, Julieta Martinez, Rayat Hossain                       |                                                                                                                                                                                                                     | [link](https://openaccess.thecvf.com/content_iccv_2017/html/Martinez_A_Simple_yet_ICCV_2017_paper.html)                                                                                                                                                |
| 2  | [A Comprehensive Guide on Activation Functions](Research_Papers_Anubhav_Reads/A_Comprehensive_Guide_on_Activation_Functions.md)                                                                 | This week | Activation Function                       | Other    | 2020 | Blog       | Ygor Rebou√ßas Serpa                                                                   |                                                                                                                                                                                                                     | [link](https://towardsdatascience.com/a-comprehensive-guide-on-activation-functions-b45ed37a4fa5)                                                                                                                                                      |
| 3  | [Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet](Research_Papers_Anubhav_Reads/Approximating_CNNs_with_Bag-of-local-Features_mode.md)                 | Reading   | CNNs, CV , Image                          | Other    | 2019 | arXiv      | Matthias Bethge, Wieland Brendel                                                      |                                                                                                                                                                                                                     | [link](https://arxiv.org/abs/1904.00760)                                                                                                                                                                                                               |
| 4  | [Pix2Pix: Image-to-Image Translation with Conditional Adversarial Nets](Research_Papers_Anubhav_Reads/Pix2Pix_Image-to-Image_Translation_with_Conditiona.md)                                    | Read      | GANs, Image                               | Other    | 2017 | CVPR       | Alexei A. Efros, Jun-Yan Zhu, Phillip Isola, Tinghui Zhou                             | Image to image translation using Conditional GANs and dataset of image pairs from one domain to another.                                                                                                            | [link](https://arxiv.org/abs/1611.07004)                                                                                                                                                                                                               |
| 5  | [Training BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs](Research_Papers_Anubhav_Reads/Training_BatchNorm_and_Only_BatchNorm_On_the_Expre.md)                | Pending   | CNNs, Image                               | Other    | 2020 | arXiv      | Ari S. Morcos, David J. Schwab, Jonathan Frankle                                      |                                                                                                                                                                                                                     | [link](https://arxiv.org/abs/2003.00152)                                                                                                                                                                                                               |
| 6  | [Arbitrary Style Transfer in Real-Time With Adaptive Instance Normalization](Research_Papers_Anubhav_Reads/Arbitrary_Style_Transfer_in_Real-Time_With_Adaptiv.md)                               | Pending   | CNNs, Image                               | Other    | 2017 | ICCV       | Serge Belongie, Xun Huang                                                             |                                                                                                                                                                                                                     | [link](https://openaccess.thecvf.com/content_iccv_2017/html/Huang_Arbitrary_Style_Transfer_ICCV_2017_paper.html)                                                                                                                                       |
| 7  | [WGAN: Wasserstein GAN](Research_Papers_Anubhav_Reads/WGAN_Wasserstein_GAN.md)                                                                                                                  | Pending   | GANs, Loss Function                       | Other    | 2017 | arXiv      | L√©on Bottou, Martin Arjovsky, Soumith Chintala                                        |                                                                                                                                                                                                                     | [link](https://arxiv.org/abs/1701.07875)                                                                                                                                                                                                               |
| 8  | [One-shot Text Field Labeling using Attention and Belief Propagation for Structure Information Extraction](Research_Papers_Anubhav_Reads/One-shot_Text_Field_Labeling_using_Attention_and_B.md) | Pending   | Image , Text                              | Other    | 2020 | arXiv      | Jun Huang, Mengli Cheng, Minghui Qiu, Wei Lin, Xing Shi                               |                                                                                                                                                                                                                     | [link](https://arxiv.org/abs/2009.04153)                                                                                                                                                                                                               |
| 9  | [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](Research_Papers_Anubhav_Reads/Perceptual_Losses_for_Real-Time_Style_Transfer_and.md)                                      | Pending   | Loss Function, NNs                        | Other    | 2016 | ECCV       | Alexandre Alahi, Justin Johnson, Li Fei-Fei                                           |                                                                                                                                                                                                                     | [link](https://arxiv.org/abs/1603.08155)                                                                                                                                                                                                               |
| 10 | [Topological Loss: Beyond the Pixel-Wise Loss for Topology-Aware Delineation](Research_Papers_Anubhav_Reads/Topological_Loss_Beyond_the_Pixel-Wise_Loss_for_To.md)                              | Pending   | Image , Loss Function, Segmentation       | Other    | 2018 | CVPR       | Agata Mosinska, Mateusz Kozi≈Ñski, Pablo M√°rquez-Neila, Pascal Fua                     |                                                                                                                                                                                                                     | [link](https://openaccess.thecvf.com/content_cvpr_2018/html/Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper.html)                                                                                                                                       |
| 11 | [Deep Double Descent: Where Bigger Models and More Data Hurt](Research_Papers_Anubhav_Reads/Deep_Double_Descent_Where_Bigger_Models_and_More_D.md)                                              | Pending   | NNs                                       | Other    | 2019 | arXiv      | Boaz Barak, Gal Kaplun, Ilya Sutskever, Preetum Nakkiran, Tristan Yang, Yamini Bansal |                                                                                                                                                                                                                     | [link](https://arxiv.org/abs/1912.02292)                                                                                                                                                                                                               |
| 12 | [StyleGAN: A Style-Based Generator Architecture for Generative Adversarial Networks](Research_Papers_Anubhav_Reads/StyleGAN_A_Style-Based_Generator_Architecture_for_.md)                       | Pending   | GANs, Image                               | Other    | 2019 | CVPR       | Samuli Laine, Tero Karras, Timo Aila                                                  |                                                                                                                                                                                                                     | [link](https://openaccess.thecvf.com/content_CVPR_2019/html/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.html)                                                                                      |
| 13 | [Image2StyleGAN: How to Embed Images Into the StyleGAN Latent Space?](Research_Papers_Anubhav_Reads/Image2StyleGAN_How_to_Embed_Images_Into_the_StyleG.md)                                      | Pending   | GANs, Image                               | Other    | 2019 | ICCV       | Peter Wonka, Rameen Abdal, Yipeng Qin                                                 |                                                                                                                                                                                                                     | [link](https://openaccess.thecvf.com/content_ICCV_2019/html/Abdal_Image2StyleGAN_How_to_Embed_Images_Into_the_StyleGAN_Latent_Space_ICCV_2019_paper.html)                                                                                              |
| 14 | [AnimeGAN: Towards the Automatic Anime Characters Creation with Generative Adversarial Networks](Research_Papers_Anubhav_Reads/AnimeGAN_Towards_the_Automatic_Anime_Characters_Cr.md)           | Pending   | GANs, Image                               | Other    | 2017 | NIPS       | Jiakai Zhang, Minjun Li, Yanghua Jin                                                  |                                                                                                                                                                                                                     | [link](https://arxiv.org/abs/1708.05509)                                                                                                                                                                                                               |
| 15 | [BEGAN: Boundary Equilibrium Generative Adversarial Networks](Research_Papers_Anubhav_Reads/BEGAN_Boundary_Equilibrium_Generative_Adversarial_.md)                                              | Pending   | GANs, Image                               | Other    | 2017 | arXiv      | David Berthelot, Luke Metz, Thomas Schumm                                             |                                                                                                                                                                                                                     | [link](https://arxiv.org/abs/1703.10717)                                                                                                                                                                                                               |
| 16 | [Adam: A Method for Stochastic Optimization](Research_Papers_Anubhav_Reads/Adam_A_Method_for_Stochastic_Optimization.md)                                                                        | Pending   | NNs, Optimizers                           | Other    | 2015 | ICLR       | Diederik P. Kingma, Jimmy Ba                                                          |                                                                                                                                                                                                                     | [link](https://arxiv.org/abs/1412.6980)                                                                                                                                                                                                                |
| 17 | [StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation](Research_Papers_Anubhav_Reads/StarGAN_Unified_Generative_Adversarial_Networks_fo.md)             | Pending   | GANs, Image                               | Other    | 2018 | CVPR       | Jaegul Choo, Jung-Woo Ha, Minje Choi, Munyoung Kim, Sunghun Kim, Yunjey Choi          |                                                                                                                                                                                                                     | [link](https://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_StarGAN_Unified_Generative_CVPR_2018_paper.pdf)                                                                                                                                     |
| 18 | [IMLE-GAN: Inclusive GAN: Improving Data and Minority Coverage in Generative Models](Research_Papers_Anubhav_Reads/IMLE-GAN_Inclusive_GAN_Improving_Data_and_Minority.md)                       | Pending   | GANs                                      | Other    | 2020 | arXiv      | Jitendra Malik, Ke Li, Larry Davis, Mario Fritz, Ning Yu, Peng Zhou                   |                                                                                                                                                                                                                     | [link](https://arxiv.org/abs/2004.03355?utm_campaign=The%20Batch&utm_medium=email&_hsmi=96406275&_hsenc=p2ANqtz-8ra-5k3I7Hv0hosTfQ1neO9Z10r3yMPB1oQfzpBEfkCQ_i0q0diEm4w21S8WqkMbOASXxQvDTIoqJbBZvX4i7S-exeOg&utm_content=96406275&utm_source=hs_email) |
| 19 | [ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning](Research_Papers_Anubhav_Reads/ATOMIC_An_Atlas_of_Machine_Commonsense_for_If-Then.md)                                            | Pending   | AGI, Dataset, Text                        | Other    | 2019 | AAAI       | Maarten Sap, Noah A. Smith, Ronan Le Bras, Yejin Choi                                 |                                                                                                                                                                                                                     | [link](https://arxiv.org/pdf/1811.00146.pdf)                                                                                                                                                                                                           |
| 20 | [COMET: Commonsense Transformers for Automatic Knowledge Graph Construction](Research_Papers_Anubhav_Reads/COMET_Commonsense_Transformers_for_Automatic_Knowl.md)                               | Pending   | AGI, Text , Transformers                  | Other    | 2019 | ACL        | Antoine Bosselut, Hannah Rashkin, Yejin Choi                                          |                                                                                                                                                                                                                     | [link](https://arxiv.org/pdf/1906.05317.pdf)                                                                                                                                                                                                           |
| 21 | [VisualCOMET: Reasoning about the Dynamic Context of a Still Image](Research_Papers_Anubhav_Reads/VisualCOMET_Reasoning_about_the_Dynamic_Context_of.md)                                        | Pending   | AGI, Dataset, Image , Text , Transformers | Other    | 2020 | ECCV       | Ali Farhadi, Chandra Bhagavatula, Jae Sung Park, Yejin Choi                           |                                                                                                                                                                                                                     | [link](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500494.pdf)                                                                                                                                                                         |
| 22 | [T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](Research_Papers_Anubhav_Reads/T5_Exploring_the_Limits_of_Transfer_Learning_with_.md)                    | Read      | Attention, Text , Transformers            | Other    | 2020 | JMLR       | Colin Raffel, Noam Shazeer, Peter J. Liu, Wei Liu, Yanqi Zhou                         | Presents a Text-to-Text transformer model with multi-task learning capabilities, simultaneously solving problems such as machine translation, document summarization, question answering, and classification tasks. | [link](https://arxiv.org/abs/1910.10683)                                                                                                                                                                                                               |
| 23 | [GPT-f: Generative Language Modeling for Automated Theorem Proving](Research_Papers_Anubhav_Reads/GPT-f_Generative_Language_Modeling_for_Automated_T.md)                                        | Pending   | Attention, Transformers                   | Other    | 2020 | arXiv      | Ilya Sutskever, Stanislas Polu                                                        |                                                                                                                                                                                                                     | [link](https://arxiv.org/abs/2009.03393)                                                                                                                                                                                                               |
| 24 | [Vision Transformer: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](Research_Papers_Anubhav_Reads/Vision_Transformer_An_Image_is_Worth_16x16_Words_T.md)           | Pending   | Attention, Image , Transformers           | Other    | 2021 | ICLR       | Alexey Dosovitskiy, Jakob Uszkoreit, Lucas Beyer, Neil Houlsby                        |                                                                                                                                                                                                                     | [link](https://arxiv.org/abs/2010.11929)                                                                                                                                                                                                               |
| 25 | [DALL¬∑E: Creating Images from Text](Research_Papers_Anubhav_Reads/DALL¬∑E_Creating_Images_from_Text.md)                                                                                          | Pending   | Image , Text , Transformers               | Other    | 2021 | Blog       | Aditya Ramesh, Gabriel Goh, Ilya Sutskever, Mikhail Pavlov, Scott Gray                |                                                                                                                                                                                                                     | [link](https://openai.com/blog/dall-e/)                                                                                                                                                                                                                |
| 26 | [Chain of Thought Prompting Elicits Reasoning in Large Language Models](Research_Papers_Anubhav_Reads/Chain_of_Thought_Prompting_Elicits_Reasoning_in_La.md)                                    | Pending   | Question-Answering, Text , Transformers   | Other    | 2022 | arXiv      | Denny Zhou, Jason Wei, Xuezhi Wang                                                    |                                                                                                                                                                                                                     | [link](https://arxiv.org/abs/2201.11903)                                                                                                                                                                                                               |


---

## Pre-Training

|   | Paper Name                                                                                                                                                   | Status  | Topic                                                                                         | Category                                                              | Year | Conference | Author                                                            | Summary                                                                                                                                                             | Link                                                    |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------- | --------------------------------------------------------------------------------------------- | --------------------------------------------------------------------- | ---- | ---------- | ----------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------- |
| 0 | [CLIP: Connecting Text and Images](Research_Papers_Anubhav_Reads/CLIP_Connecting_Text_and_Images.md)                                                         | Pending | Image , Text , Transformers                                                                   | Multimodal, Pre-Training                                              | 2021 | arXiv      | Alec Radford, Ilya Sutskever, Jong Wook Kim                       |                                                                                                                                                                     | [link](https://openai.com/blog/clip/)                   |
| 1 | [SpanBERT: Improving Pre-training by Representing and Predicting Spans](Research_Papers_Anubhav_Reads/SpanBERT_Improving_Pre-training_by_Representing_an.md) | Read    | Question-Answering, Text , Transformers                                                       | Pre-Training                                                          | 2020 | TACL       | Danqi Chen, Mandar Joshi                                          | A different pre-training strategy for BERT model to improve performance for Question Answering task.                                                                | [link](https://www.aclweb.org/anthology/2020.tacl-1.5/) |
| 2 | [Flan-T5: Scaling Instruction-Finetuned Language Models](Research_Papers_Anubhav_Reads/Flan-T5_Scaling_Instruction-Finetuned_Language_Mod.md)                | Pending | Generative, Text , Transformers                                                               | Architecture, Pre-Training                                            | 2022 | arXiv      | Hyung Won Chung, Le Hou                                           |                                                                                                                                                                     | [link](https://arxiv.org/abs/2210.11416)                |
| 3 | [Training Compute-Optimal Large Language Models](Research_Papers_Anubhav_Reads/Training_Compute-Optimal_Large_Language_Models.md)                            | Pending | Large-Language-Models, Transformers                                                           | Architecture, Optimization-No. of params, Pre-Training, Tips & Tricks | 2022 | arXiv      | Jordan Hoffmann, Laurent Sifre, Oriol Vinyals, Sebastian Borgeaud |                                                                                                                                                                     | [link](https://arxiv.org/abs/2203.15556)                |
| 4 | [VL-T5: Unifying Vision-and-Language Tasks via Text Generation](Research_Papers_Anubhav_Reads/VL-T5_Unifying_Vision-and-Language_Tasks_via_Text_.md)         | Read    | CNNs, CV , Generative, Image , Large-Language-Models, Question-Answering, Text , Transformers | Architecture, Embeddings, Multimodal, Pre-Training                    | 2021 | arXiv      | Hao Tan, Jaemin Cho, Jie Le, Mohit Bansal                         | Unifying two modalities (image and text) together in a single transformer model to solve multiple tasks in a single architecture using text prefixes similar to T5. | [link](https://arxiv.org/pdf/2102.02779.pdf)            |


---

## Prompting

|   | Paper Name                                                                                                                                                           | Status    | Topic                                      | Category                 | Year | Conference | Author                                  | Summary                                                                                                                                                                                      | Link                                     |
| - | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------- | ------------------------------------------ | ------------------------ | ---- | ---------- | --------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 0 | [Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering](Research_Papers_Anubhav_Reads/Code_Generation_with_AlphaCodium_From_Prompt_Engin.md) | Pending   | Large-Language-Models                      | Prompting, Tips & Tricks | 2024 | arXiv      | Dedy Kredo, Itamar Friedman, Tal Ridnik | This paper introduces AlphaCodium, a novel test-based, multi-stage, code-oriented iterative approach for improving the performance of Language Model Models (LLMs) on code generation tasks. | [link](https://arxiv.org/abs/2401.08500) |
| 1 | [Large Language Models for Data Annotation: A Survey](Research_Papers_Anubhav_Reads/Large_Language_Models_for_Data_Annotation_A_Survey.md)                           | This week | Dataset, Generative, Large-Language-Models | Prompting, Tips & Tricks | 2024 | arXiv      | Alimohammad Beigi, Zhen Tan             |                                                                                                                                                                                              | [link](https://arxiv.org/abs/2402.13446) |


---

## Reinforcement-Learning

|   | Paper Name                                                                                                                                                 | Status  | Topic                                              | Category                                                        | Year | Conference | Author                                                                                                                      | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Link                                                                                              |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | -------------------------------------------------- | --------------------------------------------------------------- | ---- | ---------- | --------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------- |
| 0 | [Occupancy Anticipation for Efficient Exploration and Navigation](Research_Papers_Anubhav_Reads/Occupancy_Anticipation_for_Efficient_Exploration_a.md)     | Pending | CNNs, Image                                        | Reinforcement-Learning                                          | 2020 | ECCV       | Kristen Grauman, Santhosh K. Ramakrishnan, Ziad Al-Halah                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | [link](https://arxiv.org/pdf/2008.09285.pdf)                                                      |
| 1 | [MuZero: Mastering Go, chess, shogi and Atari without rules](Research_Papers_Anubhav_Reads/MuZero_Mastering_Go,_chess,_shogi_and_Atari_withou.md)          | Pending |                                                    | Reinforcement-Learning                                          | 2020 | Nature     | David Silver, Demis Hassabis, Ioannis Antonoglou, Julian Schrittwiese                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | [link](https://deepmind.com/blog/article/muzero-mastering-go-chess-shogi-and-atari-without-rules) |
| 2 | [Training language models to follow instructions with human feedback](Research_Papers_Anubhav_Reads/Training_language_models_to_follow_instructions_wi.md) | Pending | Generative, Large-Language-Models, Training Method | Instruction-Finetuning, Reinforcement-Learning, Semi-Supervised | 2022 | arXiv      | Carroll L. Wainwright, Diogo Almeida, Jan Leike, Jeff Wu, Long Ouyang, Pamela Mishkin, Paul Christiano, Ryan Lowe, Xu Jiang | This paper presents InstructGPT, a model fine-tuned with human feedback to better align with user intent across various tasks. Despite having significantly fewer parameters than larger models, InstructGPT outperforms them in human evaluations, demonstrating improved truthfulness, reduced toxicity, and minimal performance regressions on public NLP datasets, highlighting the potential of fine-tuning with human feedback for enhancing language model alignment with human intent. | [link](https://arxiv.org/pdf/2203.02155.pdf)                                                      |
| 3 | [Constitutional AI: Harmlessness from AI Feedback](Research_Papers_Anubhav_Reads/Constitutional_AI_Harmlessness_from_AI_Feedback.md)                       | Pending | Generative, Large-Language-Models, Training Method | Instruction-Finetuning, Reinforcement-Learning, Unsupervised    | 2022 | arXiv      | Jared Kaplan, Yuntao Ba                                                                                                     | The paper introduces Constitutional AI, a method for training a safe AI assistant without human-labeled data on harmful outputs. It combines supervised learning and reinforcement learning phases, enabling the AI to engage with harmful queries by explaining its objections, thus improving control, transparency, and human-judged performance with minimal human oversight.                                                                                                              | [link](https://arxiv.org/pdf/2212.08073.pdf)                                                      |


---

## Semi-Supervised

|   | Paper Name                                                                                                                                                 | Status  | Topic                                              | Category                                                        | Year | Conference | Author                                                                                                                      | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Link                                                                           |
| - | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | -------------------------------------------------- | --------------------------------------------------------------- | ---- | ---------- | --------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------ |
| 0 | [Improved Techniques for Training GANs](Research_Papers_Anubhav_Reads/Improved_Techniques_for_Training_GANs.md)                                            | Pending | GANs, Image                                        | Semi-Supervised                                                 | 2016 | NIPS       | Alec Radford, Ian Goodfellow, Tim Salimans, Vicki Cheung, Wojciech Zaremba, Xi Chen                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | [link](http://papers.nips.cc/paper/6124-improved-techniques-for-training-gans) |
| 1 | [Training language models to follow instructions with human feedback](Research_Papers_Anubhav_Reads/Training_language_models_to_follow_instructions_wi.md) | Pending | Generative, Large-Language-Models, Training Method | Instruction-Finetuning, Reinforcement-Learning, Semi-Supervised | 2022 | arXiv      | Carroll L. Wainwright, Diogo Almeida, Jan Leike, Jeff Wu, Long Ouyang, Pamela Mishkin, Paul Christiano, Ryan Lowe, Xu Jiang | This paper presents InstructGPT, a model fine-tuned with human feedback to better align with user intent across various tasks. Despite having significantly fewer parameters than larger models, InstructGPT outperforms them in human evaluations, demonstrating improved truthfulness, reduced toxicity, and minimal performance regressions on public NLP datasets, highlighting the potential of fine-tuning with human feedback for enhancing language model alignment with human intent. | [link](https://arxiv.org/pdf/2203.02155.pdf)                                   |


---

## Tips & Tricks

|    | Paper Name                                                                                                                                                                 | Status    | Topic                                      | Category                                                              | Year | Conference | Author                                                                                 | Summary                                                                                                                                                                                                                                                                                                                                                  | Link                                                                                                                                            |
| -- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------- | ------------------------------------------ | --------------------------------------------------------------------- | ---- | ---------- | -------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------- |
| 0  | [Bag of Tricks for Image Classification with Convolutional Neural Networks](Research_Papers_Anubhav_Reads/Bag_of_Tricks_for_Image_Classification_with_Convol.md)           | Read      | CV , Image                                 | Optimizations, Tips & Tricks                                          | 2018 | arXiv      | Tong He, Zhi Zhang                                                                     | Shows a dozen tricks (mixup, label smoothing, etc.) to improve CNN accuracy and training time.                                                                                                                                                                                                                                                           | [link](https://arxiv.org/abs/1812.01187)                                                                                                        |
| 1  | [Class-Balanced Loss Based on Effective Number of Samples](Research_Papers_Anubhav_Reads/Class-Balanced_Loss_Based_on_Effective_Number_of_S.md)                            | Pending   | Loss Function                              | Tips & Tricks                                                         | 2019 | CVPR       | Menglin Jia, Yin Cui                                                                   |                                                                                                                                                                                                                                                                                                                                                          | [link](https://openaccess.thecvf.com/content_CVPR_2019/papers/Cui_Class-Balanced_Loss_Based_on_Effective_Number_of_Samples_CVPR_2019_paper.pdf) |
| 2  | [Self-Normalizing Neural Networks](Research_Papers_Anubhav_Reads/Self-Normalizing_Neural_Networks.md)                                                                      | Pending   | Activation Function, Tabular               | Optimizations, Tips & Tricks                                          | 2017 | NIPS       | Andreas Mayr, G√ºnter Klambauer, Thomas Unterthiner                                     |                                                                                                                                                                                                                                                                                                                                                          | [link](https://papers.nips.cc/paper/6698-self-normalizing-neural-networks.pdf)                                                                  |
| 3  | [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](Research_Papers_Anubhav_Reads/The_Lottery_Ticket_Hypothesis_Finding_Sparse,_Trai.md)            | Read      | NN Initialization, NNs                     | Optimization-No. of params, Tips & Tricks                             | 2019 | ICLR       | Jonathan Frankle, Michael Carbin                                                       | Lottery ticket hypothesis: dense, randomly-initialized, feed-forward networks contain subnetworks (winning tickets) that‚Äîwhen trained in isolation‚Äî reach test accuracy comparable to the original network in a similar number of iterations.                                                                                                            | [link](https://arxiv.org/abs/1803.03635)                                                                                                        |
| 4  | [All you need is a good init](Research_Papers_Anubhav_Reads/All_you_need_is_a_good_init.md)                                                                                | Pending   | NN Initialization                          | Tips & Tricks                                                         | 2015 | arXiv      | Dmytro Mishkin, Jiri Matas                                                             |                                                                                                                                                                                                                                                                                                                                                          | [link](https://arxiv.org/abs/1511.06422)                                                                                                        |
| 5  | [Word2Vec: Efficient Estimation of Word Representations in Vector Space](Research_Papers_Anubhav_Reads/Word2Vec_Efficient_Estimation_of_Word_Representati.md)              | Pending   | Text                                       | Embeddings, Tips & Tricks                                             | 2013 | arXiv      | Greg Corrado, Jeffrey Dean, Kai Chen, Tomas Mikolov                                    |                                                                                                                                                                                                                                                                                                                                                          | [link](https://arxiv.org/abs/1301.3781)                                                                                                         |
| 6  | [Understanding Loss Functions in Computer Vision](Research_Papers_Anubhav_Reads/Understanding_Loss_Functions_in_Computer_Vision.md)                                        | Pending   | CV , GANs, Image , Loss Function           | Comparison, Tips & Tricks                                             | 2020 | Blog       | Sowmya Yellapragada                                                                    |                                                                                                                                                                                                                                                                                                                                                          | [link](https://medium.com/ml-cheat-sheet/winning-at-loss-functions-2-important-loss-functions-in-computer-vision-b2b9d293e15a)                  |
| 7  | [Progressive Growing of GANs for Improved Quality, Stability, and Variation](Research_Papers_Anubhav_Reads/Progressive_Growing_of_GANs_for_Improved_Quality,_.md)          | Pending   | GANs, Image                                | Tips & Tricks                                                         | 2018 | ICLR       | Jaakko Lehtinen, Samuli Laine, Tero Karras, Timo Aila                                  |                                                                                                                                                                                                                                                                                                                                                          | [link](https://arxiv.org/abs/1710.10196)                                                                                                        |
| 8  | [Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask](Research_Papers_Anubhav_Reads/Deconstructing_Lottery_Tickets_Zeros,_Signs,_and_t.md)                     | Read      | NN Initialization, NNs                     | Comparison, Optimization-No. of params, Tips & Tricks                 | 2019 | NeurIPS    | Hattie Zhou, Janice Lan, Jason Yosinski, Rosanne Liu                                   | Follow up on Lottery Ticket Hypothesis exploring the effects of different Masking criteria as well as Mask-1 and Mask-0 actions.                                                                                                                                                                                                                         | [link](https://arxiv.org/abs/1905.01067)                                                                                                        |
| 9  | [Symbolic Knowledge Distillation: from General Language Models to Commonsense Models](Research_Papers_Anubhav_Reads/Symbolic_Knowledge_Distillation_from_General_Langu.md) | Pending   | Dataset, Text , Transformers               | Optimizations, Tips & Tricks                                          | 2021 | arXiv      | Chandra Bhagavatula, Jack Hessel, Peter West, Yejin Choi                               |                                                                                                                                                                                                                                                                                                                                                          | [link](https://arxiv.org/abs/2110.07178)                                                                                                        |
| 10 | [Transforming Sequence Tagging Into A Seq2Seq Task](Research_Papers_Anubhav_Reads/Transforming_Sequence_Tagging_Into_A_Seq2Seq_Task.md)                                    | Pending   | Generative, Text                           | Comparison, Tips & Tricks                                             | 2022 | arXiv      | Iftekhar Naim, Karthik Raman, Krishna Srinivasan                                       |                                                                                                                                                                                                                                                                                                                                                          | [link](https://arxiv.org/pdf/2203.08378.pdf)                                                                                                    |
| 11 | [Large Language Models are Zero-Shot Reasoners](Research_Papers_Anubhav_Reads/Large_Language_Models_are_Zero-Shot_Reasoners.md)                                            | Pending   | Generative, Question-Answering, Text       | Tips & Tricks, Zero-shot-learning                                     | 2022 | arXiv      | Takeshi Kojima, Yusuke Iwasawa                                                         |                                                                                                                                                                                                                                                                                                                                                          | [link](https://arxiv.org/abs/2205.11916)                                                                                                        |
| 12 | [Training Compute-Optimal Large Language Models](Research_Papers_Anubhav_Reads/Training_Compute-Optimal_Large_Language_Models.md)                                          | Pending   | Large-Language-Models, Transformers        | Architecture, Optimization-No. of params, Pre-Training, Tips & Tricks | 2022 | arXiv      | Jordan Hoffmann, Laurent Sifre, Oriol Vinyals, Sebastian Borgeaud                      |                                                                                                                                                                                                                                                                                                                                                          | [link](https://arxiv.org/abs/2203.15556)                                                                                                        |
| 13 | [ReAct: Synergizing Reasoning and Acting in Language Models](Research_Papers_Anubhav_Reads/ReAct_Synergizing_Reasoning_and_Acting_in_Language.md)                          | Pending   | Generative, Large-Language-Models, Text    | Optimizations, Tips & Tricks                                          | 2023 | ICLR       | Dian Yu, Izhak Shafran, Jeffrey Zhao, Karthik Narasimhan, Nan Du, Shunyu Yao, Yuan Cao | This paper introduces ReAct, a novel approach that leverages Large Language Models (LLMs) to interleave reasoning traces and task-specific actions. ReAct outperforms existing methods on various language and decision-making tasks, addressing issues like hallucination, error propagation, and improving human interpretability and trustworthiness. | [link](https://arxiv.org/abs/2210.03629)                                                                                                        |
| 14 | [Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering](Research_Papers_Anubhav_Reads/Code_Generation_with_AlphaCodium_From_Prompt_Engin.md)       | Pending   | Large-Language-Models                      | Prompting, Tips & Tricks                                              | 2024 | arXiv      | Dedy Kredo, Itamar Friedman, Tal Ridnik                                                | This paper introduces AlphaCodium, a novel test-based, multi-stage, code-oriented iterative approach for improving the performance of Language Model Models (LLMs) on code generation tasks.                                                                                                                                                             | [link](https://arxiv.org/abs/2401.08500)                                                                                                        |
| 15 | [Large Language Models for Data Annotation: A Survey](Research_Papers_Anubhav_Reads/Large_Language_Models_for_Data_Annotation_A_Survey.md)                                 | This week | Dataset, Generative, Large-Language-Models | Prompting, Tips & Tricks                                              | 2024 | arXiv      | Alimohammad Beigi, Zhen Tan                                                            |                                                                                                                                                                                                                                                                                                                                                          | [link](https://arxiv.org/abs/2402.13446)                                                                                                        |


---

## Unsupervised

|   | Paper Name                                                                                                                                             | Status  | Topic                                              | Category                                                     | Year | Conference | Author                                                                            | Summary                                                                                                                                                                                                                                                                                                                                                                           | Link                                         |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------ | ------- | -------------------------------------------------- | ------------------------------------------------------------ | ---- | ---------- | --------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |
| 0 | [Phrase-Based & Neural Unsupervised Machine Translation](Research_Papers_Anubhav_Reads/Phrase-Based_&_Neural_Unsupervised_Machine_Transla.md)          | Pending | NMT, Text , Transformers                           | Unsupervised                                                 | 2018 | arXiv      | Alexis Conneau, Guillaume Lample, Ludovic Denoyer, Marc'Aurelio Ranzato, Myle Ott |                                                                                                                                                                                                                                                                                                                                                                                   | [link](https://arxiv.org/abs/1804.07755)     |
| 1 | [Unsupervised Machine Translation Using Monolingual Corpora Only](Research_Papers_Anubhav_Reads/Unsupervised_Machine_Translation_Using_Monolingual.md) | Pending | GANs, NMT, Text , Transformers                     | Unsupervised                                                 | 2017 | arXiv      | Alexis Conneau, Guillaume Lample, Ludovic Denoyer, Marc'Aurelio Ranzato, Myle Ott |                                                                                                                                                                                                                                                                                                                                                                                   | [link](https://arxiv.org/abs/1711.00043)     |
| 2 | [Cross-lingual Language Model Pretraining](Research_Papers_Anubhav_Reads/Cross-lingual_Language_Model_Pretraining.md)                                  | Pending | NMT, Text , Transformers                           | Unsupervised                                                 | 2019 | arXiv      | Alexis Conneau, Guillaume Lample                                                  |                                                                                                                                                                                                                                                                                                                                                                                   | [link](https://arxiv.org/abs/1901.07291)     |
| 3 | [Constitutional AI: Harmlessness from AI Feedback](Research_Papers_Anubhav_Reads/Constitutional_AI_Harmlessness_from_AI_Feedback.md)                   | Pending | Generative, Large-Language-Models, Training Method | Instruction-Finetuning, Reinforcement-Learning, Unsupervised | 2022 | arXiv      | Jared Kaplan, Yuntao Ba                                                           | The paper introduces Constitutional AI, a method for training a safe AI assistant without human-labeled data on harmful outputs. It combines supervised learning and reinforcement learning phases, enabling the AI to engage with harmful queries by explaining its objections, thus improving control, transparency, and human-judged performance with minimal human oversight. | [link](https://arxiv.org/pdf/2212.08073.pdf) |


---

## Visualization

|   | Paper Name                                                                                                                                                        | Status  | Topic             | Category                  | Year | Conference | Author                        | Summary                                                                          | Link                                                                   |
| - | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----------------- | ------------------------- | ---- | ---------- | ----------------------------- | -------------------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| 0 | [ZF Net (Visualizing and Understanding Convolutional Networks)](Research_Papers_Anubhav_Reads/ZF_Net_Visualizing_and_Understanding_Convolutiona.md)               | Read    | CNNs, CV , Image  | Visualization             | 2014 | ECCV       | Matthew D. Zeiler, Rob Fergus | Visualize CNN Filters / Kernels using De-Convolutions on CNN filter activations. | [link](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53) |
| 1 | [Interpreting Deep Learning Models in Natural Language Processing: A Review](Research_Papers_Anubhav_Reads/Interpreting_Deep_Learning_Models_in_Natural_Langu.md) | Pending | Text              | Comparison, Visualization | 2021 | arXiv      | Diyi Yang, Xiaofei Sun        |                                                                                  | [link](https://arxiv.org/abs/2110.10470)                               |


---

## Zero-shot-learning

|   | Paper Name                                                                                                                                                                            | Status  | Topic                                   | Category                          | Year | Conference | Author                         | Summary                                                                                                                                                  | Link                                                       |
| - | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | --------------------------------------- | --------------------------------- | ---- | ---------- | ------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------- |
| 0 | [Learning to Extract Attribute Value from Product via Question Answering: A Multi-task Approach](Research_Papers_Anubhav_Reads/Learning_to_Extract_Attribute_Value_from_Product_v.md) | Read    | Question-Answering, Text , Transformers | Zero-shot-learning                | 2020 | KDD        | Li Yang, Qifan Wang            | Question Answering BERT model used to extract attributes from products. Introduce further No Answer loss and distillation to promote zero shot learning. | [link](https://dl.acm.org/doi/pdf/10.1145/3394486.3403047) |
| 1 | [Large Language Models are Zero-Shot Reasoners](Research_Papers_Anubhav_Reads/Large_Language_Models_are_Zero-Shot_Reasoners.md)                                                       | Pending | Generative, Question-Answering, Text    | Tips & Tricks, Zero-shot-learning | 2022 | arXiv      | Takeshi Kojima, Yusuke Iwasawa |                                                                                                                                                          | [link](https://arxiv.org/abs/2205.11916)                   |


---

