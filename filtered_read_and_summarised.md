# Read and Summarised Papers

<!-- no toc -->
- [Read](#Read)

---

## Read

|    | Paper Name                                                                                                                                                                            | Status | Topic                                                                                         | Category                                                      | Year | Conference | Author                                                        | Summary                                                                                                                                                                                                                                       | Link                                                                                                        |
| -- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------ | --------------------------------------------------------------------------------------------- | ------------------------------------------------------------- | ---- | ---------- | ------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------- |
| 0  | [ZF Net (Visualizing and Understanding Convolutional Networks)](Research_Papers_Anubhav_Reads/ZF_Net_Visualizing_and_Understanding_Convolutiona.md)                                   | Read   | CNNs, CV , Image                                                                              | Visualization                                                 | 2014 | ECCV       | Matthew D. Zeiler, Rob Fergus                                 | Visualize CNN Filters / Kernels using De-Convolutions on CNN filter activations.                                                                                                                                                              | [link](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53)                                      |
| 1  | [Inception-v1 (Going Deeper With Convolutions)](Research_Papers_Anubhav_Reads/Inception-v1_Going_Deeper_With_Convolutions.md)                                                         | Read   | CNNs, CV , Image                                                                              | Architecture                                                  | 2015 | CVPR       | Christian Szegedy, Wei Liu                                    | Propose the use of 1x1 conv operations to reduce the number of parameters in a deep and wide CNN                                                                                                                                              | [link](https://arxiv.org/abs/1409.4842)                                                                     |
| 2  | [ResNet (Deep Residual Learning for Image Recognition)](Research_Papers_Anubhav_Reads/ResNet_Deep_Residual_Learning_for_Image_Recogniti.md)                                           | Read   | CNNs, CV , Image                                                                              | Architecture                                                  | 2016 | CVPR       | Kaiming He, Xiangyu Zhang                                     | Introduces Residual or Skip Connections to allow increase in the depth of a DNN                                                                                                                                                               | [link](https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) |
| 3  | [Evaluation of neural network architectures for embedded systems](Research_Papers_Anubhav_Reads/Evaluation_of_neural_network_architectures_for_emb.md)                                | Read   | CNNs, CV , Image                                                                              | Comparison                                                    | 2017 | IEEE ISCAS | Adam Paszke, Alfredo Canziani, Eugenio Culurciello            | Compare CNN classification architectures on accuracy, memory footprint, parameters, operations count, inference time and power consumption.                                                                                                   | [link](https://ieeexplore.ieee.org/abstract/document/8050276)                                               |
| 4  | [SqueezeNet](Research_Papers_Anubhav_Reads/SqueezeNet.md)                                                                                                                             | Read   | CNNs, CV , Image                                                                              | Architecture, Optimization-No. of params                      | 2016 | arXiv      | Forrest N. Iandola, Song Han                                  | Explores model compression by using 1x1 convolutions called fire modules.                                                                                                                                                                     | [link](https://arxiv.org/abs/1602.07360)                                                                    |
| 5  | [Attention is All you Need](Research_Papers_Anubhav_Reads/Attention_is_All_you_Need.md)                                                                                               | Read   | Attention, Text , Transformers                                                                | Architecture                                                  | 2017 | NIPS       | Ashish Vaswani, Illia Polosukhin, Noam Shazeer, Łukasz Kaiser | Talks about Transformer architecture which brings SOTA performance for different tasks in NLP                                                                                                                                                 | [link](http://papers.nips.cc/paper/7181-attention-is-all-you-need)                                          |
| 6  | [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](Research_Papers_Anubhav_Reads/BERT_Pre-training_of_Deep_Bidirectional_Transforme.md)               | Read   | Attention, Text , Transformers                                                                | Embeddings                                                    | 2018 | NAACL      | Jacob Devlin, Kenton Lee, Kristina Toutanova, Ming-Wei Chang  | BERT is an extension to Transformer based architecture which introduces a masked word pretraining and next sentence prediction task to pretrain the model for a wide variety of tasks.                                                        | [link](https://arxiv.org/abs/1810.04805)                                                                    |
| 7  | [Reformer: The Efficient Transformer](Research_Papers_Anubhav_Reads/Reformer_The_Efficient_Transformer.md)                                                                            | Read   | Attention, Text , Transformers                                                                | Architecture, Optimization-Memory, Optimization-No. of params | 2020 | arXiv      | Anselm Levskaya, Lukasz Kaiser, Nikita Kitaev                 | Overcome time and memory complexity of Transformers by bucketing Query, Keys and using Reversible residual connections.                                                                                                                       | [link](https://arxiv.org/abs/2001.04451)                                                                    |
| 8  | [Bag of Tricks for Image Classification with Convolutional Neural Networks](Research_Papers_Anubhav_Reads/Bag_of_Tricks_for_Image_Classification_with_Convol.md)                      | Read   | CV , Image                                                                                    | Optimizations, Tips & Tricks                                  | 2018 | arXiv      | Tong He, Zhi Zhang                                            | Shows a dozen tricks (mixup, label smoothing, etc.) to improve CNN accuracy and training time.                                                                                                                                                | [link](https://arxiv.org/abs/1812.01187)                                                                    |
| 9  | [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](Research_Papers_Anubhav_Reads/The_Lottery_Ticket_Hypothesis_Finding_Sparse,_Trai.md)                       | Read   | NN Initialization, NNs                                                                        | Optimization-No. of params, Tips & Tricks                     | 2019 | ICLR       | Jonathan Frankle, Michael Carbin                              | Lottery ticket hypothesis: dense, randomly-initialized, feed-forward networks contain subnetworks (winning tickets) that—when trained in isolation— reach test accuracy comparable to the original network in a similar number of iterations. | [link](https://arxiv.org/abs/1803.03635)                                                                    |
| 10 | [Pix2Pix: Image-to-Image Translation with Conditional Adversarial Nets](Research_Papers_Anubhav_Reads/Pix2Pix_Image-to-Image_Translation_with_Conditiona.md)                          | Read   | GANs, Image                                                                                   |                                                               | 2017 | CVPR       | Alexei A. Efros, Jun-Yan Zhu, Phillip Isola, Tinghui Zhou     | Image to image translation using Conditional GANs and dataset of image pairs from one domain to another.                                                                                                                                      | [link](https://arxiv.org/abs/1611.07004)                                                                    |
| 11 | [Language-Agnostic BERT Sentence Embedding](Research_Papers_Anubhav_Reads/Language-Agnostic_BERT_Sentence_Embedding.md)                                                               | Read   | Attention, Siamese Network, Text , Transformers                                               | Embeddings                                                    | 2020 | arXiv      | Fangxiaoyu Feng, Yinfei Yang                                  | A BERT model with multilingual sentence embeddings learned over 112 languages and Zero-shot learning over unseen languages.                                                                                                                   | [link](https://arxiv.org/abs/2007.01852)                                                                    |
| 12 | [T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](Research_Papers_Anubhav_Reads/T5_Exploring_the_Limits_of_Transfer_Learning_with_.md)          | Read   | Attention, Text , Transformers                                                                |                                                               | 2020 | JMLR       | Colin Raffel, Noam Shazeer, Peter J. Liu, Wei Liu, Yanqi Zhou | Presents a Text-to-Text transformer model with multi-task learning capabilities, simultaneously solving problems such as machine translation, document summarization, question answering, and classification tasks.                           | [link](https://arxiv.org/abs/1910.10683)                                                                    |
| 13 | [Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask](Research_Papers_Anubhav_Reads/Deconstructing_Lottery_Tickets_Zeros,_Signs,_and_t.md)                                | Read   | NN Initialization, NNs                                                                        | Comparison, Optimization-No. of params, Tips & Tricks         | 2019 | NeurIPS    | Hattie Zhou, Janice Lan, Jason Yosinski, Rosanne Liu          | Follow up on Lottery Ticket Hypothesis exploring the effects of different Masking criteria as well as Mask-1 and Mask-0 actions.                                                                                                              | [link](https://arxiv.org/abs/1905.01067)                                                                    |
| 14 | [SpanBERT: Improving Pre-training by Representing and Predicting Spans](Research_Papers_Anubhav_Reads/SpanBERT_Improving_Pre-training_by_Representing_an.md)                          | Read   | Question-Answering, Text , Transformers                                                       | Pre-Training                                                  | 2020 | TACL       | Danqi Chen, Mandar Joshi                                      | A different pre-training strategy for BERT model to improve performance for Question Answering task.                                                                                                                                          | [link](https://www.aclweb.org/anthology/2020.tacl-1.5/)                                                     |
| 15 | [Learning to Extract Attribute Value from Product via Question Answering: A Multi-task Approach](Research_Papers_Anubhav_Reads/Learning_to_Extract_Attribute_Value_from_Product_v.md) | Read   | Question-Answering, Text , Transformers                                                       | Zero-shot-learning                                            | 2020 | KDD        | Li Yang, Qifan Wang                                           | Question Answering BERT model used to extract attributes from products. Introduce further No Answer loss and distillation to promote zero shot learning.                                                                                      | [link](https://dl.acm.org/doi/pdf/10.1145/3394486.3403047)                                                  |
| 16 | [VL-T5: Unifying Vision-and-Language Tasks via Text Generation](Research_Papers_Anubhav_Reads/VL-T5_Unifying_Vision-and-Language_Tasks_via_Text_.md)                                  | Read   | CNNs, CV , Generative, Image , Large-Language-Models, Question-Answering, Text , Transformers | Architecture, Embeddings, Multimodal, Pre-Training            | 2021 | arXiv      | Hao Tan, Jaemin Cho, Jie Le, Mohit Bansal                     | Unifying two modalities (image and text) together in a single transformer model to solve multiple tasks in a single architecture using text prefixes similar to T5.                                                                           | [link](https://arxiv.org/pdf/2102.02779.pdf)                                                                |


---

